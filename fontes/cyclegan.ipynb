{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n",
      "channels:1\n",
      "epoch: 0 Val H_real: -0.0644 Val H_fake: -0.0722 Val Loss G_H: 0.7381 Val Loss G_Z: 0.7807\n",
      "epoch: 20 Val H_real: -0.0154 Val H_fake: -0.0169 Val Loss G_H: 0.7081 Val Loss G_Z: 0.7103\n",
      "epoch: 40 Val H_real: -0.0320 Val H_fake: -0.0326 Val Loss G_H: 0.7154 Val Loss G_Z: 0.7143\n",
      "epoch: 60 Val H_real: -0.0482 Val H_fake: -0.0440 Val Loss G_H: 0.7210 Val Loss G_Z: 0.7189\n",
      "epoch: 80 Val H_real: -0.0670 Val H_fake: -0.0585 Val Loss G_H: 0.7284 Val Loss G_Z: 0.7294\n",
      "epoch: 100 Val H_real: -0.0780 Val H_fake: -0.0698 Val Loss G_H: 0.7343 Val Loss G_Z: 0.7383\n",
      "epoch: 120 Val H_real: -0.0931 Val H_fake: -0.0842 Val Loss G_H: 0.7418 Val Loss G_Z: 0.7454\n",
      "epoch: 140 Val H_real: -0.1080 Val H_fake: -0.0992 Val Loss G_H: 0.7498 Val Loss G_Z: 0.7550\n",
      "epoch: 160 Val H_real: -0.1231 Val H_fake: -0.1174 Val Loss G_H: 0.7595 Val Loss G_Z: 0.7628\n",
      "epoch: 180 Val H_real: -0.1476 Val H_fake: -0.1438 Val Loss G_H: 0.7738 Val Loss G_Z: 0.7754\n",
      "epoch: 200 Val H_real: -0.1677 Val H_fake: -0.1676 Val Loss G_H: 0.7867 Val Loss G_Z: 0.7833\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/10/fold1/generator.pth\n",
      "epoch: 0 Val H_real: -0.0481 Val H_fake: -0.0423 Val Loss G_H: 0.7233 Val Loss G_Z: 0.6963\n",
      "epoch: 20 Val H_real: -0.0093 Val H_fake: -0.0069 Val Loss G_H: 0.7029 Val Loss G_Z: 0.7040\n",
      "epoch: 40 Val H_real: -0.0154 Val H_fake: -0.0118 Val Loss G_H: 0.7049 Val Loss G_Z: 0.7092\n",
      "epoch: 60 Val H_real: -0.0277 Val H_fake: -0.0231 Val Loss G_H: 0.7104 Val Loss G_Z: 0.7170\n",
      "epoch: 80 Val H_real: -0.0509 Val H_fake: -0.0410 Val Loss G_H: 0.7195 Val Loss G_Z: 0.7231\n",
      "epoch: 100 Val H_real: -0.0718 Val H_fake: -0.0591 Val Loss G_H: 0.7288 Val Loss G_Z: 0.7295\n",
      "epoch: 120 Val H_real: -0.0927 Val H_fake: -0.0794 Val Loss G_H: 0.7394 Val Loss G_Z: 0.7383\n",
      "epoch: 140 Val H_real: -0.1078 Val H_fake: -0.0924 Val Loss G_H: 0.7463 Val Loss G_Z: 0.7453\n",
      "epoch: 160 Val H_real: -0.1245 Val H_fake: -0.1110 Val Loss G_H: 0.7562 Val Loss G_Z: 0.7558\n",
      "epoch: 180 Val H_real: -0.1549 Val H_fake: -0.1431 Val Loss G_H: 0.7734 Val Loss G_Z: 0.7686\n",
      "epoch: 200 Val H_real: -0.1740 Val H_fake: -0.1652 Val Loss G_H: 0.7853 Val Loss G_Z: 0.7803\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/10/fold2/generator.pth\n",
      "epoch: 0 Val H_real: -0.0301 Val H_fake: -0.0358 Val Loss G_H: 0.7198 Val Loss G_Z: 0.7435\n",
      "epoch: 20 Val H_real: 0.0156 Val H_fake: -0.0287 Val Loss G_H: 0.7159 Val Loss G_Z: 0.7218\n",
      "epoch: 40 Val H_real: 0.0272 Val H_fake: -0.0711 Val Loss G_H: 0.7386 Val Loss G_Z: 0.7431\n",
      "epoch: 60 Val H_real: -0.0418 Val H_fake: -0.1160 Val Loss G_H: 0.7638 Val Loss G_Z: 0.7607\n",
      "epoch: 80 Val H_real: -0.1717 Val H_fake: -0.1235 Val Loss G_H: 0.7671 Val Loss G_Z: 0.7742\n",
      "epoch: 100 Val H_real: -0.1763 Val H_fake: -0.1703 Val Loss G_H: 0.7916 Val Loss G_Z: 0.7863\n",
      "epoch: 120 Val H_real: -0.1799 Val H_fake: -0.2008 Val Loss G_H: 0.8083 Val Loss G_Z: 0.8046\n",
      "epoch: 140 Val H_real: -0.2213 Val H_fake: -0.2434 Val Loss G_H: 0.8321 Val Loss G_Z: 0.8249\n",
      "epoch: 160 Val H_real: -0.2555 Val H_fake: -0.2710 Val Loss G_H: 0.8482 Val Loss G_Z: 0.8403\n",
      "epoch: 180 Val H_real: -0.3010 Val H_fake: -0.3161 Val Loss G_H: 0.8742 Val Loss G_Z: 0.8625\n",
      "epoch: 200 Val H_real: -0.3574 Val H_fake: -0.3769 Val Loss G_H: 0.9099 Val Loss G_Z: 0.8848\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/10/fold3/generator.pth\n",
      "epoch: 0 Val H_real: -0.0289 Val H_fake: -0.0366 Val Loss G_H: 0.7212 Val Loss G_Z: 0.6841\n",
      "epoch: 20 Val H_real: -0.0280 Val H_fake: -0.0380 Val Loss G_H: 0.7196 Val Loss G_Z: 0.7040\n",
      "epoch: 40 Val H_real: -0.0246 Val H_fake: -0.0410 Val Loss G_H: 0.7207 Val Loss G_Z: 0.7143\n",
      "epoch: 60 Val H_real: -0.0566 Val H_fake: -0.0480 Val Loss G_H: 0.7238 Val Loss G_Z: 0.7249\n",
      "epoch: 80 Val H_real: -0.0750 Val H_fake: -0.0689 Val Loss G_H: 0.7343 Val Loss G_Z: 0.7328\n",
      "epoch: 100 Val H_real: -0.0904 Val H_fake: -0.0827 Val Loss G_H: 0.7414 Val Loss G_Z: 0.7381\n",
      "epoch: 120 Val H_real: -0.1051 Val H_fake: -0.0978 Val Loss G_H: 0.7494 Val Loss G_Z: 0.7443\n",
      "epoch: 140 Val H_real: -0.1182 Val H_fake: -0.1105 Val Loss G_H: 0.7562 Val Loss G_Z: 0.7523\n",
      "epoch: 160 Val H_real: -0.1358 Val H_fake: -0.1273 Val Loss G_H: 0.7653 Val Loss G_Z: 0.7612\n",
      "epoch: 180 Val H_real: -0.1549 Val H_fake: -0.1481 Val Loss G_H: 0.7764 Val Loss G_Z: 0.7700\n",
      "epoch: 200 Val H_real: -0.1795 Val H_fake: -0.1725 Val Loss G_H: 0.7896 Val Loss G_Z: 0.7794\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/10/fold4/generator.pth\n",
      "epoch: 0 Val H_real: -0.1148 Val H_fake: -0.1249 Val Loss G_H: 0.7667 Val Loss G_Z: 0.7319\n",
      "epoch: 20 Val H_real: 0.0033 Val H_fake: -0.0289 Val Loss G_H: 0.7155 Val Loss G_Z: 0.7094\n",
      "epoch: 40 Val H_real: -0.0158 Val H_fake: -0.0440 Val Loss G_H: 0.7233 Val Loss G_Z: 0.7166\n",
      "epoch: 60 Val H_real: -0.0689 Val H_fake: -0.0566 Val Loss G_H: 0.7294 Val Loss G_Z: 0.7259\n",
      "epoch: 80 Val H_real: -0.1010 Val H_fake: -0.0882 Val Loss G_H: 0.7455 Val Loss G_Z: 0.7376\n",
      "epoch: 100 Val H_real: -0.1261 Val H_fake: -0.1176 Val Loss G_H: 0.7610 Val Loss G_Z: 0.7552\n",
      "epoch: 120 Val H_real: -0.1609 Val H_fake: -0.1689 Val Loss G_H: 0.7886 Val Loss G_Z: 0.7758\n",
      "epoch: 140 Val H_real: -0.2123 Val H_fake: -0.2146 Val Loss G_H: 0.8139 Val Loss G_Z: 0.8015\n",
      "epoch: 160 Val H_real: -0.2510 Val H_fake: -0.2476 Val Loss G_H: 0.8328 Val Loss G_Z: 0.8156\n",
      "epoch: 180 Val H_real: -0.2839 Val H_fake: -0.2848 Val Loss G_H: 0.8541 Val Loss G_Z: 0.8408\n",
      "epoch: 200 Val H_real: -0.3180 Val H_fake: -0.3216 Val Loss G_H: 0.8752 Val Loss G_Z: 0.8643\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/10/fold5/generator.pth\n",
      "epoch: 0 Val H_real: -0.0646 Val H_fake: -0.0649 Val Loss G_H: 0.7342 Val Loss G_Z: 0.7215\n",
      "epoch: 20 Val H_real: 0.0324 Val H_fake: -0.0581 Val Loss G_H: 0.7304 Val Loss G_Z: 0.7382\n",
      "epoch: 40 Val H_real: 0.0877 Val H_fake: -0.1409 Val Loss G_H: 0.7750 Val Loss G_Z: 0.7972\n",
      "epoch: 60 Val H_real: 0.1929 Val H_fake: -0.2697 Val Loss G_H: 0.8478 Val Loss G_Z: 0.9080\n",
      "epoch: 80 Val H_real: -0.0441 Val H_fake: -0.0701 Val Loss G_H: 0.7442 Val Loss G_Z: 0.7243\n",
      "epoch: 100 Val H_real: -0.0454 Val H_fake: -0.1527 Val Loss G_H: 0.7884 Val Loss G_Z: 0.7528\n",
      "epoch: 120 Val H_real: -0.0391 Val H_fake: -0.2443 Val Loss G_H: 0.8416 Val Loss G_Z: 0.7842\n",
      "epoch: 140 Val H_real: -0.0477 Val H_fake: -0.3204 Val Loss G_H: 0.8887 Val Loss G_Z: 0.8140\n",
      "epoch: 160 Val H_real: -0.0811 Val H_fake: -0.3671 Val Loss G_H: 0.9210 Val Loss G_Z: 0.8450\n",
      "epoch: 180 Val H_real: -0.1156 Val H_fake: -0.4473 Val Loss G_H: 0.9729 Val Loss G_Z: 0.8789\n",
      "epoch: 200 Val H_real: -0.1600 Val H_fake: -0.5849 Val Loss G_H: 1.0656 Val Loss G_Z: 0.9145\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/20/fold1/generator.pth\n",
      "epoch: 0 Val H_real: -0.0089 Val H_fake: -0.0159 Val Loss G_H: 0.7105 Val Loss G_Z: 0.6639\n",
      "epoch: 20 Val H_real: 0.0034 Val H_fake: -0.0650 Val Loss G_H: 0.7338 Val Loss G_Z: 0.7280\n",
      "epoch: 40 Val H_real: 0.0152 Val H_fake: -0.1134 Val Loss G_H: 0.7592 Val Loss G_Z: 0.7530\n",
      "epoch: 60 Val H_real: -0.0588 Val H_fake: -0.1068 Val Loss G_H: 0.7556 Val Loss G_Z: 0.7334\n",
      "epoch: 80 Val H_real: -0.1319 Val H_fake: -0.1052 Val Loss G_H: 0.7545 Val Loss G_Z: 0.7187\n",
      "epoch: 100 Val H_real: -0.1426 Val H_fake: -0.1235 Val Loss G_H: 0.7641 Val Loss G_Z: 0.7379\n",
      "epoch: 120 Val H_real: -0.1544 Val H_fake: -0.1430 Val Loss G_H: 0.7748 Val Loss G_Z: 0.7548\n",
      "epoch: 140 Val H_real: -0.1726 Val H_fake: -0.1643 Val Loss G_H: 0.7867 Val Loss G_Z: 0.7718\n",
      "epoch: 160 Val H_real: -0.1949 Val H_fake: -0.1874 Val Loss G_H: 0.7997 Val Loss G_Z: 0.7856\n",
      "epoch: 180 Val H_real: -0.2249 Val H_fake: -0.2195 Val Loss G_H: 0.8180 Val Loss G_Z: 0.7976\n",
      "epoch: 200 Val H_real: -0.2580 Val H_fake: -0.2538 Val Loss G_H: 0.8380 Val Loss G_Z: 0.8189\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/20/fold2/generator.pth\n",
      "epoch: 0 Val H_real: 0.0125 Val H_fake: 0.0091 Val Loss G_H: 0.6982 Val Loss G_Z: 0.7600\n",
      "epoch: 20 Val H_real: -0.0061 Val H_fake: -0.0025 Val Loss G_H: 0.7017 Val Loss G_Z: 0.7036\n",
      "epoch: 40 Val H_real: -0.0156 Val H_fake: -0.0111 Val Loss G_H: 0.7057 Val Loss G_Z: 0.7069\n",
      "epoch: 60 Val H_real: -0.0202 Val H_fake: -0.0169 Val Loss G_H: 0.7089 Val Loss G_Z: 0.7112\n",
      "epoch: 80 Val H_real: -0.0263 Val H_fake: -0.0241 Val Loss G_H: 0.7128 Val Loss G_Z: 0.7190\n",
      "epoch: 100 Val H_real: -0.0375 Val H_fake: -0.0317 Val Loss G_H: 0.7172 Val Loss G_Z: 0.7262\n",
      "epoch: 120 Val H_real: -0.0577 Val H_fake: -0.0416 Val Loss G_H: 0.7228 Val Loss G_Z: 0.7326\n",
      "epoch: 140 Val H_real: -0.0992 Val H_fake: -0.0764 Val Loss G_H: 0.7414 Val Loss G_Z: 0.7487\n",
      "epoch: 160 Val H_real: -0.1484 Val H_fake: -0.1208 Val Loss G_H: 0.7654 Val Loss G_Z: 0.7677\n",
      "epoch: 180 Val H_real: -0.2000 Val H_fake: -0.1766 Val Loss G_H: 0.7957 Val Loss G_Z: 0.7914\n",
      "epoch: 200 Val H_real: -0.2425 Val H_fake: -0.2265 Val Loss G_H: 0.8235 Val Loss G_Z: 0.8162\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/20/fold3/generator.pth\n",
      "epoch: 0 Val H_real: 0.0220 Val H_fake: 0.0106 Val Loss G_H: 0.6966 Val Loss G_Z: 0.7241\n",
      "epoch: 20 Val H_real: -0.0365 Val H_fake: -0.0367 Val Loss G_H: 0.7186 Val Loss G_Z: 0.7062\n",
      "epoch: 40 Val H_real: -0.0503 Val H_fake: -0.0380 Val Loss G_H: 0.7185 Val Loss G_Z: 0.7167\n",
      "epoch: 60 Val H_real: -0.0589 Val H_fake: -0.0460 Val Loss G_H: 0.7224 Val Loss G_Z: 0.7236\n",
      "epoch: 80 Val H_real: -0.0717 Val H_fake: -0.0537 Val Loss G_H: 0.7265 Val Loss G_Z: 0.7286\n",
      "epoch: 100 Val H_real: -0.0863 Val H_fake: -0.0623 Val Loss G_H: 0.7312 Val Loss G_Z: 0.7316\n",
      "epoch: 120 Val H_real: -0.0990 Val H_fake: -0.0731 Val Loss G_H: 0.7370 Val Loss G_Z: 0.7372\n",
      "epoch: 140 Val H_real: -0.1185 Val H_fake: -0.0892 Val Loss G_H: 0.7457 Val Loss G_Z: 0.7426\n",
      "epoch: 160 Val H_real: -0.1452 Val H_fake: -0.1142 Val Loss G_H: 0.7594 Val Loss G_Z: 0.7488\n",
      "epoch: 180 Val H_real: -0.1694 Val H_fake: -0.1400 Val Loss G_H: 0.7736 Val Loss G_Z: 0.7606\n",
      "epoch: 200 Val H_real: -0.1984 Val H_fake: -0.1745 Val Loss G_H: 0.7928 Val Loss G_Z: 0.7733\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/20/fold4/generator.pth\n",
      "epoch: 0 Val H_real: 0.0100 Val H_fake: -0.0046 Val Loss G_H: 0.7042 Val Loss G_Z: 0.7010\n",
      "epoch: 20 Val H_real: 0.0403 Val H_fake: -0.0509 Val Loss G_H: 0.7272 Val Loss G_Z: 0.7344\n",
      "epoch: 40 Val H_real: 0.0764 Val H_fake: -0.1071 Val Loss G_H: 0.7574 Val Loss G_Z: 0.7825\n",
      "epoch: 60 Val H_real: 0.0574 Val H_fake: -0.1477 Val Loss G_H: 0.7798 Val Loss G_Z: 0.8249\n",
      "epoch: 80 Val H_real: -0.0948 Val H_fake: -0.0455 Val Loss G_H: 0.7255 Val Loss G_Z: 0.7372\n",
      "epoch: 100 Val H_real: -0.1119 Val H_fake: -0.0583 Val Loss G_H: 0.7317 Val Loss G_Z: 0.7553\n",
      "epoch: 120 Val H_real: -0.1105 Val H_fake: -0.0839 Val Loss G_H: 0.7450 Val Loss G_Z: 0.7773\n",
      "epoch: 140 Val H_real: -0.1352 Val H_fake: -0.1151 Val Loss G_H: 0.7620 Val Loss G_Z: 0.7991\n",
      "epoch: 160 Val H_real: -0.1967 Val H_fake: -0.1821 Val Loss G_H: 0.7987 Val Loss G_Z: 0.8320\n",
      "epoch: 180 Val H_real: -0.2552 Val H_fake: -0.2509 Val Loss G_H: 0.8371 Val Loss G_Z: 0.8639\n",
      "epoch: 200 Val H_real: -0.3036 Val H_fake: -0.3053 Val Loss G_H: 0.8685 Val Loss G_Z: 0.8899\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/20/fold5/generator.pth\n",
      "epoch: 0 Val H_real: 0.0190 Val H_fake: 0.0196 Val Loss G_H: 0.6928 Val Loss G_Z: 0.7628\n",
      "epoch: 20 Val H_real: 0.0212 Val H_fake: 0.0036 Val Loss G_H: 0.6989 Val Loss G_Z: 0.7096\n",
      "epoch: 40 Val H_real: -0.0034 Val H_fake: -0.0003 Val Loss G_H: 0.7004 Val Loss G_Z: 0.7103\n",
      "epoch: 60 Val H_real: -0.0289 Val H_fake: -0.0105 Val Loss G_H: 0.7054 Val Loss G_Z: 0.7107\n",
      "epoch: 80 Val H_real: -0.0485 Val H_fake: -0.0278 Val Loss G_H: 0.7141 Val Loss G_Z: 0.7154\n",
      "epoch: 100 Val H_real: -0.0652 Val H_fake: -0.0396 Val Loss G_H: 0.7204 Val Loss G_Z: 0.7209\n",
      "epoch: 120 Val H_real: -0.0719 Val H_fake: -0.0422 Val Loss G_H: 0.7222 Val Loss G_Z: 0.7293\n",
      "epoch: 140 Val H_real: -0.0851 Val H_fake: -0.0472 Val Loss G_H: 0.7254 Val Loss G_Z: 0.7382\n",
      "epoch: 160 Val H_real: -0.1113 Val H_fake: -0.0733 Val Loss G_H: 0.7397 Val Loss G_Z: 0.7500\n",
      "epoch: 180 Val H_real: -0.1296 Val H_fake: -0.0839 Val Loss G_H: 0.7460 Val Loss G_Z: 0.7637\n",
      "epoch: 200 Val H_real: -0.1693 Val H_fake: -0.1342 Val Loss G_H: 0.7737 Val Loss G_Z: 0.7772\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/30/fold1/generator.pth\n",
      "epoch: 0 Val H_real: -0.0011 Val H_fake: -0.0080 Val Loss G_H: 0.7053 Val Loss G_Z: 0.7211\n",
      "epoch: 20 Val H_real: -0.0234 Val H_fake: -0.0364 Val Loss G_H: 0.7183 Val Loss G_Z: 0.7074\n",
      "epoch: 40 Val H_real: -0.0412 Val H_fake: -0.0463 Val Loss G_H: 0.7230 Val Loss G_Z: 0.7126\n",
      "epoch: 60 Val H_real: -0.0621 Val H_fake: -0.0551 Val Loss G_H: 0.7275 Val Loss G_Z: 0.7192\n",
      "epoch: 80 Val H_real: -0.0767 Val H_fake: -0.0626 Val Loss G_H: 0.7316 Val Loss G_Z: 0.7261\n",
      "epoch: 100 Val H_real: -0.0926 Val H_fake: -0.0706 Val Loss G_H: 0.7362 Val Loss G_Z: 0.7331\n",
      "epoch: 120 Val H_real: -0.1110 Val H_fake: -0.0870 Val Loss G_H: 0.7454 Val Loss G_Z: 0.7394\n",
      "epoch: 140 Val H_real: -0.1296 Val H_fake: -0.1007 Val Loss G_H: 0.7532 Val Loss G_Z: 0.7478\n",
      "epoch: 160 Val H_real: -0.1498 Val H_fake: -0.1262 Val Loss G_H: 0.7678 Val Loss G_Z: 0.7573\n",
      "epoch: 180 Val H_real: -0.1815 Val H_fake: -0.1616 Val Loss G_H: 0.7878 Val Loss G_Z: 0.7648\n",
      "epoch: 200 Val H_real: -0.2149 Val H_fake: -0.1967 Val Loss G_H: 0.8081 Val Loss G_Z: 0.7806\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/30/fold2/generator.pth\n",
      "epoch: 0 Val H_real: -0.1192 Val H_fake: -0.1170 Val Loss G_H: 0.7627 Val Loss G_Z: 0.6614\n",
      "epoch: 20 Val H_real: -0.0260 Val H_fake: -0.0308 Val Loss G_H: 0.7155 Val Loss G_Z: 0.6956\n",
      "epoch: 40 Val H_real: -0.0409 Val H_fake: -0.0526 Val Loss G_H: 0.7263 Val Loss G_Z: 0.6975\n",
      "epoch: 60 Val H_real: -0.0508 Val H_fake: -0.0607 Val Loss G_H: 0.7308 Val Loss G_Z: 0.7033\n",
      "epoch: 80 Val H_real: -0.0498 Val H_fake: -0.0605 Val Loss G_H: 0.7312 Val Loss G_Z: 0.7154\n",
      "epoch: 100 Val H_real: -0.0544 Val H_fake: -0.0653 Val Loss G_H: 0.7345 Val Loss G_Z: 0.7235\n",
      "epoch: 120 Val H_real: -0.0696 Val H_fake: -0.0835 Val Loss G_H: 0.7453 Val Loss G_Z: 0.7363\n",
      "epoch: 140 Val H_real: -0.0868 Val H_fake: -0.1053 Val Loss G_H: 0.7580 Val Loss G_Z: 0.7500\n",
      "epoch: 160 Val H_real: -0.0970 Val H_fake: -0.1371 Val Loss G_H: 0.7769 Val Loss G_Z: 0.7640\n",
      "epoch: 180 Val H_real: -0.1269 Val H_fake: -0.1908 Val Loss G_H: 0.8085 Val Loss G_Z: 0.7824\n",
      "epoch: 200 Val H_real: -0.1715 Val H_fake: -0.2501 Val Loss G_H: 0.8437 Val Loss G_Z: 0.8002\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/30/fold3/generator.pth\n",
      "epoch: 0 Val H_real: 0.0538 Val H_fake: 0.0386 Val Loss G_H: 0.6827 Val Loss G_Z: 0.6839\n",
      "epoch: 20 Val H_real: 0.0151 Val H_fake: -0.0600 Val Loss G_H: 0.7315 Val Loss G_Z: 0.7269\n",
      "epoch: 40 Val H_real: 0.0150 Val H_fake: -0.0799 Val Loss G_H: 0.7418 Val Loss G_Z: 0.7417\n",
      "epoch: 60 Val H_real: -0.0659 Val H_fake: -0.0238 Val Loss G_H: 0.7132 Val Loss G_Z: 0.7039\n",
      "epoch: 80 Val H_real: -0.0923 Val H_fake: -0.0415 Val Loss G_H: 0.7225 Val Loss G_Z: 0.7061\n",
      "epoch: 100 Val H_real: -0.1071 Val H_fake: -0.0626 Val Loss G_H: 0.7335 Val Loss G_Z: 0.7201\n",
      "epoch: 120 Val H_real: -0.1277 Val H_fake: -0.0927 Val Loss G_H: 0.7496 Val Loss G_Z: 0.7401\n",
      "epoch: 140 Val H_real: -0.1544 Val H_fake: -0.1268 Val Loss G_H: 0.7681 Val Loss G_Z: 0.7572\n",
      "epoch: 160 Val H_real: -0.2068 Val H_fake: -0.1717 Val Loss G_H: 0.7929 Val Loss G_Z: 0.7884\n",
      "epoch: 180 Val H_real: -0.2828 Val H_fake: -0.2340 Val Loss G_H: 0.8283 Val Loss G_Z: 0.8258\n",
      "epoch: 200 Val H_real: -0.3313 Val H_fake: -0.2903 Val Loss G_H: 0.8613 Val Loss G_Z: 0.8522\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/30/fold4/generator.pth\n",
      "epoch: 0 Val H_real: -0.0710 Val H_fake: -0.0851 Val Loss G_H: 0.7461 Val Loss G_Z: 0.6674\n",
      "epoch: 20 Val H_real: -0.0098 Val H_fake: -0.0435 Val Loss G_H: 0.7218 Val Loss G_Z: 0.7151\n",
      "epoch: 40 Val H_real: -0.0418 Val H_fake: -0.0529 Val Loss G_H: 0.7262 Val Loss G_Z: 0.7119\n",
      "epoch: 60 Val H_real: -0.0792 Val H_fake: -0.0469 Val Loss G_H: 0.7236 Val Loss G_Z: 0.7103\n",
      "epoch: 80 Val H_real: -0.0996 Val H_fake: -0.0512 Val Loss G_H: 0.7265 Val Loss G_Z: 0.7140\n",
      "epoch: 100 Val H_real: -0.0938 Val H_fake: -0.0603 Val Loss G_H: 0.7320 Val Loss G_Z: 0.7207\n",
      "epoch: 120 Val H_real: -0.0826 Val H_fake: -0.0756 Val Loss G_H: 0.7408 Val Loss G_Z: 0.7260\n",
      "epoch: 140 Val H_real: -0.0913 Val H_fake: -0.0866 Val Loss G_H: 0.7477 Val Loss G_Z: 0.7349\n",
      "epoch: 160 Val H_real: -0.1346 Val H_fake: -0.1289 Val Loss G_H: 0.7714 Val Loss G_Z: 0.7538\n",
      "epoch: 180 Val H_real: -0.2099 Val H_fake: -0.1978 Val Loss G_H: 0.8102 Val Loss G_Z: 0.7834\n",
      "epoch: 200 Val H_real: -0.2997 Val H_fake: -0.2828 Val Loss G_H: 0.8586 Val Loss G_Z: 0.8208\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/30/fold5/generator.pth\n",
      "epoch: 0 Val H_real: -0.0063 Val H_fake: -0.0256 Val Loss G_H: 0.7153 Val Loss G_Z: 0.6903\n",
      "epoch: 20 Val H_real: 0.0691 Val H_fake: -0.1033 Val Loss G_H: 0.7543 Val Loss G_Z: 0.7905\n",
      "epoch: 40 Val H_real: 0.1873 Val H_fake: -0.2679 Val Loss G_H: 0.8456 Val Loss G_Z: 0.9951\n",
      "epoch: 60 Val H_real: 0.4700 Val H_fake: -0.5981 Val Loss G_H: 1.0490 Val Loss G_Z: 1.3415\n",
      "epoch: 80 Val H_real: 0.0414 Val H_fake: 0.0025 Val Loss G_H: 0.7088 Val Loss G_Z: 0.6600\n",
      "epoch: 100 Val H_real: 0.0887 Val H_fake: -0.1017 Val Loss G_H: 0.7632 Val Loss G_Z: 0.7246\n",
      "epoch: 120 Val H_real: 0.1281 Val H_fake: -0.2281 Val Loss G_H: 0.8351 Val Loss G_Z: 0.7715\n",
      "epoch: 140 Val H_real: 0.1629 Val H_fake: -0.3738 Val Loss G_H: 0.9253 Val Loss G_Z: 0.7975\n",
      "epoch: 160 Val H_real: 0.1835 Val H_fake: -0.5566 Val Loss G_H: 1.0470 Val Loss G_Z: 0.8573\n",
      "epoch: 180 Val H_real: 0.1727 Val H_fake: -0.7238 Val Loss G_H: 1.1665 Val Loss G_Z: 0.9369\n",
      "epoch: 200 Val H_real: 0.1583 Val H_fake: -0.9648 Val Loss G_H: 1.3426 Val Loss G_Z: 1.0397\n",
      "epoch: 220 Val H_real: 0.1399 Val H_fake: -1.2977 Val Loss G_H: 1.6061 Val Loss G_Z: 1.1431\n",
      "epoch: 240 Val H_real: 0.2070 Val H_fake: -1.7326 Val Loss G_H: 1.9606 Val Loss G_Z: 1.2770\n",
      "epoch: 260 Val H_real: 0.2471 Val H_fake: -2.4310 Val Loss G_H: 2.5768 Val Loss G_Z: 1.4713\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/40/fold1/generator.pth\n",
      "epoch: 0 Val H_real: 0.2693 Val H_fake: 0.2573 Val Loss G_H: 0.5839 Val Loss G_Z: 0.6884\n",
      "epoch: 20 Val H_real: 0.0758 Val H_fake: -0.0364 Val Loss G_H: 0.7193 Val Loss G_Z: 0.7232\n",
      "epoch: 40 Val H_real: 0.0873 Val H_fake: -0.0463 Val Loss G_H: 0.7243 Val Loss G_Z: 0.7246\n",
      "epoch: 60 Val H_real: 0.0341 Val H_fake: -0.0433 Val Loss G_H: 0.7227 Val Loss G_Z: 0.7137\n",
      "epoch: 80 Val H_real: -0.0019 Val H_fake: -0.0463 Val Loss G_H: 0.7244 Val Loss G_Z: 0.7168\n",
      "epoch: 100 Val H_real: -0.0059 Val H_fake: -0.0501 Val Loss G_H: 0.7271 Val Loss G_Z: 0.7299\n",
      "epoch: 120 Val H_real: 0.0101 Val H_fake: -0.0522 Val Loss G_H: 0.7294 Val Loss G_Z: 0.7380\n",
      "epoch: 140 Val H_real: 0.0183 Val H_fake: -0.0591 Val Loss G_H: 0.7346 Val Loss G_Z: 0.7455\n",
      "epoch: 160 Val H_real: -0.0192 Val H_fake: -0.0671 Val Loss G_H: 0.7408 Val Loss G_Z: 0.7551\n",
      "epoch: 180 Val H_real: -0.1013 Val H_fake: -0.1244 Val Loss G_H: 0.7727 Val Loss G_Z: 0.7740\n",
      "epoch: 200 Val H_real: -0.1903 Val H_fake: -0.1862 Val Loss G_H: 0.8084 Val Loss G_Z: 0.8006\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/40/fold2/generator.pth\n",
      "epoch: 0 Val H_real: 0.1644 Val H_fake: 0.1629 Val Loss G_H: 0.6236 Val Loss G_Z: 0.7190\n",
      "epoch: 20 Val H_real: -0.0004 Val H_fake: -0.0279 Val Loss G_H: 0.7146 Val Loss G_Z: 0.7027\n",
      "epoch: 40 Val H_real: -0.0309 Val H_fake: -0.0438 Val Loss G_H: 0.7224 Val Loss G_Z: 0.7021\n",
      "epoch: 60 Val H_real: -0.0682 Val H_fake: -0.0472 Val Loss G_H: 0.7242 Val Loss G_Z: 0.7077\n",
      "epoch: 80 Val H_real: -0.0890 Val H_fake: -0.0519 Val Loss G_H: 0.7268 Val Loss G_Z: 0.7227\n",
      "epoch: 100 Val H_real: -0.0974 Val H_fake: -0.0458 Val Loss G_H: 0.7238 Val Loss G_Z: 0.7351\n",
      "epoch: 120 Val H_real: -0.1093 Val H_fake: -0.0471 Val Loss G_H: 0.7250 Val Loss G_Z: 0.7432\n",
      "epoch: 140 Val H_real: -0.1260 Val H_fake: -0.0638 Val Loss G_H: 0.7342 Val Loss G_Z: 0.7505\n",
      "epoch: 160 Val H_real: -0.1548 Val H_fake: -0.0867 Val Loss G_H: 0.7470 Val Loss G_Z: 0.7627\n",
      "epoch: 180 Val H_real: -0.1778 Val H_fake: -0.1158 Val Loss G_H: 0.7634 Val Loss G_Z: 0.7747\n",
      "epoch: 200 Val H_real: -0.2228 Val H_fake: -0.1621 Val Loss G_H: 0.7895 Val Loss G_Z: 0.7887\n",
      "epoch: 220 Val H_real: -0.2421 Val H_fake: -0.2081 Val Loss G_H: 0.8164 Val Loss G_Z: 0.8103\n",
      "epoch: 240 Val H_real: -0.3207 Val H_fake: -0.2628 Val Loss G_H: 0.8487 Val Loss G_Z: 0.8422\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/40/fold3/generator.pth\n",
      "epoch: 0 Val H_real: -0.1620 Val H_fake: -0.1442 Val Loss G_H: 0.7767 Val Loss G_Z: 0.6515\n",
      "epoch: 20 Val H_real: -0.0401 Val H_fake: -0.0255 Val Loss G_H: 0.7130 Val Loss G_Z: 0.6984\n",
      "epoch: 40 Val H_real: -0.0650 Val H_fake: -0.0362 Val Loss G_H: 0.7184 Val Loss G_Z: 0.6911\n",
      "epoch: 60 Val H_real: -0.0848 Val H_fake: -0.0315 Val Loss G_H: 0.7159 Val Loss G_Z: 0.6874\n",
      "epoch: 80 Val H_real: -0.0940 Val H_fake: -0.0351 Val Loss G_H: 0.7185 Val Loss G_Z: 0.6943\n",
      "epoch: 100 Val H_real: -0.0942 Val H_fake: -0.0442 Val Loss G_H: 0.7239 Val Loss G_Z: 0.7101\n",
      "epoch: 120 Val H_real: -0.1032 Val H_fake: -0.0575 Val Loss G_H: 0.7319 Val Loss G_Z: 0.7279\n",
      "epoch: 140 Val H_real: -0.1359 Val H_fake: -0.0888 Val Loss G_H: 0.7493 Val Loss G_Z: 0.7411\n",
      "epoch: 160 Val H_real: -0.1910 Val H_fake: -0.1403 Val Loss G_H: 0.7780 Val Loss G_Z: 0.7635\n",
      "epoch: 180 Val H_real: -0.2677 Val H_fake: -0.2266 Val Loss G_H: 0.8263 Val Loss G_Z: 0.8009\n",
      "epoch: 200 Val H_real: -0.3116 Val H_fake: -0.2875 Val Loss G_H: 0.8618 Val Loss G_Z: 0.8294\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/40/fold4/generator.pth\n",
      "epoch: 0 Val H_real: -0.0287 Val H_fake: -0.0252 Val Loss G_H: 0.7162 Val Loss G_Z: 0.6760\n",
      "epoch: 20 Val H_real: -0.0355 Val H_fake: -0.0513 Val Loss G_H: 0.7262 Val Loss G_Z: 0.6919\n",
      "epoch: 40 Val H_real: -0.0404 Val H_fake: -0.0462 Val Loss G_H: 0.7233 Val Loss G_Z: 0.7033\n",
      "epoch: 60 Val H_real: -0.0508 Val H_fake: -0.0366 Val Loss G_H: 0.7184 Val Loss G_Z: 0.7127\n",
      "epoch: 80 Val H_real: -0.0652 Val H_fake: -0.0299 Val Loss G_H: 0.7153 Val Loss G_Z: 0.7193\n",
      "epoch: 100 Val H_real: -0.0826 Val H_fake: -0.0430 Val Loss G_H: 0.7225 Val Loss G_Z: 0.7259\n",
      "epoch: 120 Val H_real: -0.1012 Val H_fake: -0.0539 Val Loss G_H: 0.7291 Val Loss G_Z: 0.7337\n",
      "epoch: 140 Val H_real: -0.1152 Val H_fake: -0.0643 Val Loss G_H: 0.7353 Val Loss G_Z: 0.7415\n",
      "epoch: 160 Val H_real: -0.1289 Val H_fake: -0.0768 Val Loss G_H: 0.7428 Val Loss G_Z: 0.7500\n",
      "epoch: 180 Val H_real: -0.1637 Val H_fake: -0.1330 Val Loss G_H: 0.7736 Val Loss G_Z: 0.7596\n",
      "epoch: 200 Val H_real: -0.2085 Val H_fake: -0.1675 Val Loss G_H: 0.7940 Val Loss G_Z: 0.7681\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/1/40/fold5/generator.pth\n",
      "channels:2\n",
      "epoch: 0 Val H_real: -0.0217 Val H_fake: -0.0258 Val Loss G_H: 0.7151 Val Loss G_Z: 0.6979\n",
      "epoch: 20 Val H_real: -0.0067 Val H_fake: -0.0139 Val Loss G_H: 0.7076 Val Loss G_Z: 0.7095\n",
      "epoch: 40 Val H_real: -0.0342 Val H_fake: -0.0274 Val Loss G_H: 0.7139 Val Loss G_Z: 0.7135\n",
      "epoch: 60 Val H_real: -0.0868 Val H_fake: -0.0755 Val Loss G_H: 0.7379 Val Loss G_Z: 0.7406\n",
      "epoch: 80 Val H_real: -0.1005 Val H_fake: -0.0875 Val Loss G_H: 0.7437 Val Loss G_Z: 0.7442\n",
      "epoch: 100 Val H_real: -0.1105 Val H_fake: -0.0996 Val Loss G_H: 0.7497 Val Loss G_Z: 0.7507\n",
      "epoch: 120 Val H_real: -0.1173 Val H_fake: -0.1083 Val Loss G_H: 0.7540 Val Loss G_Z: 0.7558\n",
      "epoch: 140 Val H_real: -0.1227 Val H_fake: -0.1143 Val Loss G_H: 0.7569 Val Loss G_Z: 0.7613\n",
      "epoch: 160 Val H_real: -0.1246 Val H_fake: -0.1187 Val Loss G_H: 0.7591 Val Loss G_Z: 0.7656\n",
      "epoch: 180 Val H_real: -0.1282 Val H_fake: -0.1242 Val Loss G_H: 0.7620 Val Loss G_Z: 0.7691\n",
      "epoch: 200 Val H_real: -0.1327 Val H_fake: -0.1291 Val Loss G_H: 0.7646 Val Loss G_Z: 0.7714\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/10/fold1/generator.pth\n",
      "epoch: 0 Val H_real: 0.0870 Val H_fake: 0.0582 Val Loss G_H: 0.6727 Val Loss G_Z: 0.7257\n",
      "epoch: 20 Val H_real: -0.0189 Val H_fake: -0.0138 Val Loss G_H: 0.7072 Val Loss G_Z: 0.7093\n",
      "epoch: 40 Val H_real: -0.0378 Val H_fake: -0.0290 Val Loss G_H: 0.7140 Val Loss G_Z: 0.7164\n",
      "epoch: 60 Val H_real: -0.0517 Val H_fake: -0.0423 Val Loss G_H: 0.7204 Val Loss G_Z: 0.7230\n",
      "epoch: 80 Val H_real: -0.0690 Val H_fake: -0.0603 Val Loss G_H: 0.7294 Val Loss G_Z: 0.7288\n",
      "epoch: 100 Val H_real: -0.0769 Val H_fake: -0.0697 Val Loss G_H: 0.7339 Val Loss G_Z: 0.7332\n",
      "epoch: 120 Val H_real: -0.0862 Val H_fake: -0.0803 Val Loss G_H: 0.7393 Val Loss G_Z: 0.7361\n",
      "epoch: 140 Val H_real: -0.0922 Val H_fake: -0.0882 Val Loss G_H: 0.7432 Val Loss G_Z: 0.7404\n",
      "epoch: 160 Val H_real: -0.0970 Val H_fake: -0.0941 Val Loss G_H: 0.7463 Val Loss G_Z: 0.7439\n",
      "epoch: 180 Val H_real: -0.1003 Val H_fake: -0.0977 Val Loss G_H: 0.7481 Val Loss G_Z: 0.7480\n",
      "epoch: 200 Val H_real: -0.1067 Val H_fake: -0.1059 Val Loss G_H: 0.7525 Val Loss G_Z: 0.7508\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/10/fold2/generator.pth\n",
      "epoch: 0 Val H_real: -0.0462 Val H_fake: -0.0475 Val Loss G_H: 0.7256 Val Loss G_Z: 0.6866\n",
      "epoch: 20 Val H_real: -0.0166 Val H_fake: -0.0098 Val Loss G_H: 0.7047 Val Loss G_Z: 0.7129\n",
      "epoch: 40 Val H_real: -0.0414 Val H_fake: -0.0352 Val Loss G_H: 0.7170 Val Loss G_Z: 0.7241\n",
      "epoch: 60 Val H_real: -0.0560 Val H_fake: -0.0477 Val Loss G_H: 0.7229 Val Loss G_Z: 0.7276\n",
      "epoch: 80 Val H_real: -0.0644 Val H_fake: -0.0568 Val Loss G_H: 0.7273 Val Loss G_Z: 0.7323\n",
      "epoch: 100 Val H_real: -0.0740 Val H_fake: -0.0666 Val Loss G_H: 0.7323 Val Loss G_Z: 0.7384\n",
      "epoch: 120 Val H_real: -0.0816 Val H_fake: -0.0748 Val Loss G_H: 0.7364 Val Loss G_Z: 0.7428\n",
      "epoch: 140 Val H_real: -0.0905 Val H_fake: -0.0844 Val Loss G_H: 0.7412 Val Loss G_Z: 0.7463\n",
      "epoch: 160 Val H_real: -0.0934 Val H_fake: -0.0882 Val Loss G_H: 0.7431 Val Loss G_Z: 0.7480\n",
      "epoch: 180 Val H_real: -0.1008 Val H_fake: -0.0966 Val Loss G_H: 0.7475 Val Loss G_Z: 0.7510\n",
      "epoch: 200 Val H_real: -0.1102 Val H_fake: -0.1057 Val Loss G_H: 0.7523 Val Loss G_Z: 0.7558\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/10/fold3/generator.pth\n",
      "epoch: 0 Val H_real: 0.1136 Val H_fake: 0.1105 Val Loss G_H: 0.6488 Val Loss G_Z: 0.7192\n",
      "epoch: 20 Val H_real: -0.0149 Val H_fake: -0.0178 Val Loss G_H: 0.7094 Val Loss G_Z: 0.7118\n",
      "epoch: 40 Val H_real: -0.0379 Val H_fake: -0.0359 Val Loss G_H: 0.7180 Val Loss G_Z: 0.7221\n",
      "epoch: 60 Val H_real: -0.0912 Val H_fake: -0.0765 Val Loss G_H: 0.7384 Val Loss G_Z: 0.7438\n",
      "epoch: 80 Val H_real: -0.1003 Val H_fake: -0.0844 Val Loss G_H: 0.7422 Val Loss G_Z: 0.7475\n",
      "epoch: 100 Val H_real: -0.1073 Val H_fake: -0.0923 Val Loss G_H: 0.7460 Val Loss G_Z: 0.7531\n",
      "epoch: 120 Val H_real: -0.1128 Val H_fake: -0.1021 Val Loss G_H: 0.7509 Val Loss G_Z: 0.7566\n",
      "epoch: 140 Val H_real: -0.1203 Val H_fake: -0.1125 Val Loss G_H: 0.7563 Val Loss G_Z: 0.7597\n",
      "epoch: 160 Val H_real: -0.1255 Val H_fake: -0.1196 Val Loss G_H: 0.7598 Val Loss G_Z: 0.7612\n",
      "epoch: 180 Val H_real: -0.1299 Val H_fake: -0.1250 Val Loss G_H: 0.7626 Val Loss G_Z: 0.7641\n",
      "epoch: 200 Val H_real: -0.1327 Val H_fake: -0.1294 Val Loss G_H: 0.7649 Val Loss G_Z: 0.7655\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/10/fold4/generator.pth\n",
      "epoch: 0 Val H_real: 0.0015 Val H_fake: -0.0113 Val Loss G_H: 0.7074 Val Loss G_Z: 0.6513\n",
      "epoch: 20 Val H_real: -0.0200 Val H_fake: -0.0167 Val Loss G_H: 0.7094 Val Loss G_Z: 0.7049\n",
      "epoch: 40 Val H_real: -0.0453 Val H_fake: -0.0377 Val Loss G_H: 0.7192 Val Loss G_Z: 0.7142\n",
      "epoch: 60 Val H_real: -0.0684 Val H_fake: -0.0580 Val Loss G_H: 0.7290 Val Loss G_Z: 0.7238\n",
      "epoch: 80 Val H_real: -0.0842 Val H_fake: -0.0720 Val Loss G_H: 0.7358 Val Loss G_Z: 0.7320\n",
      "epoch: 100 Val H_real: -0.0962 Val H_fake: -0.0832 Val Loss G_H: 0.7413 Val Loss G_Z: 0.7383\n",
      "epoch: 120 Val H_real: -0.1010 Val H_fake: -0.0917 Val Loss G_H: 0.7456 Val Loss G_Z: 0.7428\n",
      "epoch: 140 Val H_real: -0.1044 Val H_fake: -0.0980 Val Loss G_H: 0.7488 Val Loss G_Z: 0.7466\n",
      "epoch: 160 Val H_real: -0.1105 Val H_fake: -0.1074 Val Loss G_H: 0.7537 Val Loss G_Z: 0.7510\n",
      "epoch: 180 Val H_real: -0.1156 Val H_fake: -0.1131 Val Loss G_H: 0.7565 Val Loss G_Z: 0.7559\n",
      "epoch: 200 Val H_real: -0.1180 Val H_fake: -0.1175 Val Loss G_H: 0.7589 Val Loss G_Z: 0.7587\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/10/fold5/generator.pth\n",
      "epoch: 0 Val H_real: 0.1013 Val H_fake: 0.0795 Val Loss G_H: 0.6631 Val Loss G_Z: 0.8176\n",
      "epoch: 20 Val H_real: -0.0097 Val H_fake: -0.0077 Val Loss G_H: 0.7043 Val Loss G_Z: 0.7058\n",
      "epoch: 40 Val H_real: -0.0230 Val H_fake: -0.0111 Val Loss G_H: 0.7056 Val Loss G_Z: 0.7062\n",
      "epoch: 60 Val H_real: -0.0449 Val H_fake: -0.0197 Val Loss G_H: 0.7098 Val Loss G_Z: 0.7090\n",
      "epoch: 80 Val H_real: -0.0831 Val H_fake: -0.0499 Val Loss G_H: 0.7250 Val Loss G_Z: 0.7199\n",
      "epoch: 100 Val H_real: -0.1191 Val H_fake: -0.0845 Val Loss G_H: 0.7428 Val Loss G_Z: 0.7411\n",
      "epoch: 120 Val H_real: -0.1423 Val H_fake: -0.1097 Val Loss G_H: 0.7561 Val Loss G_Z: 0.7594\n",
      "epoch: 140 Val H_real: -0.1645 Val H_fake: -0.1333 Val Loss G_H: 0.7687 Val Loss G_Z: 0.7773\n",
      "epoch: 160 Val H_real: -0.1915 Val H_fake: -0.1641 Val Loss G_H: 0.7855 Val Loss G_Z: 0.7950\n",
      "epoch: 180 Val H_real: -0.2126 Val H_fake: -0.1862 Val Loss G_H: 0.7975 Val Loss G_Z: 0.8128\n",
      "epoch: 200 Val H_real: -0.2396 Val H_fake: -0.2183 Val Loss G_H: 0.8153 Val Loss G_Z: 0.8255\n",
      "epoch: 220 Val H_real: -0.2560 Val H_fake: -0.2370 Val Loss G_H: 0.8255 Val Loss G_Z: 0.8372\n",
      "epoch: 240 Val H_real: -0.2680 Val H_fake: -0.2538 Val Loss G_H: 0.8350 Val Loss G_Z: 0.8472\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/20/fold1/generator.pth\n",
      "epoch: 0 Val H_real: 0.1447 Val H_fake: 0.1387 Val Loss G_H: 0.6350 Val Loss G_Z: 0.7250\n",
      "epoch: 20 Val H_real: -0.0143 Val H_fake: -0.0123 Val Loss G_H: 0.7069 Val Loss G_Z: 0.7062\n",
      "epoch: 40 Val H_real: -0.0248 Val H_fake: -0.0182 Val Loss G_H: 0.7093 Val Loss G_Z: 0.7076\n",
      "epoch: 60 Val H_real: -0.0378 Val H_fake: -0.0234 Val Loss G_H: 0.7122 Val Loss G_Z: 0.7103\n",
      "epoch: 80 Val H_real: -0.0843 Val H_fake: -0.0501 Val Loss G_H: 0.7260 Val Loss G_Z: 0.7232\n",
      "epoch: 100 Val H_real: -0.1578 Val H_fake: -0.1153 Val Loss G_H: 0.7597 Val Loss G_Z: 0.7576\n",
      "epoch: 120 Val H_real: -0.1780 Val H_fake: -0.1429 Val Loss G_H: 0.7742 Val Loss G_Z: 0.7709\n",
      "epoch: 140 Val H_real: -0.1866 Val H_fake: -0.1570 Val Loss G_H: 0.7817 Val Loss G_Z: 0.7809\n",
      "epoch: 160 Val H_real: -0.2085 Val H_fake: -0.1824 Val Loss G_H: 0.7955 Val Loss G_Z: 0.8003\n",
      "epoch: 180 Val H_real: -0.2430 Val H_fake: -0.2190 Val Loss G_H: 0.8156 Val Loss G_Z: 0.8211\n",
      "epoch: 200 Val H_real: -0.2640 Val H_fake: -0.2459 Val Loss G_H: 0.8305 Val Loss G_Z: 0.8367\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/20/fold2/generator.pth\n",
      "epoch: 0 Val H_real: -0.0235 Val H_fake: -0.0380 Val Loss G_H: 0.7219 Val Loss G_Z: 0.7989\n",
      "epoch: 20 Val H_real: -0.0115 Val H_fake: -0.0051 Val Loss G_H: 0.7026 Val Loss G_Z: 0.7123\n",
      "epoch: 40 Val H_real: -0.0352 Val H_fake: -0.0189 Val Loss G_H: 0.7090 Val Loss G_Z: 0.7109\n",
      "epoch: 60 Val H_real: -0.0631 Val H_fake: -0.0441 Val Loss G_H: 0.7215 Val Loss G_Z: 0.7207\n",
      "epoch: 80 Val H_real: -0.0761 Val H_fake: -0.0596 Val Loss G_H: 0.7291 Val Loss G_Z: 0.7294\n",
      "epoch: 100 Val H_real: -0.0848 Val H_fake: -0.0715 Val Loss G_H: 0.7352 Val Loss G_Z: 0.7359\n",
      "epoch: 120 Val H_real: -0.0936 Val H_fake: -0.0821 Val Loss G_H: 0.7407 Val Loss G_Z: 0.7435\n",
      "epoch: 140 Val H_real: -0.1053 Val H_fake: -0.0924 Val Loss G_H: 0.7461 Val Loss G_Z: 0.7499\n",
      "epoch: 160 Val H_real: -0.1137 Val H_fake: -0.1007 Val Loss G_H: 0.7505 Val Loss G_Z: 0.7546\n",
      "epoch: 180 Val H_real: -0.1192 Val H_fake: -0.1097 Val Loss G_H: 0.7554 Val Loss G_Z: 0.7593\n",
      "epoch: 200 Val H_real: -0.1272 Val H_fake: -0.1189 Val Loss G_H: 0.7603 Val Loss G_Z: 0.7639\n",
      "epoch: 220 Val H_real: -0.1365 Val H_fake: -0.1286 Val Loss G_H: 0.7657 Val Loss G_Z: 0.7707\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/20/fold3/generator.pth\n",
      "epoch: 0 Val H_real: -0.0515 Val H_fake: -0.0561 Val Loss G_H: 0.7301 Val Loss G_Z: 0.7223\n",
      "epoch: 20 Val H_real: -0.0170 Val H_fake: -0.0082 Val Loss G_H: 0.7043 Val Loss G_Z: 0.7049\n",
      "epoch: 40 Val H_real: -0.0364 Val H_fake: -0.0195 Val Loss G_H: 0.7092 Val Loss G_Z: 0.7070\n",
      "epoch: 60 Val H_real: -0.0535 Val H_fake: -0.0374 Val Loss G_H: 0.7179 Val Loss G_Z: 0.7123\n",
      "epoch: 80 Val H_real: -0.0644 Val H_fake: -0.0506 Val Loss G_H: 0.7245 Val Loss G_Z: 0.7193\n",
      "epoch: 100 Val H_real: -0.0821 Val H_fake: -0.0693 Val Loss G_H: 0.7342 Val Loss G_Z: 0.7287\n",
      "epoch: 120 Val H_real: -0.1057 Val H_fake: -0.0911 Val Loss G_H: 0.7454 Val Loss G_Z: 0.7443\n",
      "epoch: 140 Val H_real: -0.1325 Val H_fake: -0.1173 Val Loss G_H: 0.7592 Val Loss G_Z: 0.7581\n",
      "epoch: 160 Val H_real: -0.1410 Val H_fake: -0.1288 Val Loss G_H: 0.7652 Val Loss G_Z: 0.7641\n",
      "epoch: 180 Val H_real: -0.1464 Val H_fake: -0.1338 Val Loss G_H: 0.7680 Val Loss G_Z: 0.7705\n",
      "epoch: 200 Val H_real: -0.1571 Val H_fake: -0.1471 Val Loss G_H: 0.7752 Val Loss G_Z: 0.7744\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/20/fold4/generator.pth\n",
      "epoch: 0 Val H_real: -0.0608 Val H_fake: -0.0650 Val Loss G_H: 0.7344 Val Loss G_Z: 0.7138\n",
      "epoch: 20 Val H_real: -0.0220 Val H_fake: -0.0140 Val Loss G_H: 0.7071 Val Loss G_Z: 0.7082\n",
      "epoch: 40 Val H_real: -0.0350 Val H_fake: -0.0193 Val Loss G_H: 0.7092 Val Loss G_Z: 0.7097\n",
      "epoch: 60 Val H_real: -0.0470 Val H_fake: -0.0293 Val Loss G_H: 0.7141 Val Loss G_Z: 0.7121\n",
      "epoch: 80 Val H_real: -0.0563 Val H_fake: -0.0381 Val Loss G_H: 0.7186 Val Loss G_Z: 0.7170\n",
      "epoch: 100 Val H_real: -0.0730 Val H_fake: -0.0516 Val Loss G_H: 0.7256 Val Loss G_Z: 0.7284\n",
      "epoch: 120 Val H_real: -0.1037 Val H_fake: -0.0836 Val Loss G_H: 0.7421 Val Loss G_Z: 0.7471\n",
      "epoch: 140 Val H_real: -0.1299 Val H_fake: -0.1122 Val Loss G_H: 0.7572 Val Loss G_Z: 0.7655\n",
      "epoch: 160 Val H_real: -0.1395 Val H_fake: -0.1253 Val Loss G_H: 0.7640 Val Loss G_Z: 0.7713\n",
      "epoch: 180 Val H_real: -0.1449 Val H_fake: -0.1331 Val Loss G_H: 0.7681 Val Loss G_Z: 0.7748\n",
      "epoch: 200 Val H_real: -0.1505 Val H_fake: -0.1402 Val Loss G_H: 0.7720 Val Loss G_Z: 0.7771\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/20/fold5/generator.pth\n",
      "epoch: 0 Val H_real: 0.0359 Val H_fake: 0.0298 Val Loss G_H: 0.6885 Val Loss G_Z: 0.7553\n",
      "epoch: 20 Val H_real: -0.0040 Val H_fake: -0.0172 Val Loss G_H: 0.7088 Val Loss G_Z: 0.7132\n",
      "epoch: 40 Val H_real: -0.0131 Val H_fake: -0.0207 Val Loss G_H: 0.7102 Val Loss G_Z: 0.7117\n",
      "epoch: 60 Val H_real: -0.0288 Val H_fake: -0.0334 Val Loss G_H: 0.7165 Val Loss G_Z: 0.7100\n",
      "epoch: 80 Val H_real: -0.0588 Val H_fake: -0.0504 Val Loss G_H: 0.7253 Val Loss G_Z: 0.7200\n",
      "epoch: 100 Val H_real: -0.0979 Val H_fake: -0.0727 Val Loss G_H: 0.7370 Val Loss G_Z: 0.7378\n",
      "epoch: 120 Val H_real: -0.1225 Val H_fake: -0.0921 Val Loss G_H: 0.7471 Val Loss G_Z: 0.7493\n",
      "epoch: 140 Val H_real: -0.1402 Val H_fake: -0.1121 Val Loss G_H: 0.7577 Val Loss G_Z: 0.7609\n",
      "epoch: 160 Val H_real: -0.1611 Val H_fake: -0.1359 Val Loss G_H: 0.7704 Val Loss G_Z: 0.7767\n",
      "epoch: 180 Val H_real: -0.1838 Val H_fake: -0.1555 Val Loss G_H: 0.7809 Val Loss G_Z: 0.7932\n",
      "epoch: 200 Val H_real: -0.2051 Val H_fake: -0.1806 Val Loss G_H: 0.7947 Val Loss G_Z: 0.8112\n",
      "epoch: 220 Val H_real: -0.2256 Val H_fake: -0.2042 Val Loss G_H: 0.8077 Val Loss G_Z: 0.8237\n",
      "epoch: 240 Val H_real: -0.2439 Val H_fake: -0.2267 Val Loss G_H: 0.8203 Val Loss G_Z: 0.8325\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/30/fold1/generator.pth\n",
      "epoch: 0 Val H_real: 0.0131 Val H_fake: -0.0188 Val Loss G_H: 0.7116 Val Loss G_Z: 0.7865\n",
      "epoch: 20 Val H_real: -0.0172 Val H_fake: -0.0262 Val Loss G_H: 0.7145 Val Loss G_Z: 0.7139\n",
      "epoch: 40 Val H_real: -0.0375 Val H_fake: -0.0170 Val Loss G_H: 0.7095 Val Loss G_Z: 0.7142\n",
      "epoch: 60 Val H_real: -0.0547 Val H_fake: -0.0147 Val Loss G_H: 0.7080 Val Loss G_Z: 0.7231\n",
      "epoch: 80 Val H_real: -0.0778 Val H_fake: -0.0393 Val Loss G_H: 0.7204 Val Loss G_Z: 0.7382\n",
      "epoch: 100 Val H_real: -0.1635 Val H_fake: -0.1084 Val Loss G_H: 0.7560 Val Loss G_Z: 0.7449\n",
      "epoch: 120 Val H_real: -0.1737 Val H_fake: -0.1346 Val Loss G_H: 0.7697 Val Loss G_Z: 0.7613\n",
      "epoch: 140 Val H_real: -0.1833 Val H_fake: -0.1512 Val Loss G_H: 0.7786 Val Loss G_Z: 0.7764\n",
      "epoch: 160 Val H_real: -0.2032 Val H_fake: -0.1721 Val Loss G_H: 0.7901 Val Loss G_Z: 0.7918\n",
      "epoch: 180 Val H_real: -0.2346 Val H_fake: -0.2027 Val Loss G_H: 0.8071 Val Loss G_Z: 0.8153\n",
      "epoch: 200 Val H_real: -0.2636 Val H_fake: -0.2351 Val Loss G_H: 0.8251 Val Loss G_Z: 0.8354\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/30/fold2/generator.pth\n",
      "epoch: 0 Val H_real: -0.0241 Val H_fake: -0.0444 Val Loss G_H: 0.7240 Val Loss G_Z: 0.6753\n",
      "epoch: 20 Val H_real: -0.0231 Val H_fake: -0.0461 Val Loss G_H: 0.7241 Val Loss G_Z: 0.7146\n",
      "epoch: 40 Val H_real: -0.0577 Val H_fake: -0.0276 Val Loss G_H: 0.7143 Val Loss G_Z: 0.7018\n",
      "epoch: 60 Val H_real: -0.0857 Val H_fake: -0.0382 Val Loss G_H: 0.7192 Val Loss G_Z: 0.7124\n",
      "epoch: 80 Val H_real: -0.1051 Val H_fake: -0.0628 Val Loss G_H: 0.7315 Val Loss G_Z: 0.7242\n",
      "epoch: 100 Val H_real: -0.1233 Val H_fake: -0.0840 Val Loss G_H: 0.7424 Val Loss G_Z: 0.7334\n",
      "epoch: 120 Val H_real: -0.1388 Val H_fake: -0.1008 Val Loss G_H: 0.7512 Val Loss G_Z: 0.7411\n",
      "epoch: 140 Val H_real: -0.1493 Val H_fake: -0.1154 Val Loss G_H: 0.7590 Val Loss G_Z: 0.7507\n",
      "epoch: 160 Val H_real: -0.1631 Val H_fake: -0.1349 Val Loss G_H: 0.7695 Val Loss G_Z: 0.7619\n",
      "epoch: 180 Val H_real: -0.1745 Val H_fake: -0.1509 Val Loss G_H: 0.7781 Val Loss G_Z: 0.7707\n",
      "epoch: 200 Val H_real: -0.1833 Val H_fake: -0.1632 Val Loss G_H: 0.7849 Val Loss G_Z: 0.7775\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/30/fold3/generator.pth\n",
      "epoch: 0 Val H_real: -0.0161 Val H_fake: -0.0120 Val Loss G_H: 0.7077 Val Loss G_Z: 0.8398\n",
      "epoch: 20 Val H_real: 0.0025 Val H_fake: -0.0371 Val Loss G_H: 0.7189 Val Loss G_Z: 0.7215\n",
      "epoch: 40 Val H_real: -0.0249 Val H_fake: -0.0150 Val Loss G_H: 0.7073 Val Loss G_Z: 0.7142\n",
      "epoch: 60 Val H_real: -0.0579 Val H_fake: -0.0107 Val Loss G_H: 0.7048 Val Loss G_Z: 0.7161\n",
      "epoch: 80 Val H_real: -0.0840 Val H_fake: -0.0458 Val Loss G_H: 0.7222 Val Loss G_Z: 0.7255\n",
      "epoch: 100 Val H_real: -0.1001 Val H_fake: -0.0729 Val Loss G_H: 0.7360 Val Loss G_Z: 0.7374\n",
      "epoch: 120 Val H_real: -0.1067 Val H_fake: -0.0897 Val Loss G_H: 0.7446 Val Loss G_Z: 0.7489\n",
      "epoch: 140 Val H_real: -0.1186 Val H_fake: -0.1026 Val Loss G_H: 0.7515 Val Loss G_Z: 0.7569\n",
      "epoch: 160 Val H_real: -0.1303 Val H_fake: -0.1192 Val Loss G_H: 0.7603 Val Loss G_Z: 0.7656\n",
      "epoch: 180 Val H_real: -0.1456 Val H_fake: -0.1350 Val Loss G_H: 0.7689 Val Loss G_Z: 0.7755\n",
      "epoch: 200 Val H_real: -0.1658 Val H_fake: -0.1564 Val Loss G_H: 0.7807 Val Loss G_Z: 0.7886\n",
      "epoch: 220 Val H_real: -0.1839 Val H_fake: -0.1757 Val Loss G_H: 0.7913 Val Loss G_Z: 0.7961\n",
      "epoch: 240 Val H_real: -0.1943 Val H_fake: -0.1871 Val Loss G_H: 0.7977 Val Loss G_Z: 0.8051\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/30/fold4/generator.pth\n",
      "epoch: 0 Val H_real: -0.0831 Val H_fake: -0.0832 Val Loss G_H: 0.7438 Val Loss G_Z: 0.6912\n",
      "epoch: 20 Val H_real: -0.0111 Val H_fake: -0.0216 Val Loss G_H: 0.7111 Val Loss G_Z: 0.7121\n",
      "epoch: 40 Val H_real: -0.0201 Val H_fake: -0.0266 Val Loss G_H: 0.7133 Val Loss G_Z: 0.7186\n",
      "epoch: 60 Val H_real: -0.0463 Val H_fake: -0.0391 Val Loss G_H: 0.7196 Val Loss G_Z: 0.7278\n",
      "epoch: 80 Val H_real: -0.1355 Val H_fake: -0.0990 Val Loss G_H: 0.7506 Val Loss G_Z: 0.7437\n",
      "epoch: 100 Val H_real: -0.1495 Val H_fake: -0.1220 Val Loss G_H: 0.7623 Val Loss G_Z: 0.7559\n",
      "epoch: 120 Val H_real: -0.1498 Val H_fake: -0.1340 Val Loss G_H: 0.7684 Val Loss G_Z: 0.7648\n",
      "epoch: 140 Val H_real: -0.1516 Val H_fake: -0.1416 Val Loss G_H: 0.7724 Val Loss G_Z: 0.7715\n",
      "epoch: 160 Val H_real: -0.1602 Val H_fake: -0.1516 Val Loss G_H: 0.7778 Val Loss G_Z: 0.7782\n",
      "epoch: 180 Val H_real: -0.1691 Val H_fake: -0.1587 Val Loss G_H: 0.7819 Val Loss G_Z: 0.7838\n",
      "epoch: 200 Val H_real: -0.1813 Val H_fake: -0.1723 Val Loss G_H: 0.7895 Val Loss G_Z: 0.7929\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/30/fold5/generator.pth\n",
      "epoch: 0 Val H_real: -0.0144 Val H_fake: -0.0162 Val Loss G_H: 0.7093 Val Loss G_Z: 0.7003\n",
      "epoch: 20 Val H_real: -0.0080 Val H_fake: -0.0011 Val Loss G_H: 0.7001 Val Loss G_Z: 0.7031\n",
      "epoch: 40 Val H_real: -0.0153 Val H_fake: 0.0013 Val Loss G_H: 0.6986 Val Loss G_Z: 0.7033\n",
      "epoch: 60 Val H_real: -0.0436 Val H_fake: -0.0191 Val Loss G_H: 0.7088 Val Loss G_Z: 0.7107\n",
      "epoch: 80 Val H_real: -0.0793 Val H_fake: -0.0526 Val Loss G_H: 0.7257 Val Loss G_Z: 0.7246\n",
      "epoch: 100 Val H_real: -0.1011 Val H_fake: -0.0777 Val Loss G_H: 0.7386 Val Loss G_Z: 0.7385\n",
      "epoch: 120 Val H_real: -0.1147 Val H_fake: -0.0927 Val Loss G_H: 0.7464 Val Loss G_Z: 0.7474\n",
      "epoch: 140 Val H_real: -0.1253 Val H_fake: -0.1061 Val Loss G_H: 0.7536 Val Loss G_Z: 0.7587\n",
      "epoch: 160 Val H_real: -0.1342 Val H_fake: -0.1147 Val Loss G_H: 0.7583 Val Loss G_Z: 0.7669\n",
      "epoch: 180 Val H_real: -0.1409 Val H_fake: -0.1244 Val Loss G_H: 0.7637 Val Loss G_Z: 0.7757\n",
      "epoch: 200 Val H_real: -0.1512 Val H_fake: -0.1342 Val Loss G_H: 0.7693 Val Loss G_Z: 0.7831\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/40/fold1/generator.pth\n",
      "epoch: 0 Val H_real: 0.0481 Val H_fake: 0.0232 Val Loss G_H: 0.6903 Val Loss G_Z: 0.7390\n",
      "epoch: 20 Val H_real: -0.0014 Val H_fake: -0.0301 Val Loss G_H: 0.7158 Val Loss G_Z: 0.7190\n",
      "epoch: 40 Val H_real: -0.0444 Val H_fake: -0.0011 Val Loss G_H: 0.7002 Val Loss G_Z: 0.7039\n",
      "epoch: 60 Val H_real: -0.0562 Val H_fake: -0.0119 Val Loss G_H: 0.7051 Val Loss G_Z: 0.7068\n",
      "epoch: 80 Val H_real: -0.0671 Val H_fake: -0.0257 Val Loss G_H: 0.7120 Val Loss G_Z: 0.7159\n",
      "epoch: 100 Val H_real: -0.0717 Val H_fake: -0.0332 Val Loss G_H: 0.7159 Val Loss G_Z: 0.7256\n",
      "epoch: 120 Val H_real: -0.0793 Val H_fake: -0.0412 Val Loss G_H: 0.7203 Val Loss G_Z: 0.7344\n",
      "epoch: 140 Val H_real: -0.0860 Val H_fake: -0.0471 Val Loss G_H: 0.7235 Val Loss G_Z: 0.7424\n",
      "epoch: 160 Val H_real: -0.0996 Val H_fake: -0.0614 Val Loss G_H: 0.7313 Val Loss G_Z: 0.7578\n",
      "epoch: 180 Val H_real: -0.1190 Val H_fake: -0.0830 Val Loss G_H: 0.7430 Val Loss G_Z: 0.7743\n",
      "epoch: 200 Val H_real: -0.1465 Val H_fake: -0.1133 Val Loss G_H: 0.7593 Val Loss G_Z: 0.7951\n",
      "epoch: 220 Val H_real: -0.1731 Val H_fake: -0.1438 Val Loss G_H: 0.7760 Val Loss G_Z: 0.8109\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/40/fold2/generator.pth\n",
      "epoch: 0 Val H_real: -0.0014 Val H_fake: -0.0169 Val Loss G_H: 0.7110 Val Loss G_Z: 0.7252\n",
      "epoch: 20 Val H_real: -0.0139 Val H_fake: -0.0355 Val Loss G_H: 0.7185 Val Loss G_Z: 0.7154\n",
      "epoch: 40 Val H_real: -0.0323 Val H_fake: -0.0233 Val Loss G_H: 0.7121 Val Loss G_Z: 0.7161\n",
      "epoch: 60 Val H_real: -0.0565 Val H_fake: -0.0287 Val Loss G_H: 0.7146 Val Loss G_Z: 0.7419\n",
      "epoch: 80 Val H_real: -0.1303 Val H_fake: -0.0752 Val Loss G_H: 0.7381 Val Loss G_Z: 0.7305\n",
      "epoch: 100 Val H_real: -0.1388 Val H_fake: -0.0969 Val Loss G_H: 0.7491 Val Loss G_Z: 0.7491\n",
      "epoch: 120 Val H_real: -0.1427 Val H_fake: -0.1119 Val Loss G_H: 0.7569 Val Loss G_Z: 0.7604\n",
      "epoch: 140 Val H_real: -0.1481 Val H_fake: -0.1212 Val Loss G_H: 0.7618 Val Loss G_Z: 0.7670\n",
      "epoch: 160 Val H_real: -0.1558 Val H_fake: -0.1331 Val Loss G_H: 0.7682 Val Loss G_Z: 0.7724\n",
      "epoch: 180 Val H_real: -0.1692 Val H_fake: -0.1499 Val Loss G_H: 0.7774 Val Loss G_Z: 0.7790\n",
      "epoch: 200 Val H_real: -0.1769 Val H_fake: -0.1590 Val Loss G_H: 0.7826 Val Loss G_Z: 0.7866\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/40/fold3/generator.pth\n",
      "epoch: 0 Val H_real: 0.0895 Val H_fake: 0.0732 Val Loss G_H: 0.6664 Val Loss G_Z: 0.7673\n",
      "epoch: 20 Val H_real: 0.0002 Val H_fake: -0.0128 Val Loss G_H: 0.7067 Val Loss G_Z: 0.7175\n",
      "epoch: 40 Val H_real: 0.0153 Val H_fake: -0.0133 Val Loss G_H: 0.7064 Val Loss G_Z: 0.7231\n",
      "epoch: 60 Val H_real: 0.0092 Val H_fake: -0.0157 Val Loss G_H: 0.7075 Val Loss G_Z: 0.7090\n",
      "epoch: 80 Val H_real: -0.0052 Val H_fake: -0.0335 Val Loss G_H: 0.7165 Val Loss G_Z: 0.7209\n",
      "epoch: 100 Val H_real: -0.0437 Val H_fake: -0.0552 Val Loss G_H: 0.7278 Val Loss G_Z: 0.7433\n",
      "epoch: 120 Val H_real: -0.1021 Val H_fake: -0.0752 Val Loss G_H: 0.7382 Val Loss G_Z: 0.7538\n",
      "epoch: 140 Val H_real: -0.1305 Val H_fake: -0.1052 Val Loss G_H: 0.7539 Val Loss G_Z: 0.7647\n",
      "epoch: 160 Val H_real: -0.1443 Val H_fake: -0.1193 Val Loss G_H: 0.7614 Val Loss G_Z: 0.7732\n",
      "epoch: 180 Val H_real: -0.1537 Val H_fake: -0.1345 Val Loss G_H: 0.7698 Val Loss G_Z: 0.7838\n",
      "epoch: 200 Val H_real: -0.1623 Val H_fake: -0.1472 Val Loss G_H: 0.7769 Val Loss G_Z: 0.7961\n",
      "epoch: 220 Val H_real: -0.1822 Val H_fake: -0.1678 Val Loss G_H: 0.7885 Val Loss G_Z: 0.8087\n",
      "epoch: 240 Val H_real: -0.2017 Val H_fake: -0.1899 Val Loss G_H: 0.8011 Val Loss G_Z: 0.8239\n",
      "epoch: 260 Val H_real: -0.2274 Val H_fake: -0.2203 Val Loss G_H: 0.8184 Val Loss G_Z: 0.8453\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/40/fold4/generator.pth\n",
      "epoch: 0 Val H_real: 0.0352 Val H_fake: 0.0025 Val Loss G_H: 0.7024 Val Loss G_Z: 0.7170\n",
      "epoch: 20 Val H_real: 0.0081 Val H_fake: -0.0152 Val Loss G_H: 0.7080 Val Loss G_Z: 0.7117\n",
      "epoch: 40 Val H_real: -0.0123 Val H_fake: -0.0137 Val Loss G_H: 0.7068 Val Loss G_Z: 0.7109\n",
      "epoch: 60 Val H_real: -0.0372 Val H_fake: -0.0218 Val Loss G_H: 0.7106 Val Loss G_Z: 0.7110\n",
      "epoch: 80 Val H_real: -0.0572 Val H_fake: -0.0396 Val Loss G_H: 0.7196 Val Loss G_Z: 0.7127\n",
      "epoch: 100 Val H_real: -0.0711 Val H_fake: -0.0522 Val Loss G_H: 0.7262 Val Loss G_Z: 0.7195\n",
      "epoch: 120 Val H_real: -0.0933 Val H_fake: -0.0757 Val Loss G_H: 0.7385 Val Loss G_Z: 0.7314\n",
      "epoch: 140 Val H_real: -0.1144 Val H_fake: -0.0952 Val Loss G_H: 0.7490 Val Loss G_Z: 0.7488\n",
      "epoch: 160 Val H_real: -0.1393 Val H_fake: -0.1163 Val Loss G_H: 0.7605 Val Loss G_Z: 0.7684\n",
      "epoch: 180 Val H_real: -0.1658 Val H_fake: -0.1413 Val Loss G_H: 0.7741 Val Loss G_Z: 0.7853\n",
      "epoch: 200 Val H_real: -0.1814 Val H_fake: -0.1578 Val Loss G_H: 0.7833 Val Loss G_Z: 0.7947\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/2/40/fold5/generator.pth\n",
      "channels:3\n",
      "epoch: 0 Val H_real: 0.2222 Val H_fake: 0.2058 Val Loss G_H: 0.6047 Val Loss G_Z: 0.7105\n",
      "epoch: 20 Val H_real: -0.0180 Val H_fake: -0.0109 Val Loss G_H: 0.7063 Val Loss G_Z: 0.7092\n",
      "epoch: 40 Val H_real: -0.0351 Val H_fake: -0.0256 Val Loss G_H: 0.7127 Val Loss G_Z: 0.7140\n",
      "epoch: 60 Val H_real: -0.0604 Val H_fake: -0.0501 Val Loss G_H: 0.7248 Val Loss G_Z: 0.7246\n",
      "epoch: 80 Val H_real: -0.0805 Val H_fake: -0.0645 Val Loss G_H: 0.7319 Val Loss G_Z: 0.7369\n",
      "epoch: 100 Val H_real: -0.1001 Val H_fake: -0.0808 Val Loss G_H: 0.7402 Val Loss G_Z: 0.7469\n",
      "epoch: 120 Val H_real: -0.1158 Val H_fake: -0.1006 Val Loss G_H: 0.7504 Val Loss G_Z: 0.7550\n",
      "epoch: 140 Val H_real: -0.1300 Val H_fake: -0.1166 Val Loss G_H: 0.7588 Val Loss G_Z: 0.7596\n",
      "epoch: 160 Val H_real: -0.1438 Val H_fake: -0.1334 Val Loss G_H: 0.7675 Val Loss G_Z: 0.7664\n",
      "epoch: 180 Val H_real: -0.1534 Val H_fake: -0.1441 Val Loss G_H: 0.7732 Val Loss G_Z: 0.7708\n",
      "epoch: 200 Val H_real: -0.1624 Val H_fake: -0.1547 Val Loss G_H: 0.7789 Val Loss G_Z: 0.7772\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/10/fold1/generator.pth\n",
      "epoch: 0 Val H_real: 0.1454 Val H_fake: 0.1335 Val Loss G_H: 0.6374 Val Loss G_Z: 0.6415\n",
      "epoch: 20 Val H_real: -0.0094 Val H_fake: -0.0063 Val Loss G_H: 0.7039 Val Loss G_Z: 0.7069\n",
      "epoch: 40 Val H_real: -0.0233 Val H_fake: -0.0098 Val Loss G_H: 0.7053 Val Loss G_Z: 0.7102\n",
      "epoch: 60 Val H_real: -0.0519 Val H_fake: -0.0236 Val Loss G_H: 0.7118 Val Loss G_Z: 0.7217\n",
      "epoch: 80 Val H_real: -0.1110 Val H_fake: -0.0660 Val Loss G_H: 0.7328 Val Loss G_Z: 0.7418\n",
      "epoch: 100 Val H_real: -0.1448 Val H_fake: -0.1093 Val Loss G_H: 0.7550 Val Loss G_Z: 0.7594\n",
      "epoch: 120 Val H_real: -0.1627 Val H_fake: -0.1377 Val Loss G_H: 0.7699 Val Loss G_Z: 0.7704\n",
      "epoch: 140 Val H_real: -0.1700 Val H_fake: -0.1520 Val Loss G_H: 0.7774 Val Loss G_Z: 0.7780\n",
      "epoch: 160 Val H_real: -0.1817 Val H_fake: -0.1682 Val Loss G_H: 0.7861 Val Loss G_Z: 0.7826\n",
      "epoch: 180 Val H_real: -0.1923 Val H_fake: -0.1814 Val Loss G_H: 0.7931 Val Loss G_Z: 0.7892\n",
      "epoch: 200 Val H_real: -0.2045 Val H_fake: -0.1978 Val Loss G_H: 0.8021 Val Loss G_Z: 0.7948\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/10/fold2/generator.pth\n",
      "epoch: 0 Val H_real: 0.0539 Val H_fake: 0.0344 Val Loss G_H: 0.6861 Val Loss G_Z: 0.7559\n",
      "epoch: 20 Val H_real: -0.0209 Val H_fake: -0.0075 Val Loss G_H: 0.7044 Val Loss G_Z: 0.7113\n",
      "epoch: 40 Val H_real: -0.0344 Val H_fake: -0.0166 Val Loss G_H: 0.7082 Val Loss G_Z: 0.7137\n",
      "epoch: 60 Val H_real: -0.0493 Val H_fake: -0.0294 Val Loss G_H: 0.7141 Val Loss G_Z: 0.7211\n",
      "epoch: 80 Val H_real: -0.0749 Val H_fake: -0.0542 Val Loss G_H: 0.7264 Val Loss G_Z: 0.7353\n",
      "epoch: 100 Val H_real: -0.0971 Val H_fake: -0.0801 Val Loss G_H: 0.7396 Val Loss G_Z: 0.7460\n",
      "epoch: 120 Val H_real: -0.1109 Val H_fake: -0.0944 Val Loss G_H: 0.7470 Val Loss G_Z: 0.7536\n",
      "epoch: 140 Val H_real: -0.1235 Val H_fake: -0.1103 Val Loss G_H: 0.7551 Val Loss G_Z: 0.7602\n",
      "epoch: 160 Val H_real: -0.1346 Val H_fake: -0.1237 Val Loss G_H: 0.7622 Val Loss G_Z: 0.7649\n",
      "epoch: 180 Val H_real: -0.1463 Val H_fake: -0.1359 Val Loss G_H: 0.7686 Val Loss G_Z: 0.7705\n",
      "epoch: 200 Val H_real: -0.1595 Val H_fake: -0.1513 Val Loss G_H: 0.7768 Val Loss G_Z: 0.7763\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/10/fold3/generator.pth\n",
      "epoch: 0 Val H_real: -0.1012 Val H_fake: -0.1117 Val Loss G_H: 0.7591 Val Loss G_Z: 0.7101\n",
      "epoch: 20 Val H_real: -0.0107 Val H_fake: -0.0126 Val Loss G_H: 0.7067 Val Loss G_Z: 0.7097\n",
      "epoch: 40 Val H_real: -0.0245 Val H_fake: -0.0187 Val Loss G_H: 0.7093 Val Loss G_Z: 0.7122\n",
      "epoch: 60 Val H_real: -0.0694 Val H_fake: -0.0479 Val Loss G_H: 0.7240 Val Loss G_Z: 0.7225\n",
      "epoch: 80 Val H_real: -0.1238 Val H_fake: -0.0996 Val Loss G_H: 0.7504 Val Loss G_Z: 0.7517\n",
      "epoch: 100 Val H_real: -0.1363 Val H_fake: -0.1148 Val Loss G_H: 0.7582 Val Loss G_Z: 0.7617\n",
      "epoch: 120 Val H_real: -0.1424 Val H_fake: -0.1258 Val Loss G_H: 0.7636 Val Loss G_Z: 0.7694\n",
      "epoch: 140 Val H_real: -0.1500 Val H_fake: -0.1392 Val Loss G_H: 0.7706 Val Loss G_Z: 0.7733\n",
      "epoch: 160 Val H_real: -0.1580 Val H_fake: -0.1505 Val Loss G_H: 0.7765 Val Loss G_Z: 0.7787\n",
      "epoch: 180 Val H_real: -0.1680 Val H_fake: -0.1609 Val Loss G_H: 0.7821 Val Loss G_Z: 0.7817\n",
      "epoch: 200 Val H_real: -0.1789 Val H_fake: -0.1746 Val Loss G_H: 0.7895 Val Loss G_Z: 0.7858\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/10/fold4/generator.pth\n",
      "epoch: 0 Val H_real: -0.0028 Val H_fake: -0.0189 Val Loss G_H: 0.7117 Val Loss G_Z: 0.6499\n",
      "epoch: 20 Val H_real: -0.0257 Val H_fake: -0.0170 Val Loss G_H: 0.7085 Val Loss G_Z: 0.7042\n",
      "epoch: 40 Val H_real: -0.0432 Val H_fake: -0.0332 Val Loss G_H: 0.7161 Val Loss G_Z: 0.7128\n",
      "epoch: 60 Val H_real: -0.0644 Val H_fake: -0.0499 Val Loss G_H: 0.7245 Val Loss G_Z: 0.7218\n",
      "epoch: 80 Val H_real: -0.0898 Val H_fake: -0.0712 Val Loss G_H: 0.7352 Val Loss G_Z: 0.7305\n",
      "epoch: 100 Val H_real: -0.1098 Val H_fake: -0.0901 Val Loss G_H: 0.7447 Val Loss G_Z: 0.7421\n",
      "epoch: 120 Val H_real: -0.1211 Val H_fake: -0.1024 Val Loss G_H: 0.7510 Val Loss G_Z: 0.7502\n",
      "epoch: 140 Val H_real: -0.1318 Val H_fake: -0.1169 Val Loss G_H: 0.7587 Val Loss G_Z: 0.7593\n",
      "epoch: 160 Val H_real: -0.1454 Val H_fake: -0.1318 Val Loss G_H: 0.7666 Val Loss G_Z: 0.7658\n",
      "epoch: 180 Val H_real: -0.1580 Val H_fake: -0.1472 Val Loss G_H: 0.7748 Val Loss G_Z: 0.7707\n",
      "epoch: 200 Val H_real: -0.1720 Val H_fake: -0.1614 Val Loss G_H: 0.7825 Val Loss G_Z: 0.7778\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/10/fold5/generator.pth\n",
      "epoch: 0 Val H_real: -0.0790 Val H_fake: -0.1256 Val Loss G_H: 0.7686 Val Loss G_Z: 0.7186\n",
      "epoch: 20 Val H_real: 0.0015 Val H_fake: -0.0122 Val Loss G_H: 0.7070 Val Loss G_Z: 0.7083\n",
      "epoch: 40 Val H_real: -0.0291 Val H_fake: -0.0111 Val Loss G_H: 0.7059 Val Loss G_Z: 0.7095\n",
      "epoch: 60 Val H_real: -0.0538 Val H_fake: -0.0182 Val Loss G_H: 0.7090 Val Loss G_Z: 0.7081\n",
      "epoch: 80 Val H_real: -0.0751 Val H_fake: -0.0262 Val Loss G_H: 0.7129 Val Loss G_Z: 0.7122\n",
      "epoch: 100 Val H_real: -0.0901 Val H_fake: -0.0446 Val Loss G_H: 0.7221 Val Loss G_Z: 0.7214\n",
      "epoch: 120 Val H_real: -0.1001 Val H_fake: -0.0630 Val Loss G_H: 0.7316 Val Loss G_Z: 0.7322\n",
      "epoch: 140 Val H_real: -0.1130 Val H_fake: -0.0818 Val Loss G_H: 0.7414 Val Loss G_Z: 0.7444\n",
      "epoch: 160 Val H_real: -0.1347 Val H_fake: -0.1088 Val Loss G_H: 0.7555 Val Loss G_Z: 0.7554\n",
      "epoch: 180 Val H_real: -0.1647 Val H_fake: -0.1451 Val Loss G_H: 0.7749 Val Loss G_Z: 0.7709\n",
      "epoch: 200 Val H_real: -0.1804 Val H_fake: -0.1634 Val Loss G_H: 0.7849 Val Loss G_Z: 0.7804\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/20/fold1/generator.pth\n",
      "epoch: 0 Val H_real: 0.2010 Val H_fake: 0.1941 Val Loss G_H: 0.6095 Val Loss G_Z: 0.6779\n",
      "epoch: 20 Val H_real: -0.0104 Val H_fake: -0.0072 Val Loss G_H: 0.7043 Val Loss G_Z: 0.7064\n",
      "epoch: 40 Val H_real: -0.0266 Val H_fake: -0.0136 Val Loss G_H: 0.7068 Val Loss G_Z: 0.7076\n",
      "epoch: 60 Val H_real: -0.0507 Val H_fake: -0.0146 Val Loss G_H: 0.7073 Val Loss G_Z: 0.7111\n",
      "epoch: 80 Val H_real: -0.0881 Val H_fake: -0.0380 Val Loss G_H: 0.7190 Val Loss G_Z: 0.7250\n",
      "epoch: 100 Val H_real: -0.1449 Val H_fake: -0.0977 Val Loss G_H: 0.7495 Val Loss G_Z: 0.7527\n",
      "epoch: 120 Val H_real: -0.1572 Val H_fake: -0.1188 Val Loss G_H: 0.7604 Val Loss G_Z: 0.7648\n",
      "epoch: 140 Val H_real: -0.1645 Val H_fake: -0.1325 Val Loss G_H: 0.7675 Val Loss G_Z: 0.7751\n",
      "epoch: 160 Val H_real: -0.1789 Val H_fake: -0.1540 Val Loss G_H: 0.7790 Val Loss G_Z: 0.7864\n",
      "epoch: 180 Val H_real: -0.1975 Val H_fake: -0.1784 Val Loss G_H: 0.7922 Val Loss G_Z: 0.7977\n",
      "epoch: 200 Val H_real: -0.2164 Val H_fake: -0.1972 Val Loss G_H: 0.8025 Val Loss G_Z: 0.8087\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/20/fold2/generator.pth\n",
      "epoch: 0 Val H_real: 0.0679 Val H_fake: 0.0791 Val Loss G_H: 0.6638 Val Loss G_Z: 0.6553\n",
      "epoch: 20 Val H_real: -0.0192 Val H_fake: -0.0051 Val Loss G_H: 0.7023 Val Loss G_Z: 0.7043\n",
      "epoch: 40 Val H_real: -0.0401 Val H_fake: -0.0226 Val Loss G_H: 0.7104 Val Loss G_Z: 0.7088\n",
      "epoch: 60 Val H_real: -0.0579 Val H_fake: -0.0332 Val Loss G_H: 0.7156 Val Loss G_Z: 0.7144\n",
      "epoch: 80 Val H_real: -0.0785 Val H_fake: -0.0517 Val Loss G_H: 0.7250 Val Loss G_Z: 0.7244\n",
      "epoch: 100 Val H_real: -0.0953 Val H_fake: -0.0649 Val Loss G_H: 0.7319 Val Loss G_Z: 0.7346\n",
      "epoch: 120 Val H_real: -0.1149 Val H_fake: -0.0830 Val Loss G_H: 0.7412 Val Loss G_Z: 0.7461\n",
      "epoch: 140 Val H_real: -0.1348 Val H_fake: -0.1061 Val Loss G_H: 0.7534 Val Loss G_Z: 0.7590\n",
      "epoch: 160 Val H_real: -0.1548 Val H_fake: -0.1316 Val Loss G_H: 0.7668 Val Loss G_Z: 0.7702\n",
      "epoch: 180 Val H_real: -0.1675 Val H_fake: -0.1453 Val Loss G_H: 0.7742 Val Loss G_Z: 0.7776\n",
      "epoch: 200 Val H_real: -0.1839 Val H_fake: -0.1639 Val Loss G_H: 0.7842 Val Loss G_Z: 0.7832\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/20/fold3/generator.pth\n",
      "epoch: 0 Val H_real: -0.0945 Val H_fake: -0.1013 Val Loss G_H: 0.7545 Val Loss G_Z: 0.7349\n",
      "epoch: 20 Val H_real: -0.0285 Val H_fake: -0.0168 Val Loss G_H: 0.7087 Val Loss G_Z: 0.7095\n",
      "epoch: 40 Val H_real: -0.0446 Val H_fake: -0.0275 Val Loss G_H: 0.7134 Val Loss G_Z: 0.7092\n",
      "epoch: 60 Val H_real: -0.0583 Val H_fake: -0.0387 Val Loss G_H: 0.7187 Val Loss G_Z: 0.7136\n",
      "epoch: 80 Val H_real: -0.0708 Val H_fake: -0.0530 Val Loss G_H: 0.7258 Val Loss G_Z: 0.7218\n",
      "epoch: 100 Val H_real: -0.0830 Val H_fake: -0.0646 Val Loss G_H: 0.7317 Val Loss G_Z: 0.7310\n",
      "epoch: 120 Val H_real: -0.0987 Val H_fake: -0.0791 Val Loss G_H: 0.7392 Val Loss G_Z: 0.7400\n",
      "epoch: 140 Val H_real: -0.1144 Val H_fake: -0.0931 Val Loss G_H: 0.7465 Val Loss G_Z: 0.7485\n",
      "epoch: 160 Val H_real: -0.1237 Val H_fake: -0.1039 Val Loss G_H: 0.7522 Val Loss G_Z: 0.7559\n",
      "epoch: 180 Val H_real: -0.1296 Val H_fake: -0.1109 Val Loss G_H: 0.7560 Val Loss G_Z: 0.7607\n",
      "epoch: 200 Val H_real: -0.1431 Val H_fake: -0.1253 Val Loss G_H: 0.7637 Val Loss G_Z: 0.7715\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/20/fold4/generator.pth\n",
      "epoch: 0 Val H_real: 0.0350 Val H_fake: 0.0046 Val Loss G_H: 0.6992 Val Loss G_Z: 0.7381\n",
      "epoch: 20 Val H_real: -0.0253 Val H_fake: -0.0102 Val Loss G_H: 0.7055 Val Loss G_Z: 0.7070\n",
      "epoch: 40 Val H_real: -0.0371 Val H_fake: -0.0120 Val Loss G_H: 0.7059 Val Loss G_Z: 0.7080\n",
      "epoch: 60 Val H_real: -0.0555 Val H_fake: -0.0156 Val Loss G_H: 0.7076 Val Loss G_Z: 0.7116\n",
      "epoch: 80 Val H_real: -0.0810 Val H_fake: -0.0313 Val Loss G_H: 0.7158 Val Loss G_Z: 0.7180\n",
      "epoch: 100 Val H_real: -0.1135 Val H_fake: -0.0606 Val Loss G_H: 0.7310 Val Loss G_Z: 0.7295\n",
      "epoch: 120 Val H_real: -0.1542 Val H_fake: -0.1035 Val Loss G_H: 0.7532 Val Loss G_Z: 0.7557\n",
      "epoch: 140 Val H_real: -0.2011 Val H_fake: -0.1573 Val Loss G_H: 0.7816 Val Loss G_Z: 0.7829\n",
      "epoch: 160 Val H_real: -0.2372 Val H_fake: -0.1981 Val Loss G_H: 0.8037 Val Loss G_Z: 0.8033\n",
      "epoch: 180 Val H_real: -0.2705 Val H_fake: -0.2392 Val Loss G_H: 0.8265 Val Loss G_Z: 0.8252\n",
      "epoch: 200 Val H_real: -0.2936 Val H_fake: -0.2695 Val Loss G_H: 0.8437 Val Loss G_Z: 0.8422\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/20/fold5/generator.pth\n",
      "epoch: 0 Val H_real: -0.0344 Val H_fake: -0.0759 Val Loss G_H: 0.7411 Val Loss G_Z: 0.7010\n",
      "epoch: 20 Val H_real: 0.0040 Val H_fake: -0.0192 Val Loss G_H: 0.7103 Val Loss G_Z: 0.7126\n",
      "epoch: 40 Val H_real: -0.0291 Val H_fake: 0.0014 Val Loss G_H: 0.6995 Val Loss G_Z: 0.7062\n",
      "epoch: 60 Val H_real: -0.0578 Val H_fake: 0.0031 Val Loss G_H: 0.6982 Val Loss G_Z: 0.7089\n",
      "epoch: 80 Val H_real: -0.0903 Val H_fake: -0.0361 Val Loss G_H: 0.7178 Val Loss G_Z: 0.7255\n",
      "epoch: 100 Val H_real: -0.1179 Val H_fake: -0.0704 Val Loss G_H: 0.7354 Val Loss G_Z: 0.7476\n",
      "epoch: 120 Val H_real: -0.1287 Val H_fake: -0.0849 Val Loss G_H: 0.7428 Val Loss G_Z: 0.7586\n",
      "epoch: 140 Val H_real: -0.1398 Val H_fake: -0.0959 Val Loss G_H: 0.7484 Val Loss G_Z: 0.7675\n",
      "epoch: 160 Val H_real: -0.1588 Val H_fake: -0.1201 Val Loss G_H: 0.7612 Val Loss G_Z: 0.7775\n",
      "epoch: 180 Val H_real: -0.1762 Val H_fake: -0.1446 Val Loss G_H: 0.7744 Val Loss G_Z: 0.7886\n",
      "epoch: 200 Val H_real: -0.1966 Val H_fake: -0.1688 Val Loss G_H: 0.7876 Val Loss G_Z: 0.8021\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/30/fold1/generator.pth\n",
      "epoch: 0 Val H_real: -0.0270 Val H_fake: -0.0562 Val Loss G_H: 0.7313 Val Loss G_Z: 0.6725\n",
      "epoch: 20 Val H_real: -0.0153 Val H_fake: -0.0290 Val Loss G_H: 0.7155 Val Loss G_Z: 0.7229\n",
      "epoch: 40 Val H_real: -0.0493 Val H_fake: -0.0153 Val Loss G_H: 0.7080 Val Loss G_Z: 0.7077\n",
      "epoch: 60 Val H_real: -0.0659 Val H_fake: -0.0218 Val Loss G_H: 0.7109 Val Loss G_Z: 0.7025\n",
      "epoch: 80 Val H_real: -0.0927 Val H_fake: -0.0362 Val Loss G_H: 0.7179 Val Loss G_Z: 0.7150\n",
      "epoch: 100 Val H_real: -0.1213 Val H_fake: -0.0605 Val Loss G_H: 0.7304 Val Loss G_Z: 0.7339\n",
      "epoch: 120 Val H_real: -0.1396 Val H_fake: -0.0817 Val Loss G_H: 0.7413 Val Loss G_Z: 0.7448\n",
      "epoch: 140 Val H_real: -0.1556 Val H_fake: -0.1014 Val Loss G_H: 0.7516 Val Loss G_Z: 0.7536\n",
      "epoch: 160 Val H_real: -0.1738 Val H_fake: -0.1268 Val Loss G_H: 0.7649 Val Loss G_Z: 0.7670\n",
      "epoch: 180 Val H_real: -0.1981 Val H_fake: -0.1578 Val Loss G_H: 0.7818 Val Loss G_Z: 0.7864\n",
      "epoch: 200 Val H_real: -0.2290 Val H_fake: -0.1953 Val Loss G_H: 0.8025 Val Loss G_Z: 0.8056\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/30/fold2/generator.pth\n",
      "epoch: 0 Val H_real: 0.0185 Val H_fake: -0.0171 Val Loss G_H: 0.7137 Val Loss G_Z: 0.7401\n",
      "epoch: 20 Val H_real: -0.0144 Val H_fake: -0.0101 Val Loss G_H: 0.7051 Val Loss G_Z: 0.7142\n",
      "epoch: 40 Val H_real: -0.0259 Val H_fake: -0.0055 Val Loss G_H: 0.7026 Val Loss G_Z: 0.7141\n",
      "epoch: 60 Val H_real: -0.0454 Val H_fake: -0.0055 Val Loss G_H: 0.7027 Val Loss G_Z: 0.7154\n",
      "epoch: 80 Val H_real: -0.0698 Val H_fake: -0.0209 Val Loss G_H: 0.7104 Val Loss G_Z: 0.7245\n",
      "epoch: 100 Val H_real: -0.1307 Val H_fake: -0.0704 Val Loss G_H: 0.7357 Val Loss G_Z: 0.7374\n",
      "epoch: 120 Val H_real: -0.1631 Val H_fake: -0.1076 Val Loss G_H: 0.7553 Val Loss G_Z: 0.7555\n",
      "epoch: 140 Val H_real: -0.1851 Val H_fake: -0.1368 Val Loss G_H: 0.7708 Val Loss G_Z: 0.7730\n",
      "epoch: 160 Val H_real: -0.2037 Val H_fake: -0.1656 Val Loss G_H: 0.7863 Val Loss G_Z: 0.7894\n",
      "epoch: 180 Val H_real: -0.2267 Val H_fake: -0.1935 Val Loss G_H: 0.8016 Val Loss G_Z: 0.8085\n",
      "epoch: 200 Val H_real: -0.2541 Val H_fake: -0.2311 Val Loss G_H: 0.8224 Val Loss G_Z: 0.8230\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/30/fold3/generator.pth\n",
      "epoch: 0 Val H_real: -0.0001 Val H_fake: -0.0042 Val Loss G_H: 0.7034 Val Loss G_Z: 0.6774\n",
      "epoch: 20 Val H_real: -0.0116 Val H_fake: -0.0068 Val Loss G_H: 0.7038 Val Loss G_Z: 0.7148\n",
      "epoch: 40 Val H_real: -0.0235 Val H_fake: -0.0160 Val Loss G_H: 0.7081 Val Loss G_Z: 0.7140\n",
      "epoch: 60 Val H_real: -0.0378 Val H_fake: -0.0214 Val Loss G_H: 0.7107 Val Loss G_Z: 0.7164\n",
      "epoch: 80 Val H_real: -0.0816 Val H_fake: -0.0445 Val Loss G_H: 0.7223 Val Loss G_Z: 0.7275\n",
      "epoch: 100 Val H_real: -0.1246 Val H_fake: -0.0748 Val Loss G_H: 0.7377 Val Loss G_Z: 0.7436\n",
      "epoch: 120 Val H_real: -0.1496 Val H_fake: -0.0993 Val Loss G_H: 0.7504 Val Loss G_Z: 0.7573\n",
      "epoch: 140 Val H_real: -0.1627 Val H_fake: -0.1164 Val Loss G_H: 0.7595 Val Loss G_Z: 0.7666\n",
      "epoch: 160 Val H_real: -0.1793 Val H_fake: -0.1351 Val Loss G_H: 0.7695 Val Loss G_Z: 0.7775\n",
      "epoch: 180 Val H_real: -0.2071 Val H_fake: -0.1694 Val Loss G_H: 0.7882 Val Loss G_Z: 0.7956\n",
      "epoch: 200 Val H_real: -0.2372 Val H_fake: -0.2018 Val Loss G_H: 0.8061 Val Loss G_Z: 0.8155\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/30/fold4/generator.pth\n",
      "epoch: 0 Val H_real: -0.0137 Val H_fake: -0.0521 Val Loss G_H: 0.7297 Val Loss G_Z: 0.6860\n",
      "epoch: 20 Val H_real: 0.0008 Val H_fake: -0.0176 Val Loss G_H: 0.7093 Val Loss G_Z: 0.7116\n",
      "epoch: 40 Val H_real: -0.0144 Val H_fake: -0.0090 Val Loss G_H: 0.7046 Val Loss G_Z: 0.7117\n",
      "epoch: 60 Val H_real: -0.0462 Val H_fake: -0.0060 Val Loss G_H: 0.7028 Val Loss G_Z: 0.7140\n",
      "epoch: 80 Val H_real: -0.0575 Val H_fake: -0.0151 Val Loss G_H: 0.7069 Val Loss G_Z: 0.7221\n",
      "epoch: 100 Val H_real: -0.0704 Val H_fake: -0.0313 Val Loss G_H: 0.7151 Val Loss G_Z: 0.7289\n",
      "epoch: 120 Val H_real: -0.0801 Val H_fake: -0.0458 Val Loss G_H: 0.7225 Val Loss G_Z: 0.7350\n",
      "epoch: 140 Val H_real: -0.0897 Val H_fake: -0.0615 Val Loss G_H: 0.7306 Val Loss G_Z: 0.7401\n",
      "epoch: 160 Val H_real: -0.1011 Val H_fake: -0.0765 Val Loss G_H: 0.7386 Val Loss G_Z: 0.7485\n",
      "epoch: 180 Val H_real: -0.1154 Val H_fake: -0.0902 Val Loss G_H: 0.7460 Val Loss G_Z: 0.7578\n",
      "epoch: 200 Val H_real: -0.1341 Val H_fake: -0.1092 Val Loss G_H: 0.7564 Val Loss G_Z: 0.7676\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/30/fold5/generator.pth\n",
      "epoch: 0 Val H_real: -0.0050 Val H_fake: -0.0312 Val Loss G_H: 0.7177 Val Loss G_Z: 0.6901\n",
      "epoch: 20 Val H_real: -0.0200 Val H_fake: -0.0289 Val Loss G_H: 0.7146 Val Loss G_Z: 0.7067\n",
      "epoch: 40 Val H_real: -0.0402 Val H_fake: -0.0183 Val Loss G_H: 0.7085 Val Loss G_Z: 0.7087\n",
      "epoch: 60 Val H_real: -0.0584 Val H_fake: -0.0206 Val Loss G_H: 0.7094 Val Loss G_Z: 0.7107\n",
      "epoch: 80 Val H_real: -0.0728 Val H_fake: -0.0336 Val Loss G_H: 0.7159 Val Loss G_Z: 0.7147\n",
      "epoch: 100 Val H_real: -0.0859 Val H_fake: -0.0457 Val Loss G_H: 0.7222 Val Loss G_Z: 0.7236\n",
      "epoch: 120 Val H_real: -0.0954 Val H_fake: -0.0559 Val Loss G_H: 0.7276 Val Loss G_Z: 0.7347\n",
      "epoch: 140 Val H_real: -0.1118 Val H_fake: -0.0695 Val Loss G_H: 0.7348 Val Loss G_Z: 0.7454\n",
      "epoch: 160 Val H_real: -0.1296 Val H_fake: -0.0868 Val Loss G_H: 0.7441 Val Loss G_Z: 0.7541\n",
      "epoch: 180 Val H_real: -0.1519 Val H_fake: -0.1081 Val Loss G_H: 0.7555 Val Loss G_Z: 0.7703\n",
      "epoch: 200 Val H_real: -0.1734 Val H_fake: -0.1302 Val Loss G_H: 0.7676 Val Loss G_Z: 0.7842\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/40/fold1/generator.pth\n",
      "epoch: 0 Val H_real: -0.0432 Val H_fake: -0.0698 Val Loss G_H: 0.7375 Val Loss G_Z: 0.6270\n",
      "epoch: 20 Val H_real: -0.0138 Val H_fake: -0.0160 Val Loss G_H: 0.7089 Val Loss G_Z: 0.7169\n",
      "epoch: 40 Val H_real: -0.0333 Val H_fake: -0.0127 Val Loss G_H: 0.7064 Val Loss G_Z: 0.7207\n",
      "epoch: 60 Val H_real: -0.0518 Val H_fake: -0.0188 Val Loss G_H: 0.7093 Val Loss G_Z: 0.7185\n",
      "epoch: 80 Val H_real: -0.0758 Val H_fake: -0.0256 Val Loss G_H: 0.7127 Val Loss G_Z: 0.7161\n",
      "epoch: 100 Val H_real: -0.0899 Val H_fake: -0.0401 Val Loss G_H: 0.7200 Val Loss G_Z: 0.7202\n",
      "epoch: 120 Val H_real: -0.1030 Val H_fake: -0.0569 Val Loss G_H: 0.7286 Val Loss G_Z: 0.7292\n",
      "epoch: 140 Val H_real: -0.1170 Val H_fake: -0.0740 Val Loss G_H: 0.7376 Val Loss G_Z: 0.7421\n",
      "epoch: 160 Val H_real: -0.1352 Val H_fake: -0.0952 Val Loss G_H: 0.7488 Val Loss G_Z: 0.7554\n",
      "epoch: 180 Val H_real: -0.1501 Val H_fake: -0.1124 Val Loss G_H: 0.7580 Val Loss G_Z: 0.7652\n",
      "epoch: 200 Val H_real: -0.1684 Val H_fake: -0.1317 Val Loss G_H: 0.7686 Val Loss G_Z: 0.7788\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/40/fold2/generator.pth\n",
      "epoch: 0 Val H_real: 0.0151 Val H_fake: -0.0188 Val Loss G_H: 0.7130 Val Loss G_Z: 0.6214\n",
      "epoch: 20 Val H_real: 0.0095 Val H_fake: -0.0069 Val Loss G_H: 0.7037 Val Loss G_Z: 0.7086\n",
      "epoch: 40 Val H_real: -0.0157 Val H_fake: 0.0172 Val Loss G_H: 0.6911 Val Loss G_Z: 0.7062\n",
      "epoch: 60 Val H_real: -0.0410 Val H_fake: 0.0187 Val Loss G_H: 0.6899 Val Loss G_Z: 0.6993\n",
      "epoch: 80 Val H_real: -0.0556 Val H_fake: -0.0007 Val Loss G_H: 0.6996 Val Loss G_Z: 0.7013\n",
      "epoch: 100 Val H_real: -0.0713 Val H_fake: -0.0176 Val Loss G_H: 0.7082 Val Loss G_Z: 0.7075\n",
      "epoch: 120 Val H_real: -0.0865 Val H_fake: -0.0328 Val Loss G_H: 0.7161 Val Loss G_Z: 0.7185\n",
      "epoch: 140 Val H_real: -0.1034 Val H_fake: -0.0483 Val Loss G_H: 0.7245 Val Loss G_Z: 0.7297\n",
      "epoch: 160 Val H_real: -0.1273 Val H_fake: -0.0737 Val Loss G_H: 0.7379 Val Loss G_Z: 0.7444\n",
      "epoch: 180 Val H_real: -0.1628 Val H_fake: -0.1096 Val Loss G_H: 0.7571 Val Loss G_Z: 0.7614\n",
      "epoch: 200 Val H_real: -0.1898 Val H_fake: -0.1433 Val Loss G_H: 0.7755 Val Loss G_Z: 0.7814\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/40/fold3/generator.pth\n",
      "epoch: 0 Val H_real: 0.0148 Val H_fake: -0.0065 Val Loss G_H: 0.7049 Val Loss G_Z: 0.6865\n",
      "epoch: 20 Val H_real: -0.0012 Val H_fake: -0.0242 Val Loss G_H: 0.7133 Val Loss G_Z: 0.7100\n",
      "epoch: 40 Val H_real: -0.0327 Val H_fake: -0.0180 Val Loss G_H: 0.7093 Val Loss G_Z: 0.7181\n",
      "epoch: 60 Val H_real: -0.0467 Val H_fake: -0.0288 Val Loss G_H: 0.7143 Val Loss G_Z: 0.7314\n",
      "epoch: 80 Val H_real: -0.0783 Val H_fake: -0.0421 Val Loss G_H: 0.7208 Val Loss G_Z: 0.7226\n",
      "epoch: 100 Val H_real: -0.1187 Val H_fake: -0.0619 Val Loss G_H: 0.7311 Val Loss G_Z: 0.7198\n",
      "epoch: 120 Val H_real: -0.1476 Val H_fake: -0.0880 Val Loss G_H: 0.7448 Val Loss G_Z: 0.7376\n",
      "epoch: 140 Val H_real: -0.1638 Val H_fake: -0.1041 Val Loss G_H: 0.7533 Val Loss G_Z: 0.7514\n",
      "epoch: 160 Val H_real: -0.1799 Val H_fake: -0.1256 Val Loss G_H: 0.7648 Val Loss G_Z: 0.7645\n",
      "epoch: 180 Val H_real: -0.2049 Val H_fake: -0.1541 Val Loss G_H: 0.7804 Val Loss G_Z: 0.7790\n",
      "epoch: 200 Val H_real: -0.2310 Val H_fake: -0.1861 Val Loss G_H: 0.7979 Val Loss G_Z: 0.8014\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/40/fold4/generator.pth\n",
      "epoch: 0 Val H_real: -0.0172 Val H_fake: -0.0128 Val Loss G_H: 0.7096 Val Loss G_Z: 0.6943\n",
      "epoch: 20 Val H_real: -0.0096 Val H_fake: -0.0171 Val Loss G_H: 0.7090 Val Loss G_Z: 0.7125\n",
      "epoch: 40 Val H_real: -0.0364 Val H_fake: -0.0077 Val Loss G_H: 0.7037 Val Loss G_Z: 0.7110\n",
      "epoch: 60 Val H_real: -0.0600 Val H_fake: -0.0218 Val Loss G_H: 0.7106 Val Loss G_Z: 0.7114\n",
      "epoch: 80 Val H_real: -0.0864 Val H_fake: -0.0536 Val Loss G_H: 0.7267 Val Loss G_Z: 0.7154\n",
      "epoch: 100 Val H_real: -0.1130 Val H_fake: -0.0729 Val Loss G_H: 0.7365 Val Loss G_Z: 0.7276\n",
      "epoch: 120 Val H_real: -0.1309 Val H_fake: -0.0893 Val Loss G_H: 0.7452 Val Loss G_Z: 0.7372\n",
      "epoch: 140 Val H_real: -0.1446 Val H_fake: -0.1001 Val Loss G_H: 0.7511 Val Loss G_Z: 0.7499\n",
      "epoch: 160 Val H_real: -0.1630 Val H_fake: -0.1180 Val Loss G_H: 0.7608 Val Loss G_Z: 0.7579\n",
      "epoch: 180 Val H_real: -0.1821 Val H_fake: -0.1402 Val Loss G_H: 0.7729 Val Loss G_Z: 0.7737\n",
      "epoch: 200 Val H_real: -0.2069 Val H_fake: -0.1662 Val Loss G_H: 0.7872 Val Loss G_Z: 0.7934\n",
      "Early stopping ativado!\n",
      "Modelo salvo em: ./models_saved/cyclegan/3/40/fold5/generator.pth\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"CycleGAN.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape, mean_absolute_error as mae,mean_squared_error as mse\n",
    "import sys\n",
    "from torch.utils.data import Dataset\n",
    "import random, torch, os, numpy as np\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from matplotlib.path import Path\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "# === UNet Generator ===\n",
    "\n",
    "\n",
    "def asmape(y_true, y_pred, mask=None):\n",
    "    if mask is not None:\n",
    "         y_true, y_pred = y_true[mask==1], y_pred[mask==1]\n",
    "    if type(y_true) is list or type(y_pred) is list:\n",
    "         y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    len_ = len(y_true)\n",
    "    tmp = 100 * (np.nansum(np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))/len_)\n",
    "\n",
    "    return tmp\n",
    "\n",
    "\n",
    "\n",
    "class LoaderDataset(Dataset):\n",
    "    def __init__(self, root_zebra, root_horse, root_masks, chanels=3):\n",
    "        self.root_zebra = root_zebra\n",
    "        self.root_horse = root_horse\n",
    "        self.root_index = root_masks\n",
    "        \n",
    "        self.zebra_images = sorted(os.listdir(root_zebra))\n",
    "        self.horse_images = sorted(os.listdir(root_horse))\n",
    "        self.index = sorted(os.listdir(root_masks))\n",
    "\n",
    "        self.length_dataset = max(len(self.zebra_images), len(self.horse_images))\n",
    "        self.zebra_len = len(self.zebra_images)\n",
    "        self.horse_len = len(self.horse_images)\n",
    "        self.index_len = len(self.index)\n",
    "        self.chanels = chanels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_normalize(image):\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        min_val = torch.min(image)\n",
    "        max_val = torch.max(image)\n",
    "        scale = torch.clamp(max_val - min_val, min=1e-5)  # Evita diviso por zero\n",
    "        image_normalized = 2 * (image - min_val) / scale - 1  # Escala para [-1, 1]\n",
    "        return image_normalized, min_val, max_val\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        zebra_img = self.zebra_images[index % self.zebra_len]\n",
    "        horse_img = self.horse_images[index % self.horse_len]\n",
    "        index_ids = self.index[index % self.index_len]\n",
    "\n",
    "        zebra_path = os.path.join(self.root_zebra, zebra_img)\n",
    "        horse_path = os.path.join(self.root_horse, horse_img)\n",
    "        index_path = os.path.join(self.root_index, index_ids)\n",
    "        # print(zebra_path, horse_path, index_path)\n",
    "\n",
    "        zebra_img = np.load(zebra_path)\n",
    "        horse_img = np.load(horse_path)\n",
    "        mask = np.load(index_path)\n",
    "\n",
    "        if len(zebra_img.shape) > 3:\n",
    "            zebra_img = zebra_img.reshape(32, 32, 3)\n",
    "            horse_img = horse_img.reshape(32, 32, 3)\n",
    "\n",
    "        zebra_img = np.transpose(zebra_img, (2, 0, 1))\n",
    "        horse_img = np.transpose(horse_img, (2, 0, 1))\n",
    "\n",
    "        if self.chanels == 2:\n",
    "            zebra_img = zebra_img[:2, :, :]\n",
    "            horse_img = horse_img[:2, :, :]\n",
    "        elif self.chanels == 1:\n",
    "            zebra_img = np.sum(zebra_img, axis=0, keepdims=True)\n",
    "            horse_img = np.sum(horse_img, axis=0, keepdims=True)\n",
    "\n",
    "        zebra_img, min_val_z, max_val_z = LoaderDataset.custom_normalize(zebra_img)\n",
    "        horse_img, _, _ = LoaderDataset.custom_normalize(horse_img)\n",
    "\n",
    "        mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "        return zebra_img, horse_img, min_val_z, max_val_z, mask\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# class ToFloat32:\n",
    "#     def __call__(self, image, **kwargs):\n",
    "#         return image.float()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Instance Normalization Custom (como no TF) ===\n",
    "class InstanceNormalization(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        # escala e offset sero inicializados no forward com parmetros registrados\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (N,C,H,W)\n",
    "        mean = x.mean(dim=[2,3], keepdim=True)\n",
    "        var = x.var(dim=[2,3], keepdim=True, unbiased=False)\n",
    "        inv = 1.0 / torch.sqrt(var + self.epsilon)\n",
    "        normalized = (x - mean) * inv\n",
    "\n",
    "        # Criar escala e offset param se no existirem\n",
    "        if not hasattr(self, 'scale'):\n",
    "            self.scale = nn.Parameter(torch.ones(x.size(1), device=x.device))\n",
    "            self.offset = nn.Parameter(torch.zeros(x.size(1), device=x.device))\n",
    "        # reshape para broadcast\n",
    "        scale = self.scale.view(1, -1, 1, 1)\n",
    "        offset = self.offset.view(1, -1, 1, 1)\n",
    "        return scale * normalized + offset\n",
    "\n",
    "# === Downsample e Upsample ===\n",
    "def downsample(in_ch, out_ch, norm_type='batchnorm', apply_norm=True):\n",
    "    layers = [nn.Conv2d(in_ch, out_ch, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "    if apply_norm:\n",
    "        if norm_type == 'batchnorm':\n",
    "            layers.append(nn.BatchNorm2d(out_ch))\n",
    "        elif norm_type == 'instancenorm':\n",
    "            layers.append(InstanceNormalization())\n",
    "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def upsample(in_ch, out_ch, norm_type='batchnorm', apply_dropout=False):\n",
    "    layers = [nn.ConvTranspose2d(in_ch, out_ch, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "    if norm_type == 'batchnorm':\n",
    "        layers.append(nn.BatchNorm2d(out_ch))\n",
    "    elif norm_type == 'instancenorm':\n",
    "        layers.append(InstanceNormalization())\n",
    "    layers.append(nn.ReLU(inplace=True))\n",
    "    if apply_dropout:\n",
    "        layers.append(nn.Dropout(0.5))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=3, norm_type='batchnorm', target_size=256):\n",
    "        super().__init__()\n",
    "        self.target_size = target_size\n",
    "        self.down1 = downsample(input_channels, 64, norm_type, apply_norm=False)\n",
    "        self.down2 = downsample(64, 128, norm_type)\n",
    "        self.down3 = downsample(128, 256, norm_type)\n",
    "        self.down4 = downsample(256, 512, norm_type)\n",
    "        self.down5 = downsample(512, 512, norm_type)\n",
    "        self.down6 = downsample(512, 512, norm_type)\n",
    "        self.down7 = downsample(512, 512, norm_type)\n",
    "        self.down8 = downsample(512, 512, norm_type)\n",
    "\n",
    "        self.up1 = upsample(512, 512, norm_type, apply_dropout=True)\n",
    "        self.up2 = upsample(1024, 512, norm_type, apply_dropout=True)\n",
    "        self.up3 = upsample(1024, 512, norm_type, apply_dropout=True)\n",
    "        self.up4 = upsample(1024, 512, norm_type)\n",
    "        self.up5 = upsample(1024, 256, norm_type)\n",
    "        self.up6 = upsample(512, 128, norm_type)\n",
    "        self.up7 = upsample(256, 64, norm_type)\n",
    "\n",
    "        self.final = nn.ConvTranspose2d(128, output_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        orig_size = x.shape[-2:]  # (H, W)\n",
    "\n",
    "        # Upsample entrada para target_size x target_size\n",
    "        x = F.interpolate(x, size=(self.target_size, self.target_size), mode='bilinear', align_corners=False)\n",
    "\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "\n",
    "        u1 = self.up1(d8)\n",
    "        u1 = torch.cat([u1, d7], dim=1)\n",
    "\n",
    "        u2 = self.up2(u1)\n",
    "        u2 = torch.cat([u2, d6], dim=1)\n",
    "\n",
    "        u3 = self.up3(u2)\n",
    "        u3 = torch.cat([u3, d5], dim=1)\n",
    "\n",
    "        u4 = self.up4(u3)\n",
    "        u4 = torch.cat([u4, d4], dim=1)\n",
    "\n",
    "        u5 = self.up5(u4)\n",
    "        u5 = torch.cat([u5, d3], dim=1)\n",
    "\n",
    "        u6 = self.up6(u5)\n",
    "        u6 = torch.cat([u6, d2], dim=1)\n",
    "\n",
    "        u7 = self.up7(u6)\n",
    "        u7 = torch.cat([u7, d1], dim=1)\n",
    "\n",
    "        output = self.final(u7)\n",
    "        output = self.tanh(output)\n",
    "\n",
    "        # Downsample a sada para o tamanho original da entrada\n",
    "        output = F.interpolate(output, size=orig_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return output\n",
    "\n",
    "# === PatchGAN Discriminator ===\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels=3, norm_type='batchnorm', target=True):\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "        in_ch = input_channels * 2 if target else input_channels\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 64, kernel_size=4, stride=2, padding=1),  # no norm\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            downsample(64, 128, norm_type),\n",
    "            downsample(128, 256, norm_type),\n",
    "\n",
    "            nn.ZeroPad2d(1),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "\n",
    "            nn.BatchNorm2d(512) if norm_type == 'batchnorm' else InstanceNormalization(),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ZeroPad2d(1),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, inp, target=None):\n",
    "        if self.target and target is not None:\n",
    "            x = torch.cat([inp, target], dim=1)\n",
    "        else:\n",
    "            x = inp\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"models/checkpoint.pth.tar\"):\n",
    "    if not os.path.exists(filename.split('/')[0]):\n",
    "      os.makedirs(filename.split('/')[0])\n",
    "      \n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "def add_masked_gaussian_noise(x: torch.Tensor, mask: torch.Tensor, sigma: float) -> torch.Tensor:\n",
    "    # x: [B,C,H,W], mask: [B,1,H,W]\n",
    "    inv = (1.0 - mask).expand(-1, x.size(1), -1, -1)\n",
    "    noise = torch.randn_like(x) * sigma\n",
    "    return x + inv * noise\n",
    "\n",
    "\n",
    "\n",
    "def james_stein_reduce(errors: torch.Tensor) -> torch.Tensor:\n",
    "    mean = torch.mean(errors)\n",
    "    var = torch.var(errors, unbiased=False)\n",
    "    norm_sq = torch.sum((errors - mean) ** 2)\n",
    "    dim = errors.numel()\n",
    "    shrinkage = torch.clamp(1 - ((dim - 2) * var / (norm_sq + 1e-8)), min=0.0)\n",
    "    # reduz o vetor inteiro para escalar ajustado\n",
    "    return mean + shrinkage * (errors - mean).mean()\n",
    "\n",
    "def train_fn(\n",
    "    disc_H, disc_Z, gen_Z, gen_H, loader, val_loader,\n",
    "    opt_disc, opt_gen, l1_loss_fn, bce_loss_fn,  # l1 e bce com reduction='none'\n",
    "    d_scaler, g_scaler,\n",
    "    channel, taxa, fold, pasciencia=1000\n",
    "):\n",
    "    best_val_loss = float('inf')\n",
    "    output_buffer = \"\"\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        disc_H.train()\n",
    "        disc_Z.train()\n",
    "        gen_H.train()\n",
    "        gen_Z.train()\n",
    "\n",
    "        initial_sigma = 0.1\n",
    "        final_sigma = 0.0\n",
    "        warmup_epochs = 100\n",
    "        sigma = max(0.0, initial_sigma * (1 - epoch / warmup_epochs))\n",
    "\n",
    "        H_reals, H_fakes = 0.0, 0.0\n",
    "\n",
    "        for zebra, horse, _, _, masks in loader:\n",
    "            zebra = zebra.to(DEVICE)\n",
    "            horse = horse.to(DEVICE)\n",
    "            masks = masks.to(DEVICE).view(-1, 1, 32, 32).float()\n",
    "\n",
    "            zebra_noisy = add_masked_gaussian_noise(zebra, masks, sigma=sigma)\n",
    "            horse_noisy = add_masked_gaussian_noise(horse, masks, sigma=sigma)\n",
    "\n",
    "            # === Treina discriminadores ===\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                fake_horse = gen_H(zebra_noisy)\n",
    "                fake_zebra = gen_Z(horse_noisy)\n",
    "\n",
    "                # Disc_H real/fake\n",
    "                D_H_real = disc_H(horse * masks, horse * masks)\n",
    "                D_H_fake = disc_H(fake_horse.detach() * masks, horse * masks)\n",
    "\n",
    "                D_H_real_loss_tensor = bce_loss_fn(D_H_real, torch.ones_like(D_H_real))\n",
    "                D_H_fake_loss_tensor = bce_loss_fn(D_H_fake, torch.zeros_like(D_H_fake))\n",
    "                D_H_real_loss = james_stein_reduce(D_H_real_loss_tensor.view(-1))\n",
    "                D_H_fake_loss = james_stein_reduce(D_H_fake_loss_tensor.view(-1))\n",
    "                D_H_loss = (D_H_real_loss + D_H_fake_loss) * 0.5\n",
    "\n",
    "                # Disc_Z real/fake\n",
    "                D_Z_real = disc_Z(zebra * masks, zebra * masks)\n",
    "                D_Z_fake = disc_Z(fake_zebra.detach() * masks, zebra * masks)\n",
    "\n",
    "                D_Z_real_loss_tensor = bce_loss_fn(D_Z_real, torch.ones_like(D_Z_real))\n",
    "                D_Z_fake_loss_tensor = bce_loss_fn(D_Z_fake, torch.zeros_like(D_Z_fake))\n",
    "                D_Z_real_loss = james_stein_reduce(D_Z_real_loss_tensor.view(-1))\n",
    "                D_Z_fake_loss = james_stein_reduce(D_Z_fake_loss_tensor.view(-1))\n",
    "                D_Z_loss = (D_Z_real_loss + D_Z_fake_loss) * 0.5\n",
    "\n",
    "                D_loss = D_H_loss + D_Z_loss\n",
    "\n",
    "            opt_disc.zero_grad()\n",
    "            d_scaler.scale(D_loss).backward()\n",
    "            d_scaler.step(opt_disc)\n",
    "            d_scaler.update()\n",
    "\n",
    "            # === Treina geradores ===\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                D_H_fake = disc_H(fake_horse * masks, horse * masks)\n",
    "                D_Z_fake = disc_Z(fake_zebra * masks, zebra * masks)\n",
    "\n",
    "                loss_G_H_tensor = bce_loss_fn(D_H_fake, torch.ones_like(D_H_fake))\n",
    "                loss_G_Z_tensor = bce_loss_fn(D_Z_fake, torch.ones_like(D_Z_fake))\n",
    "                loss_G_H = james_stein_reduce(loss_G_H_tensor.view(-1))\n",
    "                loss_G_Z = james_stein_reduce(loss_G_Z_tensor.view(-1))\n",
    "\n",
    "                cycle_zebra = gen_Z(fake_horse)\n",
    "                cycle_horse = gen_H(fake_zebra)\n",
    "\n",
    "                cycle_zebra_loss_tensor = l1_loss_fn(zebra * masks, cycle_zebra * masks)\n",
    "                cycle_horse_loss_tensor = l1_loss_fn(horse * masks, cycle_horse * masks)\n",
    "                cycle_zebra_loss = james_stein_reduce(cycle_zebra_loss_tensor.view(-1))\n",
    "                cycle_horse_loss = james_stein_reduce(cycle_horse_loss_tensor.view(-1))\n",
    "\n",
    "                G_loss = loss_G_H + loss_G_Z + LAMBDA_CYCLE * (cycle_zebra_loss + cycle_horse_loss)\n",
    "\n",
    "            opt_gen.zero_grad()\n",
    "            g_scaler.scale(G_loss).backward()\n",
    "            g_scaler.step(opt_gen)\n",
    "            g_scaler.update()\n",
    "\n",
    "            H_reals += D_H_real.mean().item()\n",
    "            H_fakes += D_H_fake.mean().item()\n",
    "\n",
    "        # === Validao ===\n",
    "        disc_H.eval()\n",
    "        disc_Z.eval()\n",
    "        gen_H.eval()\n",
    "        gen_Z.eval()\n",
    "\n",
    "        val_H_reals, val_H_fakes = 0.0, 0.0\n",
    "        val_loss_G_H_total, val_loss_G_Z_total = 0.0, 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for zebra, horse, _, _, masksv in val_loader:\n",
    "                zebra = zebra.to(DEVICE)\n",
    "                horse = horse.to(DEVICE)\n",
    "                masksv = masksv.to(DEVICE).view(-1, 1, 32, 32).float()\n",
    "\n",
    "                zebra_noisy = add_masked_gaussian_noise(zebra, masksv, sigma=0.1)\n",
    "                horse_noisy = add_masked_gaussian_noise(horse, masksv, sigma=0.1)\n",
    "\n",
    "                fake_horse = gen_H(zebra_noisy)\n",
    "                fake_zebra = gen_Z(horse_noisy)\n",
    "\n",
    "                D_H_real = disc_H(horse * masksv, horse * masksv)\n",
    "                D_H_fake = disc_H(fake_horse * masksv, horse * masksv)\n",
    "                loss_G_H_tensor = bce_loss_fn(D_H_fake, torch.ones_like(D_H_fake))\n",
    "                loss_G_H = james_stein_reduce(loss_G_H_tensor.view(-1))\n",
    "\n",
    "                D_Z_real = disc_Z(zebra * masksv, zebra * masksv)\n",
    "                D_Z_fake = disc_Z(fake_zebra * masksv, zebra * masksv)\n",
    "                loss_G_Z_tensor = bce_loss_fn(D_Z_fake, torch.ones_like(D_Z_fake))\n",
    "                loss_G_Z = james_stein_reduce(loss_G_Z_tensor.view(-1))\n",
    "\n",
    "                val_H_reals += D_H_real.mean().item()\n",
    "                val_H_fakes += D_H_fake.mean().item()\n",
    "                val_loss_G_H_total += loss_G_H.item()\n",
    "                val_loss_G_Z_total += loss_G_Z.item()\n",
    "\n",
    "        val_H_reals /= len(val_loader)\n",
    "        val_H_fakes /= len(val_loader)\n",
    "        val_loss_G_H_total /= len(val_loader)\n",
    "        val_loss_G_Z_total /= len(val_loader)\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            output_buffer += (\n",
    "                f\"epoch: {epoch} \"\n",
    "                f\"Val H_real: {val_H_reals:.4f} Val H_fake: {val_H_fakes:.4f} \"\n",
    "                f\"Val Loss G_H: {val_loss_G_H_total:.4f} Val Loss G_Z: {val_loss_G_Z_total:.4f}\\n\"\n",
    "            )\n",
    "            sys.stdout.write(output_buffer)\n",
    "            output_buffer = \"\"\n",
    "\n",
    "        if val_loss_G_Z_total < best_val_loss:\n",
    "            best_val_loss = val_loss_G_Z_total\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= pasciencia:\n",
    "            print(\"Early stopping ativado!\")\n",
    "            break\n",
    "\n",
    "    save_dir = f\"./models_saved/cyclegan/{channel}/{taxa}/fold{fold}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    model_path = os.path.join(save_dir, \"generator.pth\")\n",
    "    torch.save(gen_Z.state_dict(), model_path)\n",
    "    print(f\"Save model: {model_path}\")\n",
    "\n",
    "    return gen_Z\n",
    "\n",
    "\n",
    "def test(gen_Z, test_loader, taxa, fold, chanells):\n",
    "\t\tDEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\t\tgen_Z.eval()\n",
    "\t\t\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# Criar o DataFrame com as colunas desejadas\n",
    "\t\t\tdf = pd.DataFrame([], columns=['mae', 'asmape' ,'mape', 'rmse', 'scale'], index=test_loader.dataset.horse_images)\n",
    "\n",
    "\t\t\tfor (zebra, horse, std_val, mean_val, mask), name in zip(test_loader, test_loader.dataset.horse_images):\n",
    "\t\t\t\t# Verificar as dimenses das entradas\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Mover dados para o dispositivo\n",
    "\t\t\t\tzebra = zebra.to(DEVICE)\n",
    "\t\t\t\thorse = horse.to(DEVICE)\n",
    "\n",
    "\t\t\t\t# Converter std_val e mean_val para tensores e mov-los para o dispositivo\n",
    "\t\t\t\tstd_val = torch.tensor(std_val, device=DEVICE) if not isinstance(std_val, torch.Tensor) else std_val.to(DEVICE)\n",
    "\t\t\t\tmean_val = torch.tensor(mean_val, device=DEVICE) if not isinstance(mean_val, torch.Tensor) else mean_val.to(DEVICE)\n",
    "\n",
    "\t\t\t\t# Gerar fake_zebra usando o gerador\n",
    "\t\t\t\tfake_zebra = gen_Z(horse)\n",
    "\n",
    "\t\t\t\t# Mover apenas as imagens para a CPU antes de operaes subsequentes\n",
    "\t\t\t\tzebra = zebra.cpu()\n",
    "\t\t\t\tfake_zebra = fake_zebra.cpu()\n",
    "\n",
    "\t\t\t\t# Voltar para escala original \n",
    "\t\t\t\tzebra = zebra * std_val.cpu() + mean_val.cpu()\n",
    "\t\t\t\tfake_zebra = fake_zebra * std_val.cpu() + mean_val.cpu()\n",
    "\n",
    "\t\t\t\t# Somar sobre o canal e achatar as imagens\n",
    "\t\t\t\tzebra = torch.sum(zebra, dim=1).flatten()\n",
    "\t\t\t\tfake_zebra = torch.sum(fake_zebra, dim=1).flatten()\n",
    "\n",
    "\t\t\t\t# Calcular as mtricas\n",
    "\t\t\t\tzebra_np = zebra*mask\n",
    "\t\t\t\tfake_zebra_np = fake_zebra*mask\n",
    "\n",
    "\t\t\t\t# Calcular as mtricas corretamente\n",
    "\t\t\t\tmae_value = round(mae(zebra_np, fake_zebra_np), 3)\n",
    "\t\t\t\tmape_value = round(mape(zebra_np, fake_zebra_np) * 100, 3)\n",
    "\t\t\t\trmse_value = round(np.sqrt(mse(zebra_np, fake_zebra_np)), 3)\n",
    "\t\t\t\tsmape_value = round(asmape(zebra_np, fake_zebra_np,mask=mask), 3)\n",
    "\t\t\t\t# Adicionar os resultados ao DataFram\n",
    "\t\t\t\tdf.loc[name] = [mae_value, smape_value, mape_value, rmse_value, np.max(zebra.numpy()) - np.min(zebra.numpy())]\n",
    "\n",
    "\t\t\t# Salvar o DataFrame em um arquivo CSV\n",
    "\t\t\tdirectory =  \"./resultados/resultados_ciclegan\"\n",
    "\t\t\tif not os.path.exists(directory):\n",
    "\t\t\t\t\tos.makedirs(directory)\n",
    "\n",
    "\t\t\tdf.to_csv(os.path.join(directory, f'result_{str(chanells)}c_{taxa}_{fold}.csv'))               \n",
    "\n",
    "\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"using device: {DEVICE}\")\n",
    "\n",
    "import os\n",
    "\n",
    "TRAIN_DIR = os.path.abspath(\"../dataset_final\")  \n",
    "VAL_DIR = os.path.abspath(\"../dataset_final\")  \n",
    "INDEX_TRAIN = os.path.abspath(\"../dataset_final\")  \n",
    "INDEX_VAL = os.path.abspath(\"../dataset_final\")  \n",
    "INDEX_TEST = os.path.abspath(\"../dataset_final\")  \n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-6\n",
    "LAMBDA_IDENTITY = 0.0 # loss weight for identity loss\n",
    "LAMBDA_CYCLE = 10\n",
    "NUM_WORKERS = 0\n",
    "NUM_EPOCHS = 50000\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = True\n",
    "\n",
    "\n",
    "def main(in_channels):\n",
    "    for taxa in ['10', '20', '30', '40']:\n",
    "        for fold in ['1', '2', '3', '4', '5']:\n",
    "            disc_H = PatchDiscriminator(input_channels=in_channels, norm_type='instancenorm').to(DEVICE)\n",
    "            disc_Z = PatchDiscriminator(input_channels=in_channels, norm_type='instancenorm').to(DEVICE)\n",
    "            gen_Z = UNetGenerator(input_channels=in_channels, output_channels=in_channels, norm_type='instancenorm').to(DEVICE)\n",
    "            gen_H = UNetGenerator(input_channels=in_channels, output_channels=in_channels, norm_type='instancenorm').to(DEVICE)\n",
    "\n",
    "            opt_disc = optim.Adam(\n",
    "                list(disc_H.parameters()) + list(disc_Z.parameters()),\n",
    "                lr=LEARNING_RATE,\n",
    "                betas=(0.5, 0.999),\n",
    "            )\n",
    "\n",
    "            opt_gen = optim.Adam(\n",
    "                list(gen_Z.parameters()) + list(gen_H.parameters()),\n",
    "                lr=LEARNING_RATE,\n",
    "                betas=(0.5, 0.999),\n",
    "            )\n",
    "\n",
    "            # Perdas com reduction='none' para usar James-Stein na reduo\n",
    "            l1_loss_fn = nn.L1Loss(reduction='none')\n",
    "            bce_loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "            dataset = LoaderDataset(\n",
    "                root_zebra=os.path.join(TRAIN_DIR, \"label\", str(taxa), \"folds\", f\"fold{fold}\", \"train\"),\n",
    "                root_horse=os.path.join(TRAIN_DIR, \"input\", str(taxa), \"folds\", f\"fold{fold}\", \"train\"),\n",
    "                root_masks=os.path.join(INDEX_TRAIN, \"input\", str(taxa), \"folds\", f\"fold{fold}\", \"index_train\"),\n",
    "                chanels=in_channels\n",
    "            )\n",
    "\n",
    "            val_dataset = LoaderDataset(\n",
    "                root_zebra=os.path.join(VAL_DIR, \"label\", str(taxa), \"folds\", f\"fold{fold}\", \"val\"),\n",
    "                root_horse=os.path.join(VAL_DIR, \"input\", str(taxa), \"folds\", f\"fold{fold}\", \"val\"),\n",
    "                root_masks=os.path.join(INDEX_VAL, \"input\", str(taxa), \"folds\", f\"fold{fold}\", \"index_val\"),\n",
    "                chanels=in_channels\n",
    "            )\n",
    "\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=False,\n",
    "                pin_memory=False,\n",
    "                num_workers=NUM_WORKERS,\n",
    "            )\n",
    "\n",
    "            loader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "                num_workers=NUM_WORKERS,\n",
    "                pin_memory=False,\n",
    "            )\n",
    "\n",
    "            g_scaler = torch.amp.GradScaler(True)\n",
    "            d_scaler = torch.amp.GradScaler(True)\n",
    "\n",
    "            gen_Z = train_fn(\n",
    "                disc_H,\n",
    "                disc_Z,\n",
    "                gen_Z,\n",
    "                gen_H,\n",
    "                loader,\n",
    "                val_loader,\n",
    "                opt_disc,\n",
    "                opt_gen,\n",
    "                l1_loss_fn,\n",
    "                bce_loss_fn,\n",
    "                d_scaler,\n",
    "                g_scaler,\n",
    "                in_channels,\n",
    "                taxa,\n",
    "                fold,\n",
    "                pasciencia=200\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for i in [1,2,3]:\n",
    "        print(f'channels:{i}')\n",
    "        main(in_channels=i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_envmau",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
