{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape, mean_absolute_error as mae, mean_squared_error as mse\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def asmape(y_true, y_pred, mask=None):\n",
    "    if mask is not None:\n",
    "         y_true, y_pred = y_true[mask==1], y_pred[mask==1]\n",
    "    if type(y_true) is list or type(y_pred) is list:\n",
    "         y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    len_ = len(y_true)\n",
    "    tmp = 100 * (np.nansum(np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))/len_)\n",
    "\n",
    "    return tmp\n",
    "\n",
    "\n",
    "class LoaderDataset(Dataset):\n",
    "    def __init__(self, root_zebra, root_horse, root_masks, chanels=3):\n",
    "        self.root_zebra = root_zebra\n",
    "        self.root_horse = root_horse\n",
    "        self.root_index = root_masks\n",
    "        \n",
    "        self.zebra_images = sorted(os.listdir(root_zebra))\n",
    "        self.horse_images = sorted(os.listdir(root_horse))\n",
    "        self.index = sorted(os.listdir(root_masks))\n",
    "\n",
    "        self.length_dataset = max(len(self.zebra_images), len(self.horse_images))\n",
    "        self.zebra_len = len(self.zebra_images)\n",
    "        self.horse_len = len(self.horse_images)\n",
    "        self.index_len = len(self.index)\n",
    "        self.chanels = chanels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_normalize(image):\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        min_val = torch.min(image)\n",
    "        max_val = torch.max(image)\n",
    "        scale = torch.clamp(max_val - min_val, min=1e-5)  # Evita divisão por zero\n",
    "        image_normalized = 2 * (image - min_val) / scale - 1  # Escala para [-1, 1]\n",
    "        return image_normalized, min_val, max_val\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        zebra_img = self.zebra_images[index % self.zebra_len]\n",
    "        horse_img = self.horse_images[index % self.horse_len]\n",
    "        index_ids = self.index[index % self.index_len]\n",
    "\n",
    "        zebra_path = os.path.join(self.root_zebra, zebra_img)\n",
    "        horse_path = os.path.join(self.root_horse, horse_img)\n",
    "        index_path = os.path.join(self.root_index, index_ids)\n",
    "        # print(zebra_path, horse_path, index_path)\n",
    "\n",
    "        zebra_img = np.load(zebra_path)\n",
    "        horse_img = np.load(horse_path)\n",
    "        mask = np.load(index_path)\n",
    "\n",
    "        if len(zebra_img.shape) > 3:\n",
    "            zebra_img = zebra_img.reshape(32, 32, 3)\n",
    "            horse_img = horse_img.reshape(32, 32, 3)\n",
    "\n",
    "        zebra_img = np.transpose(zebra_img, (2, 0, 1))\n",
    "        horse_img = np.transpose(horse_img, (2, 0, 1))\n",
    "\n",
    "        if self.chanels == 2:\n",
    "            zebra_img = zebra_img[:2, :, :]\n",
    "            horse_img = horse_img[:2, :, :]\n",
    "        elif self.chanels == 1:\n",
    "            zebra_img = np.sum(zebra_img, axis=0, keepdims=True)\n",
    "            horse_img = np.sum(horse_img, axis=0, keepdims=True)\n",
    "\n",
    "        zebra_img, min_val_z, max_val_z = LoaderDataset.custom_normalize(zebra_img)\n",
    "        horse_img, _, _ = LoaderDataset.custom_normalize(horse_img)\n",
    "\n",
    "        mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "        return zebra_img, horse_img, min_val_z, max_val_z, mask\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# class ToFloat32:\n",
    "#     def __call__(self, image, **kwargs):\n",
    "#         return image.float()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Instance Normalization Custom (como no TF) ===\n",
    "class InstanceNormalization(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        # escala e offset serão inicializados no forward com parâmetros registrados\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (N,C,H,W)\n",
    "        mean = x.mean(dim=[2,3], keepdim=True)\n",
    "        var = x.var(dim=[2,3], keepdim=True, unbiased=False)\n",
    "        inv = 1.0 / torch.sqrt(var + self.epsilon)\n",
    "        normalized = (x - mean) * inv\n",
    "\n",
    "        # Criar escala e offset param se não existirem\n",
    "        if not hasattr(self, 'scale'):\n",
    "            self.scale = nn.Parameter(torch.ones(x.size(1), device=x.device))\n",
    "            self.offset = nn.Parameter(torch.zeros(x.size(1), device=x.device))\n",
    "        # reshape para broadcast\n",
    "        scale = self.scale.view(1, -1, 1, 1)\n",
    "        offset = self.offset.view(1, -1, 1, 1)\n",
    "        return scale * normalized + offset\n",
    "\n",
    "# === Downsample e Upsample ===\n",
    "def downsample(in_ch, out_ch, norm_type='batchnorm', apply_norm=True):\n",
    "    layers = [nn.Conv2d(in_ch, out_ch, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "    if apply_norm:\n",
    "        if norm_type == 'batchnorm':\n",
    "            layers.append(nn.BatchNorm2d(out_ch))\n",
    "        elif norm_type == 'instancenorm':\n",
    "            layers.append(InstanceNormalization())\n",
    "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def upsample(in_ch, out_ch, norm_type='batchnorm', apply_dropout=False):\n",
    "    layers = [nn.ConvTranspose2d(in_ch, out_ch, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "    if norm_type == 'batchnorm':\n",
    "        layers.append(nn.BatchNorm2d(out_ch))\n",
    "    elif norm_type == 'instancenorm':\n",
    "        layers.append(InstanceNormalization())\n",
    "    layers.append(nn.ReLU(inplace=True))\n",
    "    if apply_dropout:\n",
    "        layers.append(nn.Dropout(0.5))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=3, norm_type='batchnorm', target_size=256):\n",
    "        super().__init__()\n",
    "        self.target_size = target_size\n",
    "        self.down1 = downsample(input_channels, 64, norm_type, apply_norm=False)\n",
    "        self.down2 = downsample(64, 128, norm_type)\n",
    "        self.down3 = downsample(128, 256, norm_type)\n",
    "        self.down4 = downsample(256, 512, norm_type)\n",
    "        self.down5 = downsample(512, 512, norm_type)\n",
    "        self.down6 = downsample(512, 512, norm_type)\n",
    "        self.down7 = downsample(512, 512, norm_type)\n",
    "        self.down8 = downsample(512, 512, norm_type)\n",
    "\n",
    "        self.up1 = upsample(512, 512, norm_type, apply_dropout=True)\n",
    "        self.up2 = upsample(1024, 512, norm_type, apply_dropout=True)\n",
    "        self.up3 = upsample(1024, 512, norm_type, apply_dropout=True)\n",
    "        self.up4 = upsample(1024, 512, norm_type)\n",
    "        self.up5 = upsample(1024, 256, norm_type)\n",
    "        self.up6 = upsample(512, 128, norm_type)\n",
    "        self.up7 = upsample(256, 64, norm_type)\n",
    "\n",
    "        self.final = nn.ConvTranspose2d(128, output_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        orig_size = x.shape[-2:]  # (H, W)\n",
    "\n",
    "        # Upsample entrada para target_size x target_size\n",
    "        x = F.interpolate(x, size=(self.target_size, self.target_size), mode='bilinear', align_corners=False)\n",
    "\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "\n",
    "        u1 = self.up1(d8)\n",
    "        u1 = torch.cat([u1, d7], dim=1)\n",
    "\n",
    "        u2 = self.up2(u1)\n",
    "        u2 = torch.cat([u2, d6], dim=1)\n",
    "\n",
    "        u3 = self.up3(u2)\n",
    "        u3 = torch.cat([u3, d5], dim=1)\n",
    "\n",
    "        u4 = self.up4(u3)\n",
    "        u4 = torch.cat([u4, d4], dim=1)\n",
    "\n",
    "        u5 = self.up5(u4)\n",
    "        u5 = torch.cat([u5, d3], dim=1)\n",
    "\n",
    "        u6 = self.up6(u5)\n",
    "        u6 = torch.cat([u6, d2], dim=1)\n",
    "\n",
    "        u7 = self.up7(u6)\n",
    "        u7 = torch.cat([u7, d1], dim=1)\n",
    "\n",
    "        output = self.final(u7)\n",
    "        output = self.tanh(output)\n",
    "\n",
    "        # Downsample a saída para o tamanho original da entrada\n",
    "        output = F.interpolate(output, size=orig_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return output\n",
    "\n",
    "# === PatchGAN Discriminator ===\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels=3, norm_type='batchnorm', target=True):\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "        in_ch = input_channels * 2 if target else input_channels\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 64, kernel_size=4, stride=2, padding=1),  # no norm\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            downsample(64, 128, norm_type),\n",
    "            downsample(128, 256, norm_type),\n",
    "\n",
    "            nn.ZeroPad2d(1),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "\n",
    "            nn.BatchNorm2d(512) if norm_type == 'batchnorm' else InstanceNormalization(),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ZeroPad2d(1),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, inp, target=None):\n",
    "        if self.target and target is not None:\n",
    "            x = torch.cat([inp, target], dim=1)\n",
    "        else:\n",
    "            x = inp\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Redução James-Stein\n",
    "def james_stein_reduce(errors: torch.Tensor) -> torch.Tensor:\n",
    "    mean = torch.mean(errors)\n",
    "    var = torch.var(errors, unbiased=False)\n",
    "    norm_sq = torch.sum((errors - mean) ** 2)\n",
    "    dim = errors.numel()\n",
    "    shrinkage = torch.clamp(1 - ((dim - 2) * var / (norm_sq + 1e-8)), min=0.0)\n",
    "    return mean + shrinkage * (errors - mean).mean()\n",
    "\n",
    "# Função de perda para o Discriminador\n",
    "def discriminator_loss(disc_real_output, disc_fake_output):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    real_labels = torch.ones_like(disc_real_output)\n",
    "    fake_labels = torch.zeros_like(disc_fake_output)\n",
    "    loss_real = criterion(disc_real_output, real_labels)\n",
    "    loss_fake = criterion(disc_fake_output, fake_labels)\n",
    "    return loss_real + loss_fake\n",
    "\n",
    "# Função de perda para o Gerador\n",
    "def generator_loss(disc_fake_output, generated_image, target_image, mask, lambda_l1=100):\n",
    "    criterion_GAN = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Perda GAN\n",
    "    real_labels = torch.ones_like(disc_fake_output )\n",
    "    loss_GAN = criterion_GAN(disc_fake_output , real_labels)\n",
    "\n",
    "    # Perda L1 com James-Stein\n",
    "    error = (generated_image - target_image) * mask  # erro mascarado\n",
    "    error_flat = error.view(-1)                      # achatar para aplicar James-Stein\n",
    "    loss_L1 = james_stein_reduce(torch.abs(error_flat)) * lambda_l1\n",
    "\n",
    "    loss_gen = loss_GAN + loss_L1\n",
    "    return loss_gen, loss_GAN, loss_L1\n",
    "\n",
    "\n",
    "def train_step(input_image, target_image, generator, discriminator, generator_optimizer, discriminator_optimizer, device):\n",
    "    \"\"\"\n",
    "    Executa um passo de treinamento para o Pix2Pix.\n",
    "    \n",
    "    :param input_image: Imagem de entrada (tensor) - shape [B, C_in, H, W]\n",
    "    :param target_image: Imagem alvo (tensor) - shape [B, C_out, H, W]\n",
    "    :param generator: Modelo Gerador\n",
    "    :param discriminator: Modelo Discriminador\n",
    "    :param generator_optimizer: Otimizador para o Gerador\n",
    "    :param discriminator_optimizer: Otimizador para o Discriminador\n",
    "    :param device: Dispositivo (CPU ou CUDA)\n",
    "    :return: Perdas do gerador e do discriminador\n",
    "    \"\"\"\n",
    "    LAMBDA = 100\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "   \n",
    "    # Zera os gradientes dos otimizadores\n",
    "    generator_optimizer.zero_grad()\n",
    "    discriminator_optimizer.zero_grad()\n",
    "    \n",
    "    with torch.amp.autocast(device_type='cuda', enabled=True):  # Mixed precision training\n",
    "        # Gera a imagem\n",
    "        generated_image = generator(input_image)\n",
    "        \n",
    "        # Concatenar input e imagens para o discriminador\n",
    "        real_combined = torch.cat([input_image, target_image], dim=1)  # [B, C_in + C_out, H, W]\n",
    "        fake_combined = torch.cat([input_image, generated_image.detach()], dim=1)  # [B, C_in + C_out, H, W]\n",
    "        \n",
    "        # Passar pelo Discriminador\n",
    "        disc_real_output = discriminator(real_combined)\n",
    "        disc_fake_output = discriminator(fake_combined)\n",
    "        \n",
    "        # Calcular a perda do Discriminador\n",
    "        loss_D = discriminator_loss(disc_real_output, disc_fake_output)\n",
    "        \n",
    "        # Calcular a perda do Gerador\n",
    "        fake_combined_for_gen = torch.cat([input_image, generated_image], dim=1)\n",
    "        disc_fake_output_for_gen = discriminator( )\n",
    "        loss_G, loss_GAN, loss_L1 = generator_loss(disc_fake_output_for_gen, generated_image, target_image, lambda_l1=LAMBDA)\n",
    "    \n",
    "    # Atualizar Discriminador\n",
    "    loss_D.backward()  # Retain graph para que os gradientes do Gerador não sejam perdidos\n",
    "    discriminator_optimizer.step()\n",
    "    \n",
    "    # Atualizar Gerador\n",
    "    loss_G.backward()\n",
    "    generator_optimizer.step()\n",
    "    \n",
    "    return loss_G.item(), loss_D.item(), loss_GAN.item(), loss_L1.item()\n",
    "\n",
    "\n",
    "def validate_step(input_image, target_image, generator, discriminator, device):\n",
    "    \"\"\"\n",
    "    Executa um passo de validação para o Pix2Pix.\n",
    "    \n",
    "    :param input_image: Imagem de entrada (tensor) - shape [B, C_in, H, W]\n",
    "    :param target_image: Imagem alvo (tensor) - shape [B, C_out, H, W]\n",
    "    :param generator: Modelo Gerador\n",
    "    :param discriminator: Modelo Discriminador\n",
    "    :param device: Dispositivo (CPU ou CUDA)\n",
    "    :return: Perdas do gerador e do discriminador\n",
    "    \"\"\"\n",
    "    LAMBDA = 100\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast(device_type='cuda', enabled=True):  # Mixed precision validation\n",
    "        # Gera a imagem\n",
    "        generated_image = generator(input_image)\n",
    "        \n",
    "        # Concatenar input e imagens para o discriminador\n",
    "        real_combined = torch.cat([input_image, target_image], dim=1)  # [B, C_in + C_out, H, W]\n",
    "        fake_combined = torch.cat([input_image, generated_image], dim=1)  # [B, C_in + C_out, H, W]\n",
    "        \n",
    "        # Passar pelo Discriminador\n",
    "        disc_real_output = discriminator(real_combined)\n",
    "        disc_fake_output = discriminator(fake_combined)\n",
    "        \n",
    "        # Calcular a perda do Discriminador\n",
    "        loss_D = discriminator_loss(disc_real_output, disc_fake_output)\n",
    "        \n",
    "        # Calcular a perda do Gerador\n",
    "        disc_fake_output_for_gen = discriminator(fake_combined)\n",
    "        loss_G, loss_GAN, loss_L1 = generator_loss(disc_fake_output_for_gen, generated_image, target_image, lambda_l1=LAMBDA)\n",
    "    \n",
    "    return loss_G.item(), loss_D.item(), loss_GAN.item(), loss_L1.item()\n",
    "\n",
    "def weights_init(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            if m.weight is not None:\n",
    "                nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.InstanceNorm2d):\n",
    "            if m.weight is not None:\n",
    "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "\n",
    "def add_masked_gaussian_noise(x: torch.Tensor, mask: torch.Tensor, sigma: float) -> torch.Tensor:\n",
    "    # x: [B,C,H,W], mask: [B,1,H,W]\n",
    "    inv = (1.0 - mask).expand(-1, x.size(1), -1, -1)\n",
    "    noise = torch.randn_like(x) * sigma\n",
    "    return x + inv * noise\n",
    "\n",
    "\n",
    "\n",
    "def james_stein_reduce(errors: torch.Tensor) -> torch.Tensor:\n",
    "    mean = torch.mean(errors)\n",
    "    var = torch.var(errors, unbiased=False)\n",
    "    norm_sq = torch.sum((errors - mean) ** 2)\n",
    "    dim = errors.numel()\n",
    "    shrinkage = torch.clamp(1 - ((dim - 2) * var / (norm_sq + 1e-8)), min=0.0)\n",
    "    # reduz o vetor inteiro para escalar ajustado\n",
    "    return mean + shrinkage * (errors - mean).mean()\n",
    "\n",
    "\n",
    "def train_and_validate(epochs, train_loader, val_loader, in_channels, taxa,fold,patience=100):\n",
    "\t\tDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\t\tsave_dir = f\"./models_saved/pix2pix/{in_channels}/{taxa}/fold{fold}\"\n",
    "\t\t# Instanciar Gerador e Discriminador\n",
    "\t\tgenerator = UNetGenerator(input_channels=in_channels, output_channels=in_channels).to(DEVICE)\n",
    "\t\tdiscriminator = PatchDiscriminator( input_channels=in_channels).to(DEVICE)\n",
    "\t\t\n",
    "\t# Multi-GPU (se disponível)\n",
    "\t\tif torch.cuda.device_count() > 1:\n",
    "\t\t\tprint(f\"Usando {torch.cuda.device_count()} GPUs\")\n",
    "\t\t\tgenerator = torch.nn.DataParallel(generator)\n",
    "\t\t\tdiscriminator = torch.nn.DataParallel(discriminator)\n",
    "\n",
    "\t\t# Inicializar Pesos\n",
    "\t\tgenerator.apply(weights_init)\n",
    "\t\tdiscriminator.apply(weights_init)\n",
    "\n",
    "\t\t# Configurar Otimizadores\n",
    "\t\tgenerator_optimizer = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.55, 0.999))\n",
    "\t\tdiscriminator_optimizer = optim.Adam(discriminator.parameters(), lr=1e-5, betas=(0.55, 0.999))\n",
    "\t\tbest_val_loss = float('inf')\n",
    "\t\toutput_buffer = \"\" \n",
    "                \n",
    "\t\tfor epoch in range(epochs):\n",
    "\t\t\t\tgenerator.train()\n",
    "\t\t\t\tdiscriminator.train()\n",
    "\t\t\t\tinitial_sigma = 0.1\n",
    "\t\t\t\tfinal_sigma = 0.0\n",
    "\t\t\t\twarmup_epochs = 100\n",
    "\t\t\t\tsigma = max(0.0, initial_sigma * (1 - epoch / warmup_epochs))\n",
    "\n",
    "\t\t\t\tfor input_image, target_image,_,_, masks in train_loader:\n",
    "\t\t\t\t\t\t# print(input_image.shape,target_image.shape)\n",
    "\t\t\t\t\t\tinput_image = input_image.to(DEVICE).to(torch.float32)\n",
    "\t\t\t\t\t\ttarget_image = target_image.to(DEVICE).to(torch.float32)\n",
    "\t\t\t\t\t\tmasks = masks.to(DEVICE)\n",
    "\t\t\t\t\t\tmasks = masks.view(-1, 1, 32, 32).float()   # [batch_size, 1, 32, 32]\n",
    "\t\t\t\t\t\tinput_image = add_masked_gaussian_noise(input_image,masks,sigma=sigma)\n",
    "\t\t\t\t\t\ttarget_image = add_masked_gaussian_noise(target_image,masks,sigma=sigma)\n",
    "\n",
    "\t\t\t\t\t\t# Zerar Gradientes\n",
    "\t\t\t\t\t\tgenerator_optimizer.zero_grad()\n",
    "\t\t\t\t\t\tdiscriminator_optimizer.zero_grad()\n",
    "\n",
    "\t\t\t\t\t\t# Forward Pass\n",
    "\t\t\t\t\t\tgenerated_image = generator(input_image)\n",
    "\n",
    "\t\t\t\t\t\t# Discriminador\n",
    "\t\t\t\t\t\treal_combined = torch.cat([input_image, target_image], dim=1)\n",
    "\t\t\t\t\t\tfake_combined = torch.cat([input_image, generated_image.detach()], dim=1)\n",
    "\t\t\t\t\t\tdisc_real_output = discriminator(real_combined)\n",
    "\t\t\t\t\t\tdisc_fake_output = discriminator(fake_combined)\n",
    "\n",
    "\t\t\t\t\t\t# Perdas\n",
    "\t\t\t\t\t\tloss_D = discriminator_loss(disc_real_output, disc_fake_output)\n",
    "\t\t\t\t\t\tdisc_fake_output_for_gen = discriminator(torch.cat([input_image, generated_image], dim=1))\n",
    "\t\t\t\t\t\tloss_G, loss_GAN, loss_L1 = generator_loss(disc_fake_output_for_gen, generated_image, target_image,masks, lambda_l1=100)\n",
    "\n",
    "\t\t\t\t\t\t# Soma das perdas\n",
    "\t\t\t\t\t\ttotal_loss = loss_D + loss_G\n",
    "\n",
    "\t\t\t\t\t\t# Backward\n",
    "\t\t\t\t\t\ttotal_loss.backward()\n",
    "\n",
    "\t\t\t\t\t\t# Atualizar Pesos\n",
    "\t\t\t\t\t\tdiscriminator_optimizer.step()\n",
    "\t\t\t\t\t\tgenerator_optimizer.step()\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\tif epoch % 10 == 0 :\n",
    "\t\t\t\t\t\t# Validação\n",
    "\t\t\t\t\t\tgenerator.eval()\n",
    "\t\t\t\t\t\tdiscriminator.eval()\n",
    "\n",
    "\t\t\t\t\t\tval_gen_loss = 0\n",
    "\t\t\t\t\t\tval_disc_loss = 0\n",
    "\t\t\t\t\t\tval_gan_loss = 0\n",
    "\t\t\t\t\t\tval_l1_loss = 0\n",
    "\t\t\t\t\t\tnum_batches = 0\n",
    "\n",
    "\t\t\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\t\t\t\tfor input_image, target_image,_,_,masksv in val_loader:\n",
    "\t\t\t\t\t\t\t\t\t\tinput_image = input_image.to(DEVICE).to(torch.float32)\n",
    "\t\t\t\t\t\t\t\t\t\ttarget_image = target_image.to(DEVICE).to(torch.float32)\n",
    "\t\t\t\t\t\t\t\t\t\tmasksv = masksv.to(DEVICE)\n",
    "\t\t\t\t\t\t\t\t\t\tmasksv = masksv.view(-1, 1, 32, 32).float()   # [batch_size, 1, 32, 32]\n",
    "\t\t\t\t\t\t\t\t\t\tinput_image = add_masked_gaussian_noise(input_image, masksv, sigma=sigma)\n",
    "\t\t\t\t\t\t\t\t\t\ttarget_image = add_masked_gaussian_noise(input_image, masksv, sigma=sigma)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\tgenerated_image = generator(input_image)\n",
    "\t\t\t\t\t\t\t\t\t\treal_combined = torch.cat([input_image, target_image], dim=1)\n",
    "\t\t\t\t\t\t\t\t\t\tfake_combined = torch.cat([input_image, generated_image], dim=1)\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\tdisc_real_output = discriminator(real_combined)\n",
    "\t\t\t\t\t\t\t\t\t\tdisc_fake_output = discriminator(fake_combined)\n",
    "\t\t\t\t\t\t\t\t\t\tloss_D = discriminator_loss(disc_real_output, disc_fake_output)\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\tdisc_fake_output_for_gen = discriminator(torch.cat([input_image, generated_image], dim=1))\n",
    "\t\t\t\t\t\t\t\t\t\tloss_G, loss_GAN, loss_L1 = generator_loss(disc_fake_output_for_gen, generated_image, target_image,masksv, lambda_l1=100)\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\tval_gen_loss += loss_G.item()\n",
    "\t\t\t\t\t\t\t\t\t\tval_disc_loss += loss_D.item()\n",
    "\t\t\t\t\t\t\t\t\t\tval_gan_loss += loss_GAN.item()\n",
    "\t\t\t\t\t\t\t\t\t\tval_l1_loss += loss_L1.item()\n",
    "\t\t\t\t\t\t\t\t\t\tnum_batches += 1\n",
    "\n",
    "\t\t\t\t\t\tval_gen_loss /= num_batches\n",
    "\t\t\t\t\t\tval_disc_loss /= num_batches\n",
    "\t\t\t\t\t\tval_gan_loss /= num_batches\n",
    "\t\t\t\t\t\tval_l1_loss /= num_batches\n",
    "\t\t\t\t\t\tif epoch % 50==0:\n",
    "\t\t\n",
    "\t\t\t\t\t\t\t\toutput_buffer += f'Epoch: {epoch}, Gen Loss: {val_gen_loss:.4f}, Disc Loss: {val_disc_loss:.4f}, GAN Loss: {val_gan_loss:.4f}, L1 Loss: {val_l1_loss:.4f}\\n'\n",
    "\t\t\t\t\t\t\t\t# Usa sys.stdout.write para imprimir sem adicionar uma nova linha\n",
    "\t\t\t\t\t\t\t\tsys.stdout.write(output_buffer)\n",
    "\t\t\t\t\t\t\t\t# Limpa o buffer para não acumular demais\n",
    "\t\t\t\t\t\t\t\toutput_buffer = \"\"\n",
    "\t\t\t\t\t\t# Criar diretório se não existir\n",
    "\t\t\t\t\t\tif val_gen_loss < best_val_loss:\n",
    "\t\t\t\t\t\t\t\t\t\tbest_val_loss = val_gen_loss\n",
    "\t\t\t\t\t\t\t\t\t\tepochs_no_improve = 0  # Reset contador de early stopping\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tepochs_no_improve += 1\n",
    "\n",
    "\t\t\t\t\t\tif epochs_no_improve >= patience:\n",
    "\t\t\t\t\t\t\t\tprint(\"Early stopping ativado!\")\n",
    "\t\t\t\t\t\t\t\tbreak  # Sai do loop de épocas, mas deve continuar para próximo fold/t\t\n",
    "                                                \t\n",
    "\t\tif not os.path.exists(save_dir):\n",
    "\t\t\tos.makedirs(save_dir)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t# Salvar modelo\n",
    "\t\t\tmodel_path = os.path.join(save_dir, \"generator.pth\")\n",
    "\t\t\ttorch.save(generator.state_dict(), model_path)\n",
    "\t\t\tprint(f\"Modelo salvo em: {model_path}\") \n",
    "\t\t\t\n",
    "\t\treturn generator\n",
    "\n",
    "\n",
    "def train_and_validate1(epochs, train_loader, val_loader, in_channels, taxa, fold, patience=50):\n",
    "\tDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\tsave_dir =  f\"./models_saved/pix2pix/{in_channels}/{taxa}/fold{fold}\"\n",
    "\n",
    "\t# Instanciar Gerador e Discriminador\n",
    "\tgenerator = UNetGenerator(input_channels=in_channels, output_channels=in_channels)\n",
    "\tdiscriminator = PatchDiscriminator(input_channels=in_channels)\n",
    "\n",
    "\t# Multi-GPU (se disponível)\n",
    "\t# if torch.cuda.device_count() > 1:\n",
    "\t# \tprint(f\"Usando {torch.cuda.device_count()} GPUs\")\n",
    "\t# \tgenerator = torch.nn.DataParallel(generator)\n",
    "\t# \tdiscriminator = torch.nn.DataParallel(discriminator)\n",
    "\n",
    "\tgenerator = generator.to(DEVICE)\n",
    "\tdiscriminator = discriminator.to(DEVICE)\n",
    "\n",
    "\t# Inicializar Pesos\n",
    "\tgenerator.apply(weights_init)\n",
    "\tdiscriminator.apply(weights_init)\n",
    "\n",
    "\t# Configurar Otimizadores\n",
    "\tgenerator_optimizer = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.55, 0.999))\n",
    "\tdiscriminator_optimizer = optim.Adam(discriminator.parameters(), lr=1e-5, betas=(0.55, 0.999))\n",
    "\n",
    "\tbest_val_loss = float('inf')\n",
    "\tepochs_no_improve = 0\n",
    "\toutput_buffer = \"\"\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tgenerator.train()\n",
    "\t\tdiscriminator.train()\n",
    "\n",
    "\t\tinitial_sigma = 0.1\n",
    "\t\tfinal_sigma = 0.0\n",
    "\t\twarmup_epochs = 50\n",
    "\t\tsigma = max(0.0, initial_sigma * (1 - epoch / warmup_epochs))\n",
    "\n",
    "\t\tfor input_image, target_image, _, _, masks in train_loader:\n",
    "\t\t\tinput_image = input_image.to(DEVICE).float()\n",
    "\t\t\ttarget_image = target_image.to(DEVICE).float()\n",
    "\t\t\tmasks = masks.to(DEVICE).view(-1, 1, 32, 32).float()\n",
    "\n",
    "\t\t\tinput_image = add_masked_gaussian_noise(input_image, masks, sigma=sigma)\n",
    "\t\t\ttarget_image = add_masked_gaussian_noise(target_image, masks, sigma=sigma)\n",
    "\n",
    "\t\t\tgenerator_optimizer.zero_grad()\n",
    "\t\t\tdiscriminator_optimizer.zero_grad()\n",
    "\n",
    "\t\t\tgenerated_image = generator(input_image)\n",
    "\n",
    "\t\t\treal_combined = torch.cat([input_image, target_image], dim=1)\n",
    "\t\t\tfake_combined = torch.cat([input_image, generated_image.detach()], dim=1)\n",
    "\n",
    "\t\t\tdisc_real_output = discriminator(real_combined)\n",
    "\t\t\tdisc_fake_output = discriminator(fake_combined)\n",
    "\n",
    "\t\t\tloss_D = discriminator_loss(disc_real_output, disc_fake_output)\n",
    "\n",
    "\t\t\tdisc_fake_output_for_gen = discriminator(torch.cat([input_image, generated_image], dim=1))\n",
    "\t\t\tloss_G, loss_GAN, loss_L1 = generator_loss(disc_fake_output_for_gen, generated_image, target_image, masks, lambda_l1=100)\n",
    "\n",
    "\t\t\ttotal_loss = loss_D + loss_G\n",
    "\t\t\ttotal_loss.backward()\n",
    "\t\t\tdiscriminator_optimizer.step()\n",
    "\t\t\tgenerator_optimizer.step()\n",
    "\n",
    "\t\t# Validação\n",
    "\t\tif epoch % 10 == 0:\n",
    "\t\t\tgenerator.eval()\n",
    "\t\t\tdiscriminator.eval()\n",
    "\n",
    "\t\t\tval_gen_loss = 0\n",
    "\t\t\tval_disc_loss = 0\n",
    "\t\t\tval_gan_loss = 0\n",
    "\t\t\tval_l1_loss = 0\n",
    "\t\t\tnum_batches = 0\n",
    "\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tfor input_image, target_image, _, _, masksv in val_loader:\n",
    "\t\t\t\t\tinput_image = input_image.to(DEVICE).float()\n",
    "\t\t\t\t\ttarget_image = target_image.to(DEVICE).float()\n",
    "\t\t\t\t\tmasksv = masksv.to(DEVICE).view(-1, 1, 32, 32).float()\n",
    "\n",
    "\t\t\t\t\tinput_image = add_masked_gaussian_noise(input_image, masksv, sigma=sigma)\n",
    "\t\t\t\t\ttarget_image = add_masked_gaussian_noise(target_image, masksv, sigma=sigma)\n",
    "\n",
    "\t\t\t\t\tgenerated_image = generator(input_image)\n",
    "\n",
    "\t\t\t\t\treal_combined = torch.cat([input_image, target_image], dim=1)\n",
    "\t\t\t\t\tfake_combined = torch.cat([input_image, generated_image], dim=1)\n",
    "\n",
    "\t\t\t\t\tdisc_real_output = discriminator(real_combined)\n",
    "\t\t\t\t\tdisc_fake_output = discriminator(fake_combined)\n",
    "\n",
    "\t\t\t\t\tloss_D = discriminator_loss(disc_real_output, disc_fake_output)\n",
    "\t\t\t\t\tdisc_fake_output_for_gen = discriminator(torch.cat([input_image, generated_image], dim=1))\n",
    "\t\t\t\t\tloss_G, loss_GAN, loss_L1 = generator_loss(disc_fake_output_for_gen, generated_image, target_image, masksv, lambda_l1=100)\n",
    "\n",
    "\t\t\t\t\tval_gen_loss += loss_G.item()\n",
    "\t\t\t\t\tval_disc_loss += loss_D.item()\n",
    "\t\t\t\t\tval_gan_loss += loss_GAN.item()\n",
    "\t\t\t\t\tval_l1_loss += loss_L1.item()\n",
    "\t\t\t\t\tnum_batches += 1\n",
    "\n",
    "\t\t\tval_gen_loss /= num_batches\n",
    "\t\t\tval_disc_loss /= num_batches\n",
    "\t\t\tval_gan_loss /= num_batches\n",
    "\t\t\tval_l1_loss /= num_batches\n",
    "\n",
    "\t\t\tif epoch % 50 == 0:\n",
    "\t\t\t\toutput_buffer += f'Epoch: {epoch}, Gen Loss: {val_gen_loss:.4f}, Disc Loss: {val_disc_loss:.4f}, GAN Loss: {val_gan_loss:.4f}, L1 Loss: {val_l1_loss:.4f}\\n'\n",
    "\t\t\t\tsys.stdout.write(output_buffer)\n",
    "\t\t\t\toutput_buffer = \"\"\n",
    "\n",
    "\t\t\t# Early Stopping\n",
    "\t\t\tif val_gen_loss < best_val_loss:\n",
    "\t\t\t\tbest_val_loss = val_gen_loss\n",
    "\t\t\t\tepochs_no_improve = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\tepochs_no_improve += 1\n",
    "\n",
    "\t\t\tif  val_gen_loss > best_val_loss and epochs_no_improve >= patience:\n",
    "\t\t\t\tprint(\"Early stopping ativado!\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t# Salvar modelo\n",
    "\tif not os.path.exists(save_dir):\n",
    "\t\tos.makedirs(save_dir)\n",
    "\n",
    "\t# Remover wrapper do DataParallel antes de salvar\n",
    "\tif isinstance(generator, torch.nn.DataParallel):\n",
    "\t\ttorch.save(generator.module.state_dict(), os.path.join(save_dir, \"generator.pth\"))\n",
    "\telse:\n",
    "\t\ttorch.save(generator.state_dict(), os.path.join(save_dir, \"generator.pth\"))\n",
    "\n",
    "\tprint(f\"Modelo salvo em: {save_dir}/generator.pth\")\n",
    "\treturn generator\n",
    "              \n",
    "              \n",
    "\n",
    "# Função de teste\n",
    "def test(gen_Z, test_loader, taxa, fold, chanells):\n",
    "    DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    gen_Z.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Criar o DataFrame com as colunas desejadas\n",
    "        df = pd.DataFrame([], columns=['mae', 'asmape', 'mape', 'rmse', 'scale'], index=test_loader.dataset.horse_images)\n",
    "\n",
    "        for (zebra, horse, std_val, mean_val, mask), name in zip(test_loader, test_loader.dataset.horse_images):\n",
    "\n",
    "            # Mover dados para o dispositivo\n",
    "            zebra = zebra.to(DEVICE)\n",
    "            horse = horse.to(DEVICE)\n",
    "\n",
    "            # Converter std_val e mean_val para tensores e movê-los para o dispositivo\n",
    "            std_val = torch.tensor(std_val, device=DEVICE) if not isinstance(std_val, torch.Tensor) else std_val.to(DEVICE)\n",
    "            mean_val = torch.tensor(mean_val, device=DEVICE) if not isinstance(mean_val, torch.Tensor) else mean_val.to(DEVICE)\n",
    "\n",
    "            # Gerar fake_zebra usando o gerador\n",
    "            fake_zebra = gen_Z(horse)\n",
    "\n",
    "            # Mover apenas as imagens para a CPU antes de operações subsequentes\n",
    "            zebra = zebra.cpu()\n",
    "            fake_zebra = fake_zebra.cpu()\n",
    "\n",
    "            # Voltar para escala original \n",
    "            zebra = zebra * std_val.cpu() + mean_val.cpu()\n",
    "            fake_zebra = fake_zebra * std_val.cpu() + mean_val.cpu()\n",
    "\n",
    "            # Somar sobre o canal e achatar as imagens\n",
    "            zebra = torch.sum(zebra, dim=1).flatten()\n",
    "            fake_zebra = torch.sum(fake_zebra, dim=1).flatten()\n",
    "\n",
    "            # Certificar que zebra e fake_zebra estão na CPU e sem gradientes antes de usar métricas\n",
    "            zebra_np = zebra * mask\n",
    "            fake_zebra_np = fake_zebra * mask\n",
    "\n",
    "            # Calcular as métricas corretamente\n",
    "            mae_value = round(mae(zebra_np, fake_zebra_np), 3)\n",
    "            mape_value = round(mape(zebra_np, fake_zebra_np) * 100, 3)\n",
    "            rmse_value = round(np.sqrt(mse(zebra_np, fake_zebra_np)), 3)\n",
    "            smape_value = round(asmape(zebra_np, fake_zebra_np, mask), 3)\n",
    "\n",
    "            # Adicionar os resultados ao DataFrame\n",
    "            df.loc[name] = [mae_value, smape_value, mape_value, rmse_value, np.max(zebra.numpy()) - np.min(zebra.numpy())]\n",
    "\n",
    "        # Salvar o DataFrame em um arquivo CSV\n",
    "        directory = \"./resultados/resultados_pix\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        df.to_csv(os.path.join(directory, f'result_{str(chanells)}c_{taxa}_{fold}.csv'))\n",
    "\n",
    "\n",
    "# Parâmetros de treinamento\n",
    "\n",
    "BATCH_SIZE = 400\n",
    "NUM_EPOCHS = 50000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "TRAIN_DIR = os.path.abspath(\"../dataset_final\")  \n",
    "VAL_DIR = os.path.abspath(\"../dataset_final\")  \n",
    "INDEX_TRAIN = os.path.abspath(\"../dataset_final\")  \n",
    "INDEX_VAL = os.path.abspath(\"../dataset_final\")  \n",
    "INDEX_TEST = os.path.abspath(\"../dataset_final\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel 1\n",
      "Usando 2 GPUs\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 84, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_220986/2833226606.py\", line 204, in forward\n    u5 = self.up5(u4)\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 219, in forward\n    input = module(input)\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 176, in forward\n    return F.batch_norm(\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/functional.py\", line 2512, in batch_norm\n    return torch.batch_norm(\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 200.00 MiB. GPU 1 has a total capacity of 23.64 GiB of which 127.06 MiB is free. Process 3538589 has 19.64 GiB memory in use. Including non-PyTorch memory, this process has 3.86 GiB memory in use. Of the allocated memory 3.19 GiB is allocated by PyTorch, and 126.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m]:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(in_channels)\u001b[0m\n\u001b[1;32m     30\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Treino        \u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtaxa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Teste\u001b[39;00m\n\u001b[1;32m     36\u001b[0m test(generator, test_loader\u001b[38;5;241m=\u001b[39mtest_loader, taxa\u001b[38;5;241m=\u001b[39mtaxa, fold\u001b[38;5;241m=\u001b[39mfold, chanells\u001b[38;5;241m=\u001b[39min_channels)\n",
      "Cell \u001b[0;32mIn[2], line 648\u001b[0m, in \u001b[0;36mtrain_and_validate1\u001b[0;34m(epochs, train_loader, val_loader, in_channels, taxa, fold, patience)\u001b[0m\n\u001b[1;32m    645\u001b[0m generator_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    646\u001b[0m discriminator_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 648\u001b[0m generated_image \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m real_combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([input_image, target_image], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    651\u001b[0m fake_combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([input_image, generated_image\u001b[38;5;241m.\u001b[39mdetach()], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:186\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:201\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:109\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    107\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 109\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 84, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_220986/2833226606.py\", line 204, in forward\n    u5 = self.up5(u4)\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 219, in forward\n    input = module(input)\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 176, in forward\n    return F.batch_norm(\n  File \"/home/mauricio/.conda/envs/pyt_envmau/lib/python3.10/site-packages/torch/nn/functional.py\", line 2512, in batch_norm\n    return torch.batch_norm(\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 200.00 MiB. GPU 1 has a total capacity of 23.64 GiB of which 127.06 MiB is free. Process 3538589 has 19.64 GiB memory in use. Including non-PyTorch memory, this process has 3.86 GiB memory in use. Of the allocated memory 3.19 GiB is allocated by PyTorch, and 126.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main(in_channels):\n",
    "    for taxa in ['10','20','30','40']:\n",
    "        for fold in ['1','2','3','4','5']:  \n",
    "            dataset = LoaderDataset(\n",
    "                  root_zebra=os.path.join( TRAIN_DIR, \"label\", str(taxa), \"folds\", f\"fold{fold}\", \"train\"),\n",
    "                  root_horse=os.path.join( TRAIN_DIR, \"input\", str(taxa), \"folds\", f\"fold{fold}\", \"train\"),\n",
    "\t\t\t\t\t\t\t\t\troot_masks=os.path.join( INDEX_TRAIN, \"input\", str(taxa), \"folds\", f\"fold{fold}\", \"index_train\"),\n",
    "\t\t\t\t\t\t\t\t\tchanels=in_channels\n",
    "\t\t\t\t\t\t\t  )\n",
    "            \n",
    "            val_dataset = LoaderDataset(\n",
    "                root_zebra=os.path.join( VAL_DIR, \"label\", str(taxa), \"folds\", f\"fold{fold}\", \"val\"),\n",
    "\t\t\t\t\t\t\t\troot_horse=os.path.join( VAL_DIR, \"input\", str(taxa), \"folds\", f\"fold{fold}\", \"val\"),\n",
    "\t\t\t\t\t\t\t\troot_masks=os.path.join( INDEX_VAL, \"input\", str(taxa), \"folds\", f\"fold{fold}\", \"index_val\"),\n",
    "\t\t\t\t\t\t\t\tchanels=in_channels\n",
    "\t\t\t\t\t\t  )\n",
    "\n",
    "            test_dataset = LoaderDataset(\n",
    "                  root_zebra=os.path.join( VAL_DIR, \"label\", str(taxa), \"folds\", f\"fold{fold}\", \"test\"),\n",
    "\t\t\t\t\t\t\t\t\troot_horse=os.path.join( VAL_DIR, \"input\", str(taxa), \"folds\", f\"fold{fold}\", \"test\"),\n",
    "\t\t\t\t\t\t\t\t\troot_masks=os.path.join(INDEX_TEST, \"input\", str(taxa), \"folds\", f\"fold{fold}\", \"index\"),\n",
    "\t\t\t\t\t\t\t\t\tchanels=in_channels\n",
    "\t\t\t\t\t\t)\n",
    "            \t\t\t\n",
    "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=False,)\n",
    "            loader = DataLoader(dataset, batch_size=BATCH_SIZE,  shuffle=True, pin_memory=False,    )\n",
    "            \n",
    "            test_loader = DataLoader( test_dataset,  batch_size=1,shuffle=False,pin_memory=False )\n",
    "            \n",
    "            test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "            \n",
    "            # Treino        \n",
    "            generator = train_and_validate1(NUM_EPOCHS, loader, val_loader, in_channels,taxa, fold)\n",
    "\n",
    "            # Teste\n",
    "            test(generator, test_loader=test_loader, taxa=taxa, fold=fold, chanells=in_channels)\n",
    "            \n",
    "import multiprocessing as mp\n",
    "if __name__ == '__main__':\n",
    "    mp.freeze_support()\n",
    "    for i in [1,2,3]:\n",
    "        print(f'channel {i}')\n",
    "        main(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_envmau",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
