{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "# from torchvision.utils import save_image\n",
    "import sys \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "##############################\n",
    "#           U-NET\n",
    "##############################\n",
    "\n",
    "\n",
    "\n",
    "# === Instance Normalization Custom (como no TF) ===\n",
    "class InstanceNormalization(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        # escala e offset serão inicializados no forward com parâmetros registrados\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (N,C,H,W)\n",
    "        mean = x.mean(dim=[2,3], keepdim=True)\n",
    "        var = x.var(dim=[2,3], keepdim=True, unbiased=False)\n",
    "        inv = 1.0 / torch.sqrt(var + self.epsilon)\n",
    "        normalized = (x - mean) * inv\n",
    "\n",
    "        # Criar escala e offset param se não existirem\n",
    "        if not hasattr(self, 'scale'):\n",
    "            self.scale = nn.Parameter(torch.ones(x.size(1), device=x.device))\n",
    "            self.offset = nn.Parameter(torch.zeros(x.size(1), device=x.device))\n",
    "        # reshape para broadcast\n",
    "        scale = self.scale.view(1, -1, 1, 1)\n",
    "        offset = self.offset.view(1, -1, 1, 1)\n",
    "        return scale * normalized + offset\n",
    "\n",
    "# === Downsample e Upsample ===\n",
    "def downsample(in_ch, out_ch, norm_type='instancenorm', apply_norm=True):\n",
    "    layers = [nn.Conv2d(in_ch, out_ch, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "    if apply_norm:\n",
    "        if norm_type == 'batchnorm':\n",
    "            layers.append(nn.BatchNorm2d(out_ch))\n",
    "        elif norm_type == 'instancenorm':\n",
    "            layers.append(InstanceNormalization())\n",
    "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def upsample(in_ch, out_ch, norm_type='instancenorm', apply_dropout=False):\n",
    "    layers = [nn.ConvTranspose2d(in_ch, out_ch, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "    if norm_type == 'batchnorm':\n",
    "        layers.append(nn.BatchNorm2d(out_ch))\n",
    "    elif norm_type == 'instancenorm':\n",
    "        layers.append(InstanceNormalization())\n",
    "    layers.append(nn.ReLU(inplace=True))\n",
    "    if apply_dropout:\n",
    "        layers.append(nn.Dropout(0.5))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=3, norm_type='instancenorm', target_size=256):\n",
    "        super().__init__()\n",
    "        self.target_size = target_size\n",
    "        self.down1 = downsample(input_channels, 64, norm_type, apply_norm=False)\n",
    "        self.down2 = downsample(64, 128, norm_type)\n",
    "        self.down3 = downsample(128, 256, norm_type)\n",
    "        self.down4 = downsample(256, 512, norm_type)\n",
    "        self.down5 = downsample(512, 512, norm_type)\n",
    "        self.down6 = downsample(512, 512, norm_type)\n",
    "        self.down7 = downsample(512, 512, norm_type)\n",
    "        self.down8 = downsample(512, 512, norm_type)\n",
    "\n",
    "        self.up1 = upsample(512, 512, norm_type, apply_dropout=True)\n",
    "        self.up2 = upsample(1024, 512, norm_type, apply_dropout=True)\n",
    "        self.up3 = upsample(1024, 512, norm_type, apply_dropout=True)\n",
    "        self.up4 = upsample(1024, 512, norm_type)\n",
    "        self.up5 = upsample(1024, 256, norm_type)\n",
    "        self.up6 = upsample(512, 128, norm_type)\n",
    "        self.up7 = upsample(256, 64, norm_type)\n",
    "\n",
    "        self.final = nn.ConvTranspose2d(128, output_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        orig_size = x.shape[-2:]  # (H, W)\n",
    "\n",
    "        # Upsample entrada para target_size x target_size\n",
    "        x = F.interpolate(x, size=(self.target_size, self.target_size), mode='bilinear', align_corners=False)\n",
    "\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "\n",
    "        u1 = self.up1(d8)\n",
    "        u1 = torch.cat([u1, d7], dim=1)\n",
    "\n",
    "        u2 = self.up2(u1)\n",
    "        u2 = torch.cat([u2, d6], dim=1)\n",
    "\n",
    "        u3 = self.up3(u2)\n",
    "        u3 = torch.cat([u3, d5], dim=1)\n",
    "\n",
    "        u4 = self.up4(u3)\n",
    "        u4 = torch.cat([u4, d4], dim=1)\n",
    "\n",
    "        u5 = self.up5(u4)\n",
    "        u5 = torch.cat([u5, d3], dim=1)\n",
    "\n",
    "        u6 = self.up6(u5)\n",
    "        u6 = torch.cat([u6, d2], dim=1)\n",
    "\n",
    "        u7 = self.up7(u6)\n",
    "        u7 = torch.cat([u7, d1], dim=1)\n",
    "\n",
    "        output = self.final(u7)\n",
    "        output = self.tanh(output)\n",
    "\n",
    "        # Downsample a saída para o tamanho original da entrada\n",
    "        output = F.interpolate(output, size=orig_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return output\n",
    "\n",
    "# === PatchGAN Discriminator ===\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels=3, norm_type='batchnorm', target=True):\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "        in_ch = input_channels * 2 if target else input_channels\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 64, kernel_size=4, stride=2, padding=1),  # no norm\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            downsample(64, 128, norm_type),\n",
    "            downsample(128, 256, norm_type),\n",
    "\n",
    "            nn.ZeroPad2d(1),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "\n",
    "            nn.BatchNorm2d(512) if norm_type == 'batchnorm' else InstanceNormalization(),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ZeroPad2d(1),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, inp, target=None):\n",
    "        if self.target and target is not None:\n",
    "            x = torch.cat([inp, target], dim=1)\n",
    "        else:\n",
    "            x = inp\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iniciando treinamento com 1 canais\n",
      "\n",
      "\n",
      "\n",
      "Treinando taxa=10, fold=1\n",
      "[Epoch 102/1000] [Batch 65/65] [D loss: 0.002819] [G loss: 2.180770, adv: 1.057216, pixel: 0.024692, cycle: 0.020823] ETA: 0:37:26    \n",
      "Early stopping at epoch 102 with best loss 0.016918\n",
      "Treinando taxa=10, fold=2\n",
      "[Epoch 109/1000] [Batch 65/65] [D loss: 0.002413] [G loss: 2.164432, adv: 1.041265, pixel: 0.029739, cycle: 0.026081] ETA: 0:38:39     \n",
      "Early stopping at epoch 109 with best loss 0.015440\n",
      "Treinando taxa=10, fold=3\n",
      "[Epoch 79/1000] [Batch 65/65] [D loss: 0.037119] [G loss: 1.332749, adv: 0.639435, pixel: 0.016326, cycle: 0.018777] ETA: 0:39:55         \n",
      "Early stopping at epoch 79 with best loss 0.017438\n",
      "Treinando taxa=10, fold=4\n",
      "[Epoch 105/1000] [Batch 65/65] [D loss: 0.033095] [G loss: 2.204990, adv: 1.078011, pixel: 0.015839, cycle: 0.016564] ETA: 0:38:54        \n",
      "Early stopping at epoch 105 with best loss 0.016711\n",
      "Treinando taxa=10, fold=5\n",
      "[Epoch 75/1000] [Batch 65/65] [D loss: 0.003379] [G loss: 2.174751, adv: 1.062019, pixel: 0.015558, cycle: 0.017578] ETA: 0:40:05     \n",
      "Early stopping at epoch 75 with best loss 0.015491\n",
      "Treinando taxa=20, fold=1\n",
      "[Epoch 98/1000] [Batch 65/65] [D loss: 0.015019] [G loss: 2.144336, adv: 1.025824, pixel: 0.037417, cycle: 0.027635] ETA: 0:38:13   :22   \n",
      "Early stopping at epoch 98 with best loss 0.039556\n",
      "Treinando taxa=20, fold=2\n",
      "[Epoch 219/1000] [Batch 65/65] [D loss: 0.001259] [G loss: 2.124875, adv: 1.015311, pixel: 0.037820, cycle: 0.028217] ETA: 0:33:42      \n",
      "Early stopping at epoch 219 with best loss 0.035432\n",
      "Treinando taxa=20, fold=3\n",
      "[Epoch 79/1000] [Batch 65/65] [D loss: 0.214502] [G loss: 0.764750, adv: 0.314870, pixel: 0.042661, cycle: 0.046175] ETA: 0:39:14         \n",
      "Early stopping at epoch 79 with best loss 0.039049\n",
      "Treinando taxa=20, fold=4\n",
      "[Epoch 101/1000] [Batch 65/65] [D loss: 0.018819] [G loss: 2.198143, adv: 1.041409, pixel: 0.038995, cycle: 0.038164] ETA: 0:38:23        \n",
      "Early stopping at epoch 101 with best loss 0.038640\n",
      "Treinando taxa=20, fold=5\n",
      "[Epoch 100/1000] [Batch 65/65] [D loss: 0.002830] [G loss: 2.211958, adv: 1.035981, pixel: 0.048357, cycle: 0.045820] ETA: 0:39:25       \n",
      "Early stopping at epoch 100 with best loss 0.037886\n",
      "Treinando taxa=30, fold=1\n",
      "[Epoch 108/1000] [Batch 65/65] [D loss: 0.018086] [G loss: 1.727894, adv: 0.800689, pixel: 0.056875, cycle: 0.034820] ETA: 0:37:07        \n",
      "Early stopping at epoch 108 with best loss 0.066638\n",
      "Treinando taxa=30, fold=2\n",
      "[Epoch 198/1000] [Batch 65/65] [D loss: 0.001317] [G loss: 2.061880, adv: 0.958338, pixel: 0.071177, cycle: 0.037013] ETA: 0:35:02      \n",
      "Early stopping at epoch 198 with best loss 0.064278\n",
      "Treinando taxa=30, fold=3\n",
      "[Epoch 188/1000] [Batch 65/65] [D loss: 0.001883] [G loss: 2.103105, adv: 0.982964, pixel: 0.062696, cycle: 0.037241] ETA: 0:35:07        \n",
      "Early stopping at epoch 188 with best loss 0.065078\n",
      "Treinando taxa=30, fold=4\n",
      "[Epoch 108/1000] [Batch 65/65] [D loss: 0.006636] [G loss: 1.952499, adv: 0.905934, pixel: 0.058977, cycle: 0.040827] ETA: 0:38:20       \n",
      "Early stopping at epoch 108 with best loss 0.068632\n",
      "Treinando taxa=30, fold=5\n",
      "[Epoch 158/1000] [Batch 65/65] [D loss: 0.006076] [G loss: 2.153177, adv: 1.014763, pixel: 0.047078, cycle: 0.038287] ETA: 0:37:19       \n",
      "Early stopping at epoch 158 with best loss 0.065034\n",
      "Treinando taxa=40, fold=1\n",
      "[Epoch 78/1000] [Batch 65/65] [D loss: 0.002529] [G loss: 2.346935, adv: 1.026675, pixel: 0.108717, cycle: 0.092434] ETA: 0:39:30        \n",
      "Early stopping at epoch 78 with best loss 0.101212\n",
      "Treinando taxa=40, fold=2\n",
      "[Epoch 137/1000] [Batch 65/65] [D loss: 0.001665] [G loss: 2.244292, adv: 1.014905, pixel: 0.098150, cycle: 0.058166] ETA: 0:37:34    \n",
      "Early stopping at epoch 137 with best loss 0.097807\n",
      "Treinando taxa=40, fold=3\n",
      "[Epoch 292/1000] [Batch 65/65] [D loss: 0.000819] [G loss: 2.250972, adv: 1.038991, pixel: 0.098263, cycle: 0.037364] ETA: 0:30:42      \n",
      "Early stopping at epoch 292 with best loss 0.096456\n",
      "Treinando taxa=40, fold=4\n",
      "[Epoch 152/1000] [Batch 65/65] [D loss: 0.002753] [G loss: 2.207585, adv: 1.004862, pixel: 0.090608, cycle: 0.053627] ETA: 0:36:45        \n",
      "Early stopping at epoch 152 with best loss 0.101557\n",
      "Treinando taxa=40, fold=5\n",
      "[Epoch 125/1000] [Batch 65/65] [D loss: 0.113537] [G loss: 2.342314, adv: 1.073189, pixel: 0.117037, cycle: 0.039450] ETA: 0:38:08        \n",
      "Early stopping at epoch 125 with best loss 0.097564\n",
      "\n",
      "\n",
      "\n",
      "Iniciando treinamento com 2 canais\n",
      "\n",
      "\n",
      "\n",
      "Treinando taxa=10, fold=1\n",
      "[Epoch 194/1000] [Batch 65/65] [D loss: 0.002726] [G loss: 2.219043, adv: 1.065902, pixel: 0.027895, cycle: 0.029672] ETA: 0:35:25        \n",
      "Early stopping at epoch 194 with best loss 0.012908\n",
      "Treinando taxa=10, fold=2\n",
      "[Epoch 68/1000] [Batch 65/65] [D loss: 0.022522] [G loss: 1.613801, adv: 0.788384, pixel: 0.011319, cycle: 0.012858] ETA: 0:41:42    9   \n",
      "Early stopping at epoch 68 with best loss 0.014285\n",
      "Treinando taxa=10, fold=3\n",
      "[Epoch 62/1000] [Batch 65/65] [D loss: 0.149365] [G loss: 1.276568, adv: 0.610741, pixel: 0.018415, cycle: 0.018335] ETA: 0:42:48         \n",
      "Early stopping at epoch 62 with best loss 0.015566\n",
      "Treinando taxa=10, fold=4\n",
      "[Epoch 93/1000] [Batch 65/65] [D loss: 0.027643] [G loss: 2.667805, adv: 1.297062, pixel: 0.022675, cycle: 0.025503] ETA: 0:39:46      \n",
      "Early stopping at epoch 93 with best loss 0.013071\n",
      "Treinando taxa=10, fold=5\n",
      "[Epoch 107/1000] [Batch 65/65] [D loss: 0.006456] [G loss: 1.843208, adv: 0.902747, pixel: 0.011310, cycle: 0.013202] ETA: 0:39:11   31   \n",
      "Early stopping at epoch 107 with best loss 0.011009\n",
      "Treinando taxa=20, fold=1\n",
      "[Epoch 99/1000] [Batch 65/65] [D loss: 0.074554] [G loss: 2.147035, adv: 1.030224, pixel: 0.034889, cycle: 0.025849] ETA: 0:38:05     8   \n",
      "Early stopping at epoch 99 with best loss 0.026434\n",
      "Treinando taxa=20, fold=2\n",
      "[Epoch 108/1000] [Batch 65/65] [D loss: 0.011927] [G loss: 1.879056, adv: 0.888755, pixel: 0.035737, cycle: 0.032905] ETA: 0:39:28     \n",
      "Early stopping at epoch 108 with best loss 0.023888\n",
      "Treinando taxa=20, fold=3\n",
      "[Epoch 76/1000] [Batch 65/65] [D loss: 0.077030] [G loss: 3.178751, adv: 1.528165, pixel: 0.037908, cycle: 0.042257] ETA: 0:42:56         \n",
      "Early stopping at epoch 76 with best loss 0.025314\n",
      "Treinando taxa=20, fold=4\n",
      "[Epoch 85/1000] [Batch 65/65] [D loss: 0.064702] [G loss: 1.076258, adv: 0.506880, pixel: 0.020121, cycle: 0.021189] ETA: 0:39:55     \n",
      "Early stopping at epoch 85 with best loss 0.025028\n",
      "Treinando taxa=20, fold=5\n",
      "[Epoch 208/1000] [Batch 65/65] [D loss: 0.000765] [G loss: 1.938986, adv: 0.944436, pixel: 0.016321, cycle: 0.016897] ETA: 0:34:35   \n",
      "Early stopping at epoch 208 with best loss 0.020948\n",
      "Treinando taxa=30, fold=1\n",
      "[Epoch 84/1000] [Batch 65/65] [D loss: 0.002400] [G loss: 2.077137, adv: 0.998005, pixel: 0.027494, cycle: 0.026816] ETA: 0:39:41     \n",
      "Early stopping at epoch 84 with best loss 0.037635\n",
      "Treinando taxa=30, fold=2\n",
      "[Epoch 187/1000] [Batch 65/65] [D loss: 0.002042] [G loss: 1.959418, adv: 0.945434, pixel: 0.022490, cycle: 0.023030] ETA: 0:35:43    \n",
      "Early stopping at epoch 187 with best loss 0.032337\n",
      "Treinando taxa=30, fold=3\n",
      "[Epoch 75/1000] [Batch 65/65] [D loss: 0.006847] [G loss: 2.066664, adv: 0.969054, pixel: 0.049342, cycle: 0.039607] ETA: 0:40:29        \n",
      "Early stopping at epoch 75 with best loss 0.038397\n",
      "Treinando taxa=30, fold=4\n",
      "[Epoch 101/1000] [Batch 65/65] [D loss: 0.002413] [G loss: 2.315877, adv: 1.079043, pixel: 0.051494, cycle: 0.053149] ETA: 0:39:29      \n",
      "Early stopping at epoch 101 with best loss 0.036643\n",
      "Treinando taxa=30, fold=5\n",
      "[Epoch 203/1000] [Batch 65/65] [D loss: 0.002031] [G loss: 1.994701, adv: 0.962538, pixel: 0.022460, cycle: 0.023582] ETA: 0:34:50   :14   \n",
      "Early stopping at epoch 203 with best loss 0.029877\n",
      "Treinando taxa=40, fold=1\n",
      "[Epoch 99/1000] [Batch 65/65] [D loss: 0.012203] [G loss: 2.112521, adv: 1.003083, pixel: 0.036027, cycle: 0.035163] ETA: 0:39:54      \n",
      "Early stopping at epoch 99 with best loss 0.048917\n",
      "Treinando taxa=40, fold=2\n",
      "[Epoch 146/1000] [Batch 65/65] [D loss: 0.005956] [G loss: 1.954724, adv: 0.913260, pixel: 0.039329, cycle: 0.044437] ETA: 0:37:55        \n",
      "Early stopping at epoch 146 with best loss 0.043139\n",
      "Treinando taxa=40, fold=3\n",
      "[Epoch 179/1000] [Batch 65/65] [D loss: 0.002275] [G loss: 2.069462, adv: 0.983883, pixel: 0.033939, cycle: 0.033878] ETA: 0:36:30        \n",
      "Early stopping at epoch 179 with best loss 0.041760\n",
      "Treinando taxa=40, fold=4\n",
      "[Epoch 90/1000] [Batch 65/65] [D loss: 0.038957] [G loss: 1.499368, adv: 0.674896, pixel: 0.054433, cycle: 0.047571] ETA: 0:40:05      \n",
      "Early stopping at epoch 90 with best loss 0.047818\n",
      "Treinando taxa=40, fold=5\n",
      "[Epoch 160/1000] [Batch 65/65] [D loss: 0.048353] [G loss: 2.855994, adv: 1.323656, pixel: 0.075325, cycle: 0.066679] ETA: 0:37:11        \n",
      "Early stopping at epoch 160 with best loss 0.043855\n",
      "\n",
      "\n",
      "\n",
      "Iniciando treinamento com 3 canais\n",
      "\n",
      "\n",
      "\n",
      "Treinando taxa=10, fold=1\n",
      "[Epoch 70/1000] [Batch 65/65] [D loss: 0.019030] [G loss: 2.288669, adv: 1.121674, pixel: 0.014060, cycle: 0.015631] ETA: 0:40:16     3   \n",
      "Early stopping at epoch 70 with best loss 0.014501\n",
      "Treinando taxa=10, fold=2\n",
      "[Epoch 281/1000] [Batch 65/65] [D loss: 0.001036] [G loss: 2.148959, adv: 1.049194, pixel: 0.016334, cycle: 0.017118] ETA: 0:32:38       \n",
      "Early stopping at epoch 281 with best loss 0.012304\n",
      "Treinando taxa=10, fold=3\n",
      "[Epoch 61/1000] [Batch 65/65] [D loss: 0.006972] [G loss: 2.221756, adv: 1.080797, pixel: 0.018682, cycle: 0.020740] ETA: 0:40:06     3   \n",
      "Early stopping at epoch 61 with best loss 0.015019\n",
      "Treinando taxa=10, fold=4\n",
      "[Epoch 159/1000] [Batch 65/65] [D loss: 0.007932] [G loss: 1.748613, adv: 0.849412, pixel: 0.015573, cycle: 0.017107] ETA: 0:35:31     \n",
      "Early stopping at epoch 159 with best loss 0.012703\n",
      "Treinando taxa=10, fold=5\n",
      "[Epoch 103/1000] [Batch 65/65] [D loss: 0.012348] [G loss: 1.541658, adv: 0.746453, pixel: 0.016697, cycle: 0.016028] ETA: 0:38:38    \n",
      "Early stopping at epoch 103 with best loss 0.013892\n",
      "Treinando taxa=20, fold=1\n",
      "[Epoch 61/1000] [Batch 65/65] [D loss: 0.002794] [G loss: 2.260367, adv: 1.079917, pixel: 0.036003, cycle: 0.032265] ETA: 0:39:32   :36   \n",
      "Early stopping at epoch 61 with best loss 0.031251\n",
      "Treinando taxa=20, fold=2\n",
      "[Epoch 103/1000] [Batch 65/65] [D loss: 0.089649] [G loss: 2.685716, adv: 1.301526, pixel: 0.028572, cycle: 0.027046] ETA: 0:38:11       \n",
      "Early stopping at epoch 103 with best loss 0.027510\n",
      "Treinando taxa=20, fold=3\n",
      "[Epoch 250/1000] [Batch 65/65] [D loss: 0.002686] [G loss: 2.137569, adv: 1.033456, pixel: 0.023948, cycle: 0.023354] ETA: 0:31:42        \n",
      "Early stopping at epoch 250 with best loss 0.023685\n",
      "Treinando taxa=20, fold=4\n",
      "[Epoch 90/1000] [Batch 65/65] [D loss: 0.006760] [G loss: 1.960692, adv: 0.949272, pixel: 0.021720, cycle: 0.020214] ETA: 0:39:32     5   \n",
      "Early stopping at epoch 90 with best loss 0.029930\n",
      "Treinando taxa=20, fold=5\n",
      "[Epoch 134/1000] [Batch 65/65] [D loss: 0.001443] [G loss: 2.177488, adv: 1.026085, pixel: 0.041611, cycle: 0.041853] ETA: 0:36:47        \n",
      "Early stopping at epoch 134 with best loss 0.025503\n",
      "Treinando taxa=30, fold=1\n",
      "[Epoch 158/1000] [Batch 65/65] [D loss: 0.127996] [G loss: 2.069490, adv: 0.961765, pixel: 0.042910, cycle: 0.051525] ETA: 0:34:50       \n",
      "Early stopping at epoch 158 with best loss 0.038519\n",
      "Treinando taxa=30, fold=2\n",
      "[Epoch 194/1000] [Batch 65/65] [D loss: 0.007145] [G loss: 1.835749, adv: 0.876889, pixel: 0.030472, cycle: 0.025749] ETA: 0:34:55        \n",
      "Early stopping at epoch 194 with best loss 0.036047\n",
      "Treinando taxa=30, fold=3\n",
      "[Epoch 83/1000] [Batch 65/65] [D loss: 0.017750] [G loss: 2.192541, adv: 1.037848, pixel: 0.042233, cycle: 0.037305] ETA: 0:38:47   :40   \n",
      "Early stopping at epoch 83 with best loss 0.040950\n",
      "Treinando taxa=30, fold=4\n",
      "[Epoch 180/1000] [Batch 65/65] [D loss: 0.010791] [G loss: 1.832225, adv: 0.869485, pixel: 0.034238, cycle: 0.029509] ETA: 0:35:27    7   \n",
      "Early stopping at epoch 180 with best loss 0.036355\n",
      "Treinando taxa=30, fold=5\n",
      "[Epoch 148/1000] [Batch 65/65] [D loss: 0.007269] [G loss: 2.262923, adv: 1.076929, pixel: 0.034198, cycle: 0.037434] ETA: 0:36:13       \n",
      "Early stopping at epoch 148 with best loss 0.040509\n",
      "Treinando taxa=40, fold=1\n",
      "[Epoch 259/1000] [Batch 65/65] [D loss: 0.001464] [G loss: 2.046204, adv: 0.967133, pixel: 0.046725, cycle: 0.032606] ETA: 0:31:06        \n",
      "Early stopping at epoch 259 with best loss 0.049806\n",
      "Treinando taxa=40, fold=2\n",
      "[Epoch 106/1000] [Batch 65/65] [D loss: 0.001899] [G loss: 2.223805, adv: 1.049685, pixel: 0.045204, cycle: 0.039616] ETA: 0:37:47        \n",
      "Early stopping at epoch 106 with best loss 0.053045\n",
      "Treinando taxa=40, fold=3\n",
      "[Epoch 60/1000] [Batch 65/65] [D loss: 0.009344] [G loss: 2.103079, adv: 0.974642, pixel: 0.054116, cycle: 0.049840] ETA: 0:40:38         \n",
      "Early stopping at epoch 60 with best loss 0.059345\n",
      "Treinando taxa=40, fold=4\n",
      "[Epoch 114/1000] [Batch 65/65] [D loss: 0.097139] [G loss: 1.511570, adv: 0.686392, pixel: 0.050157, cycle: 0.044315] ETA: 0:37:35        \n",
      "Early stopping at epoch 114 with best loss 0.052355\n",
      "Treinando taxa=40, fold=5\n",
      "[Epoch 273/1000] [Batch 65/65] [D loss: 0.086369] [G loss: 1.126266, adv: 0.505880, pixel: 0.040773, cycle: 0.036866] ETA: 0:24:34     \n",
      "Early stopping at epoch 273 with best loss 0.047063\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def asmape(y_true, y_pred, mask=None):\n",
    "    if mask is not None:\n",
    "         y_true, y_pred = y_true[mask==1], y_pred[mask==1]\n",
    "    if type(y_true) is list or type(y_pred) is list:\n",
    "         y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    len_ = len(y_true)\n",
    "    tmp = 100 * (np.nansum(np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))/len_)\n",
    "\n",
    "    return tmp\n",
    "\n",
    "class LoaderDataset(Dataset):\n",
    "    def __init__(self, root_zebra, root_horse, root_masks, chanels=3):\n",
    "        self.root_zebra = root_zebra\n",
    "        self.root_horse = root_horse\n",
    "        self.root_index = root_masks\n",
    "        \n",
    "        self.zebra_images = sorted(os.listdir(root_zebra))\n",
    "        self.horse_images = sorted(os.listdir(root_horse))\n",
    "        self.index = sorted(os.listdir(root_masks))\n",
    "\n",
    "        self.length_dataset = max(len(self.zebra_images), len(self.horse_images))\n",
    "        self.zebra_len = len(self.zebra_images)\n",
    "        self.horse_len = len(self.horse_images)\n",
    "        self.index_len = len(self.index)\n",
    "        self.chanels = chanels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_normalize(image):\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        min_val = torch.min(image)\n",
    "        max_val = torch.max(image)\n",
    "        scale = torch.clamp(max_val - min_val, min=1e-5)  # Evita divisão por zero\n",
    "        image_normalized = 2 * (image - min_val) / scale - 1  # Escala para [-1, 1]\n",
    "        return image_normalized, min_val, max_val\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        zebra_img = self.zebra_images[index % self.zebra_len]\n",
    "        horse_img = self.horse_images[index % self.horse_len]\n",
    "        index_ids = self.index[index % self.index_len]\n",
    "\n",
    "        zebra_path = os.path.join(self.root_zebra, zebra_img)\n",
    "        horse_path = os.path.join(self.root_horse, horse_img)\n",
    "        index_path = os.path.join(self.root_index, index_ids)\n",
    "        # print(zebra_path, horse_path, index_path)\n",
    "\n",
    "        zebra_img = np.load(zebra_path)\n",
    "        horse_img = np.load(horse_path)\n",
    "        mask = np.load(index_path)\n",
    "\n",
    "        if len(zebra_img.shape) > 3:\n",
    "            zebra_img = zebra_img.reshape(32, 32, 3)\n",
    "            horse_img = horse_img.reshape(32, 32, 3)\n",
    "\n",
    "        zebra_img = np.transpose(zebra_img, (2, 0, 1))\n",
    "        horse_img = np.transpose(horse_img, (2, 0, 1))\n",
    "\n",
    "        if self.chanels == 2:\n",
    "            zebra_img = zebra_img[:2, :, :]\n",
    "            horse_img = horse_img[:2, :, :]\n",
    "        elif self.chanels == 1:\n",
    "            zebra_img = np.sum(zebra_img, axis=0, keepdims=True)\n",
    "            horse_img = np.sum(horse_img, axis=0, keepdims=True)\n",
    "\n",
    "        zebra_img, min_val_z, max_val_z = LoaderDataset.custom_normalize(zebra_img)\n",
    "        horse_img, _, _ = LoaderDataset.custom_normalize(horse_img)\n",
    "\n",
    "        mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "        return zebra_img, horse_img, min_val_z, max_val_z, mask\n",
    "\n",
    "import pandas as pd \n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape, mean_absolute_error as mae, mean_squared_error as mse\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "def add_masked_gaussian_noise(x: torch.Tensor, mask: torch.Tensor, sigma: float) -> torch.Tensor:\n",
    "    # x: [B,C,H,W], mask: [B,1,H,W]\n",
    "    inv = (1.0 - mask).expand(-1, x.size(1), -1, -1)\n",
    "    noise = torch.randn_like(x) * sigma\n",
    "    return x + inv * noise\n",
    "\n",
    "\n",
    "\n",
    "def train_fn(dataloader, G_AB, G_BA, D_A, D_B, opt_G, opt_D_A, opt_D_B,\n",
    "             pixelwise_loss, cycle_loss, adversarial_loss, val_loader, scaler,\n",
    "             patience, save_dir, NUM_EPOCHS=1000):\n",
    "    \n",
    "    def safe_save(model, path):\n",
    "        tmp_path = path + \".tmp\"\n",
    "        torch.save(model.state_dict(), tmp_path)\n",
    "        os.replace(tmp_path, path)\n",
    "    \n",
    "    def james_stein_reduce(errors: torch.Tensor) -> torch.Tensor:\n",
    "        mean = torch.mean(errors)\n",
    "        var = torch.var(errors, unbiased=False)\n",
    "        norm_sq = torch.sum((errors - mean) ** 2)\n",
    "        dim = errors.numel()\n",
    "        shrinkage = torch.clamp(1 - ((dim - 2) * var / (norm_sq + 1e-8)), min=0.0)\n",
    "        return mean + shrinkage * (errors - mean).mean()\n",
    "    \n",
    "    initial_best_loss = float('inf')\n",
    "\n",
    "    G_AB.train()\n",
    "    G_BA.train()\n",
    "    D_A.train()\n",
    "    D_B.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    device = next(G_AB.parameters()).device\n",
    "    prev_time = time.time()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        H_reals = 0\n",
    "        H_fakes = 0\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        initial_sigma = 0.1\n",
    "        final_sigma = 0.0\n",
    "        warmup_epochs = 100\n",
    "        sigma = max(0.0, initial_sigma * (1 - epoch / warmup_epochs))\n",
    "        \n",
    "        for i, (real_A, real_B, _, _, masks) in enumerate(dataloader):\n",
    "            real_A = real_A.to(device)\n",
    "            real_B = real_B.to(device)\n",
    "            masks = masks.to(device).view(-1, 1, 32, 32).float()\n",
    "            # Adicionar ruído gaussiano às regiões mascaradas\n",
    "            real_A = add_masked_gaussian_noise(real_A, masks, sigma=sigma)\n",
    "            real_B = add_masked_gaussian_noise(real_B, masks, sigma=sigma)\n",
    "\n",
    "\n",
    "            # Criar tensores valid e fake usando a saída do discriminador com input e target reais\n",
    "            valid = torch.ones_like(D_A(real_A * masks, real_A * masks))\n",
    "            fake = torch.zeros_like(valid)\n",
    "\n",
    "            # Train Generators\n",
    "            opt_G.zero_grad()\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                fake_B = G_AB(real_A)\n",
    "                fake_A = G_BA(real_B)\n",
    "\n",
    "                pred_fake_B = D_B(fake_B * masks, real_B * masks)\n",
    "                pred_fake_A = D_A(fake_A * masks, real_A * masks)\n",
    "\n",
    "                loss_GAN_AB = adversarial_loss(pred_fake_B, valid)\n",
    "                loss_GAN_BA = adversarial_loss(pred_fake_A, valid)\n",
    "\n",
    "                recov_A = G_BA(fake_B)\n",
    "                recov_B = G_AB(fake_A)\n",
    "\n",
    "                loss_cycle_A = cycle_loss(recov_A * masks, real_A * masks)\n",
    "                loss_cycle_B = cycle_loss(recov_B * masks, real_B * masks)\n",
    "\n",
    "                loss_pixelwise = (pixelwise_loss(fake_A * masks, real_A * masks) +\n",
    "                                \tpixelwise_loss(fake_B * masks, real_B * masks)) / 2\n",
    "\n",
    "                loss_G_raw = loss_GAN_AB + loss_GAN_BA + loss_cycle_A + loss_cycle_B + loss_pixelwise\n",
    "                loss_G = james_stein_reduce(loss_G_raw.flatten())\n",
    "\n",
    "            scaler.scale(loss_G).backward()\n",
    "            scaler.step(opt_G)\n",
    "            scaler.update()\n",
    "\n",
    "            # Train Discriminator A\n",
    "            opt_D_A.zero_grad()\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                pred_real = D_A(real_A * masks, real_A * masks)\n",
    "                pred_fake = D_A(fake_A.detach() * masks, real_A * masks)\n",
    "                loss_D_A = (adversarial_loss(pred_real, valid) + adversarial_loss(pred_fake, fake)) / 2\n",
    "            scaler.scale(loss_D_A).backward()\n",
    "            scaler.step(opt_D_A)\n",
    "\n",
    "            # Train Discriminator B\n",
    "            opt_D_B.zero_grad()\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                pred_real = D_B(real_B * masks, real_B * masks)\n",
    "                pred_fake = D_B(fake_B.detach() * masks, real_B * masks)\n",
    "                loss_D_B = (adversarial_loss(pred_real, valid) + adversarial_loss(pred_fake, fake)) / 2\n",
    "            scaler.scale(loss_D_B).backward()\n",
    "            scaler.step(opt_D_B)\n",
    "\n",
    "            total_loss += loss_G.item()\n",
    "\n",
    "            batches_done = epoch * len(dataloader) + i + 1\n",
    "            batches_left = NUM_EPOCHS * len(dataloader) - batches_done\n",
    "            time_left = datetime.timedelta(seconds=int(batches_left * (time.time() - prev_time)))\n",
    "            prev_time = time.time()\n",
    "\n",
    "            sys.stdout.write(\n",
    "                f\"\\r[Epoch {epoch+1}/{NUM_EPOCHS}] [Batch {i+1}/{len(dataloader)}] \"\n",
    "                f\"[D loss: {(loss_D_A.item() + loss_D_B.item())/2:.6f}] \"\n",
    "                f\"[G loss: {loss_G.item():.6f}, adv: {(loss_GAN_AB.item() + loss_GAN_BA.item())/2:.6f}, \"\n",
    "                f\"pixel: {loss_pixelwise.item():.6f}, cycle: {(loss_cycle_A.item() + loss_cycle_B.item())/2:.6f}] \"\n",
    "                f\"ETA: {time_left}   \"\n",
    "            )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        # Validação\n",
    "        val_loss = 0\n",
    "        G_AB.eval()\n",
    "        G_BA.eval()\n",
    "        with torch.no_grad():\n",
    "            for real_A, real_B, _, _, masks in val_loader:\n",
    "                real_A = real_A.to(device)\n",
    "                real_B = real_B.to(device)\n",
    "                masks = masks.to(device).view(-1, 1, 32, 32).float()\n",
    "                # Adicionar ruído gaussiano às regiões mascaradas\n",
    "                real_A = add_masked_gaussian_noise(real_A, masks, sigma=sigma)\n",
    "                real_B = add_masked_gaussian_noise(real_B, masks, sigma=sigma)\n",
    "\n",
    "                fake_A = G_BA(real_B)\n",
    "                fake_B = G_AB(real_A)\n",
    "\n",
    "                loss_val = (pixelwise_loss(fake_A * masks, real_A * masks) +\n",
    "                            pixelwise_loss(fake_B * masks, real_B * masks)) / 2\n",
    "                val_loss += loss_val.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        \n",
    "\n",
    "        if initial_best_loss > avg_val_loss:\n",
    "          initial_best_loss = avg_val_loss\n",
    "          epochs_no_improve = 0\n",
    "        if initial_best_loss < avg_val_loss:\n",
    "           epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "          print(f\"\\nEarly stopping at epoch {epoch+1} with best loss {initial_best_loss:.6f}\")\n",
    "          break \n",
    "        \n",
    "        if os.path.exists(save_dir) == False:\n",
    "          os.makedirs(save_dir)\n",
    "        \n",
    "        model_path_ab = f\"{save_dir}/G_AB.pth\"\n",
    "        model_path_ba = f\"{save_dir}/G_BA.pth\"\n",
    "\n",
    "        if os.path.exists(model_path_ab) and os.path.exists(model_path_ba):\n",
    "            G_AB.load_state_dict(torch.load(model_path_ab, weights_only=True))\n",
    "            G_BA.load_state_dict(torch.load(model_path_ba, weights_only=True))\n",
    "        else:\n",
    "            pass\n",
    "        safe_save(G_AB, model_path_ab)\n",
    "        safe_save(G_BA, model_path_ba)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(gen_Z, test_loader, taxa, fold, chanells,DEVICE):\n",
    "\n",
    "\t\tgen_Z.eval()\n",
    "\t\t\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# Criar o DataFrame com as colunas desejadas\n",
    "\t\t\tdf = pd.DataFrame([], columns=['mae', 'asmape', 'mape', 'rmse', 'scale'], index=test_loader.dataset.horse_images)\n",
    "\n",
    "\t\t\tfor (zebra, horse, std_val, mean_val, masks), name in zip(test_loader, test_loader.dataset.horse_images):\n",
    "\t\t\t\t\t# Verificar as dimensões das entradas\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Mover dados para o dispositivo\n",
    "\t\t\t\t\tzebra = zebra.to(DEVICE)\n",
    "\t\t\t\t\thorse = horse.to(DEVICE)\n",
    "\n",
    "\t\t\t\t\t# Converter std_val e mean_val para tensores e movê-los para o dispositivo\n",
    "\t\t\t\t\tstd_val = torch.tensor(std_val, device=DEVICE) if not isinstance(std_val, torch.Tensor) else std_val.to(DEVICE)\n",
    "\t\t\t\t\tmean_val = torch.tensor(mean_val, device=DEVICE) if not isinstance(mean_val, torch.Tensor) else mean_val.to(DEVICE)\n",
    "\n",
    "\t\t\t\t\t# Gerar fake_zebra usando o gerador\n",
    "\t\t\t\t\tfake_zebra = gen_Z(horse)\n",
    "\n",
    "\t\t\t\t\t# Mover apenas as imagens para a CPU antes de operações subsequentes\n",
    "\t\t\t\t\tzebra = zebra.cpu()\n",
    "\t\t\t\t\tfake_zebra = fake_zebra.cpu()\n",
    "\n",
    "\t\t\t\t\t# Voltar para escala original \n",
    "\t\t\t\t\tzebra = zebra * std_val.cpu() + mean_val.cpu()\n",
    "\t\t\t\t\tfake_zebra = fake_zebra * std_val.cpu() + mean_val.cpu()\n",
    "\n",
    "\t\t\t\t\t# Somar sobre o canal e achatar as imagens\n",
    "\t\t\t\t\tzebra = torch.sum(zebra, dim=1).flatten()*masks\n",
    "\t\t\t\t\tfake_zebra = torch.sum(fake_zebra, dim=1).flatten()*masks\n",
    "\n",
    "\t\t\t\t\t# Calcular as métricas\n",
    "\t\t\t\t\tmae_value = round(mae(zebra, fake_zebra), 3)\n",
    "\t\t\t\t\tmape_value = round(mape(zebra, fake_zebra) * 100, 3)\n",
    "\t\t\t\t\trmse_value = round(np.sqrt(mse(zebra, fake_zebra)), 3)\n",
    "\t\t\t\t\tsmape_value = round(asmape(zebra, fake_zebra, masks), 3)\n",
    "\n",
    "\t\t\t\t\t# Adicionar os resultados ao DataFrame\n",
    "\t\t\t\t\tdf.loc[name] = [mae_value,smape_value , mape_value, rmse_value, np.max(zebra.numpy()) - np.min(zebra.numpy())]\n",
    "\n",
    "\t\t\t# Salvar o DataFrame em um arquivo CSV\n",
    "\t\t\tdirectory = \"./resultados/resultados_discogan\"\n",
    "\t\t\tif not os.path.exists(directory):\n",
    "\t\t\t\t\tos.makedirs(directory)\n",
    "\n",
    "\t\t\tdf.to_csv(os.path.join(directory, f'result_{str(chanells)}c_{taxa}_{fold}.csv'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------\n",
    "#  Treinamento\n",
    "# ----------\n",
    "# Parâmetros de treinamento\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=100, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.verbose = verbose\n",
    "        self.best_epoch = 0\n",
    "\n",
    "    def __call__(self, val_loss, epoch=None):\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if epoch is not None:\n",
    "                self.best_epoch = epoch\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                if epoch is not None:\n",
    "                    print(f\"Epoch {epoch}: EarlyStopping counter = {self.counter}/{self.patience}\")\n",
    "                else:\n",
    "                    print(f\"EarlyStopping: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "\n",
    "def main(in_channels):\n",
    "\n",
    "    scal = torch.amp.GradScaler(True)\n",
    "    TRAIN_DIR = os.path.abspath(\"../dataset_final\")  \n",
    "    VAL_DIR = os.path.abspath(\"../dataset_final\")  \n",
    "    INDEX_TRAIN = os.path.abspath(\"../dataset_final\")  \n",
    "    INDEX_VAL = os.path.abspath(\"../dataset_final\")  \n",
    "    INDEX_TEST = os.path.abspath(\"../dataset_final\")    \n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 1000\n",
    "\n",
    "    lrd= 1e-4\n",
    "    lrg= 1e-3\n",
    "    b1 =0.5\n",
    "    b2= 0.999\n",
    "    \n",
    "    \n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    for taxa in ['10', '20', '30', '40']:\n",
    "        for fold in ['1', '2', '3', '4', '5']:\n",
    "            print(f\"Treinando taxa={taxa}, fold={fold}\")\n",
    "            save_dir = f\"./models_saved/discogan/{in_channels}/{taxa}/fold{fold}\"        \n",
    "\n",
    "            # Inicializar perdas\n",
    "            adversarial_loss = torch.nn.MSELoss().to(device)\n",
    "            cycle_loss = torch.nn.L1Loss().to(device)\n",
    "            pixelwise_loss = torch.nn.L1Loss().to(device)\n",
    "\n",
    "            # Inicializar modelos\n",
    "            G_AB = UNetGenerator(input_channels=in_channels, output_channels=in_channels).to(device)\n",
    "            G_BA = UNetGenerator(input_channels=in_channels, output_channels=in_channels).to(device)\n",
    "            D_A = PatchDiscriminator(input_channels=in_channels).to(device)\n",
    "            D_B = PatchDiscriminator(input_channels=in_channels).to(device)\n",
    "\n",
    "            # Inicializar pesos\n",
    "            G_AB.apply(weights_init_normal)\n",
    "            G_BA.apply(weights_init_normal)\n",
    "            D_A.apply(weights_init_normal)\n",
    "            D_B.apply(weights_init_normal)\n",
    "\n",
    "            # Otimizadores\n",
    "            optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=1e-4, betas=(b1, b2))\n",
    "            optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lrd, betas=(b1, b2))\n",
    "            optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lrg, betas=(b1, b2))\n",
    "\n",
    "            # Carregar datasets e loaders\n",
    "            train_dataset = LoaderDataset(\n",
    "                root_zebra=os.path.join(TRAIN_DIR, \"label\", taxa, \"folds\", f\"fold{fold}\", \"train\"),\n",
    "                root_horse=os.path.join(TRAIN_DIR, \"input\", taxa, \"folds\", f\"fold{fold}\", \"train\"),\n",
    "                root_masks=os.path.join(INDEX_TRAIN, \"input\", taxa, \"folds\", f\"fold{fold}\", \"index_train\"),\n",
    "                chanels=in_channels\n",
    "            )\n",
    "            val_dataset = LoaderDataset(\n",
    "                root_zebra=os.path.join(VAL_DIR, \"label\", taxa, \"folds\", f\"fold{fold}\", \"val\"),\n",
    "                root_horse=os.path.join(VAL_DIR, \"input\", taxa, \"folds\", f\"fold{fold}\", \"val\"),\n",
    "                root_masks=os.path.join(INDEX_VAL, \"input\", taxa, \"folds\", f\"fold{fold}\", \"index_val\"),\n",
    "                chanels=in_channels\n",
    "            )\n",
    "            test_dataset = LoaderDataset(\n",
    "                root_zebra=os.path.join(VAL_DIR, \"label\", taxa, \"folds\", f\"fold{fold}\", \"test\"),\n",
    "                root_horse=os.path.join(VAL_DIR, \"input\", taxa, \"folds\", f\"fold{fold}\", \"test\"),\n",
    "                root_masks=os.path.join(INDEX_TEST, \"input\", taxa, \"folds\", f\"fold{fold}\", \"index\"),\n",
    "                chanels=in_channels\n",
    "            )\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=(device==\"cuda\"))\n",
    "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=(device==\"cuda\"))\n",
    "            test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=(device==\"cuda\"))\n",
    "\n",
    "            scaler = torch.amp.GradScaler('cuda') if device == \"cuda\" else None\n",
    "            best_val_loss = float('inf')\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "            \n",
    "            train_fn(\n",
    "                    train_loader, G_AB, G_BA, D_A, D_B,\n",
    "                    optimizer_G, optimizer_D_A, optimizer_D_B,\n",
    "                    pixelwise_loss, cycle_loss, adversarial_loss,\n",
    "                    val_loader, scal,\n",
    "                    50,\n",
    "                    save_dir=save_dir, \n",
    "                    NUM_EPOCHS=NUM_EPOCHS\n",
    "                )\n",
    "           \n",
    "\n",
    "            # Salvar modelo final depois do treino\n",
    "            save_dir = f\"./models_saved/discogan/{in_channels}/{taxa}/fold{fold}\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            # model_path = os.path.join(save_dir, \"generator_final.pth\")\n",
    "            # torch.save(G_BA.state_dict(), model_path)\n",
    "            # print(f\"Modelo salvo em: {model_path}\")\n",
    "\n",
    "            # Teste\n",
    "            test(G_BA, test_loader, taxa, fold, in_channels,device)\n",
    "\n",
    "import gc\n",
    "if __name__ == '__main__':\n",
    "    for i in [1, 2, 3]:\n",
    "        print(f\"\\n\\n\\nIniciando treinamento com {i} canais\\n\\n\\n\")\n",
    "        main(i)\n",
    "        # Libera memória (GPU e CPU)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_envmau",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
